{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12804856,"sourceType":"datasetVersion","datasetId":8096351},{"sourceId":12826938,"sourceType":"datasetVersion","datasetId":8111862},{"sourceId":12873480,"sourceType":"datasetVersion","datasetId":8143613}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemini ","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics supervision numpy opencv-python scikit-learn pandas\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade ultralytics torch torchvision","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install mplsoccer\nimport sys\nimport os\nimport cv2\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import deque\n\n# Machine Learning & Computer Vision Libraries\nfrom ultralytics import YOLO\nimport supervision as sv\nfrom sklearn.cluster import KMeans\nimport easyocr\n\n# Google Gemini for AI Commentary\nimport google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\nimport time\n\n# Plotting for Heatmaps\nfrom mplsoccer import Pitch\n\n# --- Video Utilities ---\ndef read_video(video_path):\n    \"\"\"Reads a video file and returns a list of its frames.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frames.append(frame)\n    cap.release()\n    return frames\n\ndef save_video(output_video_frames, output_video_path):\n    \"\"\"Saves a list of frames as a video file.\"\"\"\n    if not output_video_frames:\n        print(\"No frames to save.\")\n        return\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_video_path, fourcc, 24, (output_video_frames[0].shape[1], output_video_frames[0].shape[0]))\n    for frame in output_video_frames:\n        out.write(frame)\n    out.release()\n\n# --- BBox Utilities ---\ndef get_center_of_bbox(bbox):\n    x1, y1, x2, y2 = bbox\n    return int((x1 + x2) / 2), int((y1 + y2) / 2)\n\ndef get_bbox_width(bbox):\n    return int(bbox[2] - bbox[0])\n\ndef get_foot_position(bbox):\n    x1, y1, x2, y2 = bbox\n    return int((x1 + x2) / 2), int(y2)\n    \ndef measure_distance(p1, p2):\n    return ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**0.5\n\ndef measure_xy_distance(p1, p2):\n    return p1[0] - p2[0], p1[1] - p2[1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImprovedCommentaryEngine:\n    def __init__(self, clip_duration_seconds=5, fps=24):\n        self.clip_length_frames = int(clip_duration_seconds * fps)\n        self.frame_buffer = deque(maxlen=self.clip_length_frames)\n        self.temp_video_path = \"/kaggle/working/temp_commentary_clip.mp4\"\n        self.latest_commentary = \"Match analysis is starting...\"\n        self.fps = fps\n        self.match_context = {\n            'possession_changes': [], 'recent_events': [],\n            'ball_position_history': [], 'player_movements': []\n        }\n        \n        print(\"ðŸŽ™ï¸ Initializing Enhanced Gemini Commentary Engine...\")\n        try:\n            # user_secrets = UserSecretsClient()\n            api_key = \"abcd\"\n            genai.configure(api_key=api_key)\n            self.model = genai.GenerativeModel('models/gemini-2.5-flash')\n            print(\"âœ… Gemini 2.5 Flash model loaded successfully.\")\n        except Exception as e:\n            self.model = None\n            print(f\"âš ï¸ Could not initialize Gemini model: {e}\")\n\n    def update_with_context(self, frame, tracks_data, frame_num, events_data=None):\n        if not self.model: return\n        \n        game_context = self._extract_game_context(tracks_data, frame_num, events_data)\n        \n        self.match_context['recent_events'].append(game_context)\n        if len(self.match_context['recent_events']) > 10:\n            self.match_context['recent_events'].pop(0)\n        \n        self.frame_buffer.append(frame)\n        \n        if len(self.frame_buffer) == self.clip_length_frames:\n            print(\"Generating tactical summary...\")\n            new_comment = self._generate_contextual_commentary(game_context)\n            if new_comment:\n                self.latest_commentary = new_comment\n            self.frame_buffer.clear()\n\n    def _extract_game_context(self, tracks_data, frame_num, events_data):\n        context = {\n            'frame_num': frame_num,\n            'timestamp': f\"{int(frame_num / (self.fps * 60))}:{int((frame_num / self.fps) % 60):02d}\",\n            'players_detected': len(tracks_data['players'][frame_num]),\n            'ball_detected': 1 in tracks_data['ball'][frame_num],\n            'possession': None, 'ball_speed': 0, 'recent_events': []\n        }\n        \n        for player_id, player_info in tracks_data['players'][frame_num].items():\n            if player_info.get('has_ball', False):\n                context['possession'] = f\"Player {player_id} (Team {player_info.get('team', 'Unknown')})\"\n                break\n        \n        if events_data is not None and not events_data.empty:\n            recent_events = events_data[\n                (events_data['minute'] * 60 + events_data['second']) >= (frame_num / self.fps - 10)\n            ].tail(3)\n            context['recent_events'] = recent_events.to_dict('records')\n        \n        return context\n\n    def _generate_contextual_commentary(self, game_context):\n        video_file = None\n        try:\n            height, width, _ = self.frame_buffer[0].shape\n            writer = cv2.VideoWriter(self.temp_video_path, cv2.VideoWriter_fourcc(*'mp4v'), self.fps, (width, height))\n            for frame in self.frame_buffer:\n                writer.write(frame)\n            writer.release()\n\n            video_file = genai.upload_file(path=self.temp_video_path)\n            while video_file.state.name == \"PROCESSING\":\n                time.sleep(2)\n                video_file = genai.get_file(video_file.name)\n            \n            if video_file.state.name == \"FAILED\":\n                return \"Video processing failed.\"\n            \n            context_prompt = self._create_detailed_prompt(game_context)\n            response = self.model.generate_content([context_prompt, video_file])\n            return response.text.strip().replace('\\n', ' ')\n            \n        except Exception as e:\n            print(f\"Commentary generation error: {e}\")\n            return self._generate_fallback_commentary(game_context)\n        finally:\n            if video_file: genai.delete_file(video_file.name)\n            if os.path.exists(self.temp_video_path): os.remove(self.temp_video_path)\n\n    def _create_detailed_prompt(self, context):\n        prompt = f\"\"\"You are a professional football (soccer) tactical analyst.\n\n        CURRENT GAME STATE:\n        - Match Time: {context['timestamp']}\n        - Ball Possession: {context.get('possession', 'Unclear')}\n        - Recent Match Events: {self._format_recent_events(context.get('recent_events', []))}\n\n        INSTRUCTIONS:\n        1. Analyze the short video clip of a football match.\n        2. Provide a brief, factual, tactical summary of the most significant action.\n        3. Describe the sequence of play objectively. Example: \"The player in red receives a pass, moves past a defender, and attempts a shot which is blocked.\"\n        4. Do NOT use emotional or exciting commentary language like \"incredible!\" or \"what a save!\".\n        5. Your entire response must be a single, concise sentence (max 25 words).\n\n        Analyze the clip and provide your tactical summary:\"\"\"\n        return prompt\n\n    def _format_recent_events(self, events):\n        if not events: return \"No recent significant events detected.\"\n        \n        formatted = []\n        for event in events[-3:]:\n            if isinstance(event, dict):\n                event_type = event.get('type_name', 'Unknown')\n                team = event.get('team_name', 'Unknown Team')\n                formatted.append(f\"- {event_type} by {team}\")\n        \n        return \"\\n\".join(formatted) if formatted else \"No recent significant events detected.\"\n\n    def _generate_fallback_commentary(self, context):\n        if context.get('possession'):\n            return f\"Play continues with {context['possession']} in possession.\"\n        return \"The match continues with both teams looking for opportunities.\"\n\nclass RealTimeTicker:\n    \"\"\"\n    Generates a simple, real-time text commentary for each frame based on game state.\n    \"\"\"\n    def __init__(self, fps=24):\n        self.fps = fps\n        self.last_player_id = -1\n        self.last_team_id = -1\n        self.ticker_text = \"Match begins!\"\n        self.text_display_frames = 0\n\n    def _get_ball_carrier(self, player_track):\n        for player_id, data in player_track.items():\n            if data.get('has_ball', False):\n                return player_id, data.get('team')\n        return -1, -1\n\n    def update(self, tracks, frame_num):\n        if self.text_display_frames > 0:\n            self.text_display_frames -= 1\n            return self.ticker_text\n        \n        player_track = tracks['players'][frame_num]\n        current_player_id, current_team_id = self._get_ball_carrier(player_track)\n\n        if (current_player_id != -1 and self.last_player_id != -1 and \n            current_player_id != self.last_player_id and current_team_id == self.last_team_id):\n            self.ticker_text = f\"Pass from Player {self.last_player_id} to Player {current_player_id}.\"\n            self.text_display_frames = self.fps * 2\n        \n        elif current_player_id != -1 and self.last_team_id != -1 and current_team_id != self.last_team_id:\n            self.ticker_text = f\"Team {current_team_id} gains possession!\"\n            self.text_display_frames = self.fps * 2\n        \n        else:\n            if current_player_id != -1:\n                self.ticker_text = f\"Player {current_player_id} (Team {current_team_id}) on the ball.\"\n            else:\n                self.ticker_text = \"Ball is loose.\"\n\n        if current_player_id != -1:\n            self.last_player_id = current_player_id\n            self.last_team_id = current_team_id\n        else:\n            self.last_player_id = -1\n            \n        return self.ticker_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class JerseyNumberRecognizer:\n    def __init__(self):\n        self.reader = easyocr.Reader(['en'], gpu=True)\n        self.jersey_cache = {}\n        print(\"âœ… Jersey OCR module initialized.\")\n\n    def recognize_jersey_number(self, player_crop, tracker_id):\n        if tracker_id in self.jersey_cache: return self.jersey_cache[tracker_id]\n        if player_crop.size == 0: return None\n        \n        crop_gray = cv2.cvtColor(player_crop, cv2.COLOR_BGR2GRAY)\n        results = self.reader.readtext(crop_gray, allowlist='0123456789', detail=1)\n\n        best_result = None\n        for (bbox, text, prob) in results:\n            if prob > 0.6 and text.isdigit() and len(text) <= 2:\n                if best_result is None or prob > best_result[2]:\n                    best_result = (bbox, text, prob)\n        \n        if best_result:\n            self.jersey_cache[tracker_id] = best_result[1]\n            return best_result[1]\n        \n        return None\n\nclass Tracker:\n    def __init__(self, model_name='yolov8x.pt'):\n        self.model = YOLO(model_name)\n        self.tracker = sv.ByteTrack()\n        self.jersey_recognizer = JerseyNumberRecognizer()\n\n    def get_object_tracks(self, frames, read_from_stub=False, stub_path=None):\n        if read_from_stub and stub_path and os.path.exists(stub_path):\n            with open(stub_path, 'rb') as f: return pickle.load(f)\n\n        tracks = {\"players\": [], \"referees\": [], \"ball\": []}\n        \n        for frame_num, frame in enumerate(frames):\n            if frame_num % 20 == 0: print(f\"Processing frame {frame_num}/{len(frames)}\")\n            results = self.model.predict(frame, conf=0.1)[0]\n            detections = sv.Detections.from_ultralytics(results)\n            \n            # Filter for players (class_id for 'person' is typically 0)\n            player_detections = detections[detections.class_id == 0]\n            tracked_players = self.tracker.update_with_detections(player_detections)\n            \n            tracks[\"players\"].append({})\n            tracks[\"referees\"].append({})\n            \n            for detection_data in tracked_players:\n                bbox = detection_data[0]\n                track_id = detection_data[4]\n                \n                player_crop = frame[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n                jersey_num = self.jersey_recognizer.recognize_jersey_number(player_crop, track_id)\n                tracks[\"players\"][frame_num][track_id] = {\"bbox\": bbox.tolist(), \"jersey_number\": jersey_num}\n\n            # Filter for ball (class_id for 'sports ball' is typically 32)\n            ball_detections = detections[detections.class_id == 32]\n            tracks[\"ball\"].append({})\n            if len(ball_detections) > 0:\n                tracks[\"ball\"][frame_num][1] = {\"bbox\": ball_detections.xyxy[0].tolist()}\n        \n        if stub_path:\n            with open(stub_path, 'wb') as f: pickle.dump(tracks, f)\n        return tracks\n\n    def add_position_to_tracks(self, tracks):\n        for type, obj_tracks in tracks.items():\n            for frame_num, track in enumerate(obj_tracks):\n                for id, info in track.items():\n                    bbox = info['bbox']\n                    info['position'] = get_foot_position(bbox) if type != 'ball' else get_center_of_bbox(bbox)\n    \n    def interpolate_ball_positions(self, ball_positions):\n        ball_bboxes = [x.get(1, {}).get('bbox', []) for x in ball_positions]\n        df = pd.DataFrame(ball_bboxes, columns=['x1', 'y1', 'x2', 'y2']).interpolate().bfill()\n        return [{1: {\"bbox\": x}} for x in df.to_numpy().tolist()]\n\n    def _draw_player_ellipse(self, frame, bbox, color, track_id, jersey_num):\n        y2 = int(bbox[3])\n        x_center, _ = get_center_of_bbox(bbox)\n        width = get_bbox_width(bbox)\n        cv2.ellipse(frame, center=(x_center, y2), axes=(int(width), int(0.35 * width)), angle=0.0, startAngle=-45, endAngle=235, color=color, thickness=2, lineType=cv2.LINE_4)\n        \n        label = f\"#{jersey_num}\" if jersey_num else str(track_id)\n        (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n        rect_w, rect_h = w + 10, h + 10\n        x1_rect, y1_rect = x_center - rect_w//2, (y2 - rect_h//2) + 15\n        \n        cv2.rectangle(frame, (x1_rect, y1_rect), (x1_rect + rect_w, y1_rect + rect_h), color, cv2.FILLED)\n        cv2.putText(frame, label, (x1_rect + 5, y1_rect + h + 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n        return frame\n\n    def _draw_triangle(self, frame, bbox, color):\n        y, x = int(bbox[1]), int(get_center_of_bbox(bbox)[0])\n        points = np.array([[x, y], [x - 10, y - 20], [x + 10, y - 20]])\n        cv2.drawContours(frame, [points], 0, color, cv2.FILLED)\n        cv2.drawContours(frame, [points], 0, (0, 0, 0), 2)\n        return frame\n\n    def _draw_team_ball_control(self, frame, frame_num, team_ball_control):\n        overlay = frame.copy()\n        cv2.rectangle(overlay, (10, 10), (350, 70), (255, 255, 255), -1)\n        cv2.addWeighted(overlay, 0.5, frame, 0.5, 0, frame)\n        \n        team_1_frames = np.sum(team_ball_control[:frame_num + 1] == 1)\n        team_2_frames = np.sum(team_ball_control[:frame_num + 1] == 2)\n        total = max(1, team_1_frames + team_2_frames)\n        p1 = (team_1_frames / total) * 100\n        p2 = (team_2_frames / total) * 100\n        \n        cv2.putText(frame, f\"Team 1 Possession: {p1:.1f}%\", (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 2)\n        cv2.putText(frame, f\"Team 2 Possession: {p2:.1f}%\", (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 2)\n        return frame\n\n    def _draw_commentary_overlay(self, frame, text):\n        h, w, _ = frame.shape\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        thickness = 2\n        \n        font_scale = 1.0\n        (text_w, text_h), _ = cv2.getTextSize(text, font, font_scale, thickness)\n        \n        target_w = w * 0.9\n        if text_w > target_w:\n            font_scale = target_w / text_w\n            \n        (text_w, text_h), _ = cv2.getTextSize(text, font, font_scale, thickness)\n\n        banner_h = text_h + 20\n        overlay = frame.copy()\n        cv2.rectangle(overlay, (0, h - banner_h), (w, h), (0, 0, 0), -1)\n        cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)\n        \n        text_x = (w - text_w) // 2\n        text_y = h - 10\n        cv2.putText(frame, text, (text_x, text_y), font, font_scale, (255, 255, 255), thickness)\n        \n        return frame\n\nclass EventDetector:\n    def __init__(self):\n        self.shot_speed_threshold_mps = 15\n        self.frame_rate = 24\n\n    def detect_events(self, tracks):\n        player_assigner = PlayerBallAssigner()\n        ball_possession_log = []\n        for frame_num in range(len(tracks['players'])):\n            player_track = tracks['players'][frame_num]\n            ball_bbox = tracks['ball'][frame_num].get(1, {}).get('bbox')\n            assigned_player_id = player_assigner.assign_ball_to_player(player_track, ball_bbox) if ball_bbox else -1\n            ball_possession_log.append(assigned_player_id)\n\n        events = []\n        last_player_with_ball, pass_start_info = -1, {}\n        for frame_num, current_player_id in enumerate(ball_possession_log):\n            ball_pos_transformed = tracks['ball'][frame_num].get(1, {}).get('position_transformed')\n            if not ball_pos_transformed: continue\n\n            is_valid_pass = (current_player_id != last_player_with_ball and last_player_with_ball != -1 and current_player_id != -1)\n            if is_valid_pass:\n                start_player_team = tracks['players'][pass_start_info['frame']][last_player_with_ball].get('team')\n                end_player_team = tracks['players'][frame_num].get(current_player_id, {}).get('team')\n                if start_player_team == end_player_team and start_player_team is not None:\n                    events.append({\n                        \"type_name\": \"Pass\", \"player_name\": f\"Player_{last_player_with_ball}\",\n                        \"team_name\": f\"Team {start_player_team}\", \"x\": pass_start_info['position'][0],\n                        \"y\": pass_start_info['position'][1], \"end_x\": ball_pos_transformed[0],\n                        \"end_y\": ball_pos_transformed[1], \"minute\": int(frame_num / (self.frame_rate * 60)),\n                        \"second\": int((frame_num / self.frame_rate) % 60)\n                    })\n            \n            if current_player_id != -1:\n                pass_start_info = {'frame': frame_num, 'position': ball_pos_transformed}\n            last_player_with_ball = current_player_id\n            \n        return pd.DataFrame(events)\n\n# Other classes (TeamAssigner, PlayerBallAssigner, etc.)\nclass TeamAssigner:\n    def __init__(self):\n        self.team_colors, self.player_team_dict, self.kmeans = {}, {}, None\n    def get_player_color(self, frame, bbox):\n        image = frame[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n        if image.size == 0: return np.array([0,0,0])\n        top_half = image[0:int(image.shape[0] / 2), :]\n        if top_half.size == 0: return np.array([0,0,0])\n        kmeans = KMeans(n_clusters=2, init=\"k-means++\", n_init=1, random_state=0).fit(top_half.reshape(-1, 3))\n        labels = kmeans.labels_.reshape(top_half.shape[0], top_half.shape[1])\n        corner_clusters = [labels[0, 0], labels[0, -1], labels[-1, 0], labels[-1, -1]]\n        non_player_cluster = max(set(corner_clusters), key=corner_clusters.count)\n        return kmeans.cluster_centers_[1 - non_player_cluster]\n    def assign_team_color(self, frame, player_detections):\n        if not player_detections: return\n        colors = [self.get_player_color(frame, det[\"bbox\"]) for _, det in player_detections.items()]\n        self.kmeans = KMeans(n_clusters=2, init=\"k-means++\", n_init=10, random_state=0).fit(colors)\n        self.team_colors[1], self.team_colors[2] = self.kmeans.cluster_centers_\n    def get_player_team(self, frame, bbox, player_id):\n        if player_id in self.player_team_dict: return self.player_team_dict[player_id]\n        if self.kmeans is None: return 0\n        color = self.get_player_color(frame, bbox)\n        team_id = self.kmeans.predict(color.reshape(1, -1))[0] + 1\n        self.player_team_dict[player_id] = team_id\n        return team_id\n\nclass PlayerBallAssigner:\n    def __init__(self): self.max_dist = 70\n    def assign_ball_to_player(self, players, ball_bbox):\n        if not ball_bbox: return -1\n        ball_pos, min_dist, assigned_player = get_center_of_bbox(ball_bbox), float('inf'), -1\n        for id, player in players.items():\n            dist = measure_distance(get_foot_position(player['bbox']), ball_pos)\n            if dist < self.max_dist and dist < min_dist: min_dist, assigned_player = dist, id\n        return assigned_player\n\nclass CameraMovementEstimator:\n    def __init__(self, frame):\n        self.lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n        self.features = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n    def get_camera_movement(self, frames, read_from_stub=False, stub_path=None):\n        if read_from_stub and stub_path and os.path.exists(stub_path):\n            with open(stub_path, 'rb') as f: return pickle.load(f)\n        movements = [[0, 0]] * len(frames)\n        old_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n        old_features = cv2.goodFeaturesToTrack(old_gray, **self.features)\n        for i in range(1, len(frames)):\n            new_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n            new_features, status, _ = cv2.calcOpticalFlowPyrLK(old_gray, new_gray, old_features, None, **self.lk_params)\n            \n            good_new = new_features[status==1]\n            good_old = old_features[status==1]\n\n            move_x, move_y = 0, 0\n            if len(good_new) > 0:\n                move_x, move_y = np.mean(good_old - good_new, axis=0).ravel()\n\n            movements[i] = [move_x, move_y]\n            old_gray = new_gray.copy()\n            old_features = good_new.reshape(-1, 1, 2)\n        if stub_path:\n            with open(stub_path, 'wb') as f: pickle.dump(movements, f)\n        return movements\n    def add_adjust_positions_to_tracks(self, tracks, movements):\n        for type, obj_tracks in tracks.items():\n            for i, track in enumerate(obj_tracks):\n                for id, info in track.items():\n                    info['position_adjusted'] = (info['position'][0] + movements[i][0], info['position'][1] + movements[i][1])\n\nclass ViewTransformer:\n    def __init__(self):\n        court_w, court_l = 34, 52.5\n        self.pixel_verts = np.float32([[110, 1035], [265, 275], [910, 260], [1640, 915]])\n        self.target_verts = np.float32([[0, court_w], [0, 0], [court_l, 0], [court_l, court_w]])\n        self.transformer = cv2.getPerspectiveTransform(self.pixel_verts, self.target_verts)\n    def transform_point(self, point):\n        p = (int(point[0]), int(point[1]))\n        is_inside = cv2.pointPolygonTest(self.pixel_verts, p, False) >= 0\n        if not is_inside: return None\n        reshaped = np.array(point).reshape(-1, 1, 2).astype(np.float32)\n        transformed = cv2.perspectiveTransform(reshaped, self.transformer)\n        return transformed.reshape(-1, 2)\n    def add_transformed_position_to_tracks(self, tracks):\n        for type, obj_tracks in tracks.items():\n            for track in obj_tracks:\n                for id, info in track.items():\n                    pos = info.get('position_adjusted', info.get('position'))\n                    if pos:\n                        transformed = self.transform_point(pos)\n                        info['position_transformed'] = transformed.squeeze().tolist() if transformed is not None else None\n\nclass SpeedAndDistanceEstimator:\n    def __init__(self):\n        self.frame_window, self.frame_rate = 24, 24\n    def add_speed_and_distance_to_tracks(self, tracks):\n        total_dist = {}\n        for type, obj_tracks in tracks.items():\n            if type not in [\"players\", \"referees\"]: continue\n            for i in range(len(obj_tracks)):\n                for id, info in obj_tracks[i].items():\n                    if i > 0:\n                        prev_info = tracks[type][i-1].get(id)\n                        if prev_info and info.get('position_transformed') and prev_info.get('position_transformed'):\n                            dist = measure_distance(info['position_transformed'], prev_info['position_transformed'])\n                            total_dist[id] = total_dist.get(id, 0) + dist\n                            speed_mps = dist * self.frame_rate\n                            info['speed'] = speed_mps * 3.6 # km/h\n                            info['distance'] = total_dist[id]\n    def draw_speed_and_distance(self, frames, tracks):\n        output_frames = []\n        for i, frame in enumerate(frames):\n            for type, obj_tracks in tracks.items():\n                if type not in [\"players\", \"referees\"]: continue\n                for id, info in obj_tracks[i].items():\n                    if \"speed\" in info:\n                        x, y = get_foot_position(info['bbox'])\n                        cv2.putText(frame, f\"{info['speed']:.1f} km/h\", (x - 20, y + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)\n            output_frames.append(frame)\n        return output_frames","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    # --- SETUP ---\n    INPUT_VIDEO_PATH = \"/kaggle/input/football-video2/CityUtdR.mp4\"\n    STUB_PATH = \"/kaggle/working/tracks_stub.pkl\"\n    OUTPUT_VIDEO_PATH = \"/kaggle/working/final_analysis_video-gemini.mp4\"\n    \n    frames = read_video(INPUT_VIDEO_PATH)\n    if not frames:\n        print(\"Video file not found or could not be read. Check the path.\")\n        return None\n\n    cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n    fps = cap.get(cv2.CAP_PROP_FPS) or 24\n    cap.release()\n\n    # --- INITIALIZE ALL MODULES ---\n    tracker = Tracker('yolov8x.pt')\n    commentary_engine = ImprovedCommentaryEngine(fps=fps)\n    camera_estimator = CameraMovementEstimator(frames[0])\n    view_transformer = ViewTransformer()\n    speed_estimator = SpeedAndDistanceEstimator()\n    team_assigner = TeamAssigner()\n    player_assigner = PlayerBallAssigner()\n    ticker = RealTimeTicker(fps=fps)\n\n    # --- STAGE 1: TRACKING ---\n    print(\"Stage 1: Performing object detection and tracking...\")\n    tracks = tracker.get_object_tracks(frames, read_from_stub=False, stub_path=STUB_PATH)\n    tracks[\"ball\"] = tracker.interpolate_ball_positions(tracks[\"ball\"])\n    tracker.add_position_to_tracks(tracks)\n    \n    # --- STAGE 2: MOTION & PERSPECTIVE ---\n    print(\"Stage 2: Estimating camera motion and transforming perspective...\")\n    camera_movement = camera_estimator.get_camera_movement(frames)\n    camera_estimator.add_adjust_positions_to_tracks(tracks, camera_movement)\n    view_transformer.add_transformed_position_to_tracks(tracks)\n    speed_estimator.add_speed_and_distance_to_tracks(tracks)\n    \n    # --- STAGE 3: TEAM ASSIGNMENT ---\n    print(\"Stage 3: Assigning teams...\")\n    team_assigner.assign_team_color(frames[0], tracks['players'][0])\n    \n    for frame_num, frame in enumerate(frames):\n        player_track = tracks['players'][frame_num]\n        for player_id, track in player_track.items():\n            team = team_assigner.get_player_team(frame, track['bbox'], player_id)\n            tracks['players'][frame_num][player_id]['team'] = team\n            tracks['players'][frame_num][player_id]['team_color'] = team_assigner.team_colors.get(team, (0,0,255))\n    \n    # --- STAGE 4: GENERATE EVENTS DATA ---\n    print(\"Stage 4: Detecting events for commentary context...\")\n    event_detector = EventDetector()\n    events_df = event_detector.detect_events(tracks)\n    print(f\"Detected {len(events_df)} events for commentary context\")\n    \n    # --- STAGE 5: BALL POSSESSION & COMMENTARY ---\n    print(\"Stage 5: Tracking ball possession and generating all commentary...\")\n    team_ball_control = []\n    ticker_history = []\n    gemini_history = []\n    \n    for frame_num, frame in enumerate(frames):\n        player_track = tracks['players'][frame_num]\n        ball_bbox = tracks['ball'][frame_num].get(1, {}).get('bbox')\n        \n        for player_id in tracks['players'][frame_num]:\n            tracks['players'][frame_num][player_id]['has_ball'] = False\n            \n        assigned_player = player_assigner.assign_ball_to_player(player_track, ball_bbox)\n        if assigned_player != -1:\n            tracks['players'][frame_num][assigned_player]['has_ball'] = True\n            team_ball_control.append(tracks['players'][frame_num][assigned_player]['team'])\n        else:\n            team_ball_control.append(team_ball_control[-1] if team_ball_control else 0)\n        \n        ticker_history.append(ticker.update(tracks, frame_num))\n        commentary_engine.update_with_context(frame, tracks, frame_num, events_df)\n        gemini_history.append(commentary_engine.latest_commentary)\n        \n        if frame_num % 100 == 0:\n            print(f\"Commentary progress: {frame_num}/{len(frames)} frames\")\n\n    team_ball_control = np.array(team_ball_control)\n\n    # --- STAGE 6: VISUALIZATION & SAVING ---\n    print(\"Stage 6: Combining commentary and saving final video...\")\n    display_commentary = ticker_history.copy()\n    last_gemini_comment = gemini_history[0]\n    for i, comment in enumerate(gemini_history):\n        if comment != last_gemini_comment:\n            start_frame = max(0, i - commentary_engine.clip_length_frames)\n            for j in range(start_frame, i):\n                if j < len(display_commentary):\n                    display_commentary[j] = comment\n            last_gemini_comment = comment\n\n    output_frames = []\n    for frame_num, frame in enumerate(frames):\n        frame_copy = frame.copy()\n        current_commentary = display_commentary[frame_num] if frame_num < len(display_commentary) else \" \"\n        \n        player_dict = tracks[\"players\"][frame_num]\n        ball_dict = tracks.get(\"ball\", [])[frame_num]\n        \n        for track_id, player in player_dict.items():\n            color = player.get(\"team_color\", (0, 0, 255))\n            frame_copy = tracker._draw_player_ellipse(frame_copy, player[\"bbox\"], color, track_id, player.get(\"jersey_number\"))\n            if player.get('has_ball', False):\n                frame_copy = tracker._draw_triangle(frame_copy, player[\"bbox\"], (0, 0, 255))\n        \n        if 1 in ball_dict:\n            frame_copy = tracker._draw_triangle(frame_copy, ball_dict[1][\"bbox\"], (0, 255, 0))\n            \n        frame_copy = tracker._draw_team_ball_control(frame_copy, frame_num, team_ball_control)\n        frame_copy = tracker._draw_commentary_overlay(frame_copy, current_commentary)\n        output_frames.append(frame_copy)\n    \n    output_frames = speed_estimator.draw_speed_and_distance(output_frames, tracks)\n    save_video(output_frames, OUTPUT_VIDEO_PATH)\n\n    # --- FINAL STATISTICS ---\n    print(\"\\n\" + \"=\"*50)\n    print(\"MATCH ANALYSIS COMPLETE\")\n    print(\"=\"*50)\n    print(f\"âœ… Video saved to: {OUTPUT_VIDEO_PATH}\")\n    \n    # ... (rest of main function)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BLIP","metadata":{}},{"cell_type":"code","source":"# !pip install ultralytics supervision numpy opencv-python scikit-learn pandas\n# !pip install --upgrade ultralytics torch torchvision\n# !pip install mplsoccer transformers accelerate easyocr\n# !pip install Pillow timm datasets diffusers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T09:27:13.246695Z","iopub.execute_input":"2025-08-26T09:27:13.246981Z","iopub.status.idle":"2025-08-26T09:27:28.046793Z","shell.execute_reply.started":"2025-08-26T09:27:13.246959Z","shell.execute_reply":"2025-08-26T09:27:28.045837Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.186)\nRequirement already satisfied: supervision in /usr/local/lib/python3.11/dist-packages (0.26.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.4)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.8.0)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.23.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.16)\nRequirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from supervision) (0.7.1)\nRequirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.11/dist-packages (from supervision) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.8.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.3.3.83)\nRequirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.9.90)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.7.3.90)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.5.8.93)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.8.90)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.8.93)\nRequirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1.3)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch>=1.8.0->ultralytics) (75.2.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.186)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.23.0)\nRequirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.4)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.16)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.93)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.83)\nRequirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.90)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.3.90)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.8.93)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.93)\nRequirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1.3)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch) (75.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.0->ultralytics) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: mplsoccer in /usr/local/lib/python3.11/dist-packages (1.5.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.4)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.10.1)\nRequirement already satisfied: easyocr in /usr/local/lib/python3.11/dist-packages (1.7.2)\nRequirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mplsoccer) (3.7.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mplsoccer) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from mplsoccer) (2.2.3)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from mplsoccer) (11.2.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from mplsoccer) (2.32.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from mplsoccer) (1.15.3)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from mplsoccer) (0.12.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.8.0)\nRequirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.23.0)\nRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from easyocr) (4.11.0.86)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.25.2)\nRequirement already satisfied: python-bidi in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.6.6)\nRequirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.1.1)\nRequirement already satisfied: pyclipper in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.3.0.post6)\nRequirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.11.1.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mplsoccer) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mplsoccer) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mplsoccer) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mplsoccer) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mplsoccer) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mplsoccer) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->mplsoccer) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->mplsoccer) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->mplsoccer) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->mplsoccer) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->mplsoccer) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->mplsoccer) (2.4.1)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\nRequirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\nRequirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch>=2.0.0->accelerate) (75.2.0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->mplsoccer) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->mplsoccer) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->mplsoccer) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->mplsoccer) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->mplsoccer) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->mplsoccer) (2025.6.15)\nRequirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2.37.0)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2025.6.11)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (0.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mplsoccer) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->mplsoccer) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->mplsoccer) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->mplsoccer) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->mplsoccer) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->mplsoccer) (2024.2.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.34.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.8.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.23.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.34.4)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.4)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.7.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers) (3.23.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.8.93)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.8.90)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.8.90)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.8.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.3.3.83)\nRequirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.9.90)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.7.3.90)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.5.8.93)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.8.90)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.8.93)\nRequirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1.3)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.0)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch->timm) (75.2.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# !pip install paddlepaddle paddleocr\n# !pip install torchvision-nightly --pre\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T09:27:28.048357Z","iopub.execute_input":"2025-08-26T09:27:28.048674Z","iopub.status.idle":"2025-08-26T09:27:59.637585Z","shell.execute_reply.started":"2025-08-26T09:27:28.048650Z","shell.execute_reply":"2025-08-26T09:27:59.636500Z"}},"outputs":[{"name":"stdout","text":"Collecting paddlepaddle\n  Downloading paddlepaddle-3.1.1-cp311-cp311-manylinux1_x86_64.whl.metadata (8.8 kB)\nCollecting paddleocr\n  Downloading paddleocr-3.2.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (0.28.1)\nRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (1.26.4)\nRequirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (3.20.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (11.2.1)\nCollecting opt_einsum==3.3.0 (from paddlepaddle)\n  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (3.5)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from paddlepaddle) (4.14.0)\nCollecting paddlex<3.3.0,>=3.2.0 (from paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr)\n  Downloading paddlex-3.2.0-py3-none-any.whl.metadata (80 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m80.5/80.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML>=6 in /usr/local/lib/python3.11/dist-packages (from paddleocr) (6.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21->paddlepaddle) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21->paddlepaddle) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21->paddlepaddle) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21->paddlepaddle) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21->paddlepaddle) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21->paddlepaddle) (2.4.1)\nCollecting aistudio_sdk>=0.3.5 (from paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr)\n  Downloading aistudio_sdk-0.3.5-py3-none-any.whl.metadata (1.0 kB)\nRequirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (5.2.0)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (6.9.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (3.18.0)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (0.34.4)\nCollecting modelscope>=1.28.0 (from paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr)\n  Downloading modelscope-1.29.1-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (25.0)\nRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.11/dist-packages (from paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (2.2.3)\nRequirement already satisfied: prettytable in /usr/local/lib/python3.11/dist-packages (from paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (3.16.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (9.0.0)\nRequirement already satisfied: pydantic>=2 in /usr/local/lib/python3.11/dist-packages (from paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (2.11.7)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (2.32.4)\nCollecting ruamel.yaml (from paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr)\n  Downloading ruamel.yaml-0.18.15-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (5.10.0)\nRequirement already satisfied: imagesize in /usr/local/lib/python3.11/dist-packages (from paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (1.4.1)\nCollecting opencv-contrib-python==4.10.0.84 (from paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr)\n  Downloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: pyclipper in /usr/local/lib/python3.11/dist-packages (from paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (1.3.0.post6)\nCollecting pypdfium2>=4 (from paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr)\n  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (2.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->paddlepaddle) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->paddlepaddle) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->paddlepaddle) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->paddlepaddle) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->paddlepaddle) (0.16.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from aistudio_sdk>=0.3.5->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (4.67.1)\nCollecting bce-python-sdk (from aistudio_sdk>=0.3.5->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr)\n  Downloading bce_python_sdk-0.9.42-py3-none-any.whl.metadata (416 bytes)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from aistudio_sdk>=0.3.5->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (8.2.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from modelscope>=1.28.0->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (75.2.0)\nRequirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.11/dist-packages (from modelscope>=1.28.0->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (2.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (3.4.2)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->paddlepaddle) (1.3.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (2025.3.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (1.1.5)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21->paddlepaddle) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21->paddlepaddle) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21->paddlepaddle) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21->paddlepaddle) (2024.2.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (0.2.13)\nCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr)\n  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21->paddlepaddle) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (1.17.0)\nRequirement already satisfied: pycryptodome>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from bce-python-sdk->aistudio_sdk>=0.3.5->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (3.23.0)\nRequirement already satisfied: future>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from bce-python-sdk->aistudio_sdk>=0.3.5->paddlex<3.3.0,>=3.2.0->paddlex[ocr-core]<3.3.0,>=3.2.0->paddleocr) (1.0.0)\nDownloading paddlepaddle-3.1.1-cp311-cp311-manylinux1_x86_64.whl (187.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m187.5/187.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading paddleocr-3.2.0-py3-none-any.whl (75 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading paddlex-3.2.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.7/68.7 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading aistudio_sdk-0.3.5-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading modelscope-1.29.1-py3-none-any.whl (5.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading ruamel.yaml-0.18.15-py3-none-any.whl (119 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bce_python_sdk-0.9.42-py3-none-any.whl (351 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m351.6/351.6 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ruamel.yaml.clib, pypdfium2, bce-python-sdk, ruamel.yaml, modelscope, aistudio_sdk, paddlex, opencv-contrib-python, opt_einsum, paddlepaddle, paddleocr\n  Attempting uninstall: opencv-contrib-python\n    Found existing installation: opencv-contrib-python 4.11.0.86\n    Uninstalling opencv-contrib-python-4.11.0.86:\n      Successfully uninstalled opencv-contrib-python-4.11.0.86\n  Attempting uninstall: opt_einsum\n    Found existing installation: opt_einsum 3.4.0\n    Uninstalling opt_einsum-3.4.0:\n      Successfully uninstalled opt_einsum-3.4.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aistudio_sdk-0.3.5 bce-python-sdk-0.9.42 modelscope-1.29.1 opencv-contrib-python-4.10.0.84 opt_einsum-3.3.0 paddleocr-3.2.0 paddlepaddle-3.1.1 paddlex-3.2.0 pypdfium2-4.30.0 ruamel.yaml-0.18.15 ruamel.yaml.clib-0.2.12\n\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision-nightly (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision-nightly\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# import sys\n# import os\n# import cv2\n# import pickle\n# import pandas as pd\n# import numpy as np\n# import matplotlib.pyplot as plt\n# from collections import deque\n# import time\n# import math\n# from typing import Dict, List, Tuple, Optional\n\n# # Machine Learning & Computer Vision Libraries\n# from ultralytics import YOLO\n# import supervision as sv\n# from sklearn.cluster import KMeans\n# import easyocr\n# try:\n#     from paddleocr import PaddleOCR\n# except ImportError:\n#     PaddleOCR = None\n\n# # Advanced Vision Models\n# from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\n# from transformers import BlipProcessor, BlipForConditionalGeneration\n# from transformers import Blip2Processor, Blip2ForConditionalGeneration\n# import torch\n# from PIL import Image, ImageEnhance, ImageFilter\n\n# # Plotting for Heatmaps\n# from mplsoccer import Pitch\n\n# # --- Enhanced Video Utilities ---\n# def read_video(video_path):\n#     \"\"\"Reads a video file and returns a list of its frames.\"\"\"\n#     cap = cv2.VideoCapture(video_path)\n#     frames = []\n#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n#     while True:\n#         ret, frame = cap.read()\n#         if not ret:\n#             break\n#         frames.append(frame)\n    \n#     cap.release()\n#     print(f\"âœ… Loaded {len(frames)}/{total_frames} frames from video\")\n#     return frames\n\n# def save_video(output_video_frames, output_video_path, fps=24):\n#     \"\"\"Saves a list of frames as a video file with enhanced quality.\"\"\"\n#     if not output_video_frames:\n#         print(\"No frames to save.\")\n#         return\n    \n#     height, width, channels = output_video_frames[0].shape\n#     fourcc = cv2.VideoWriter_fourcc(*'H264')  # Better quality codec\n#     out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n    \n#     for frame in output_video_frames:\n#         out.write(frame)\n#     out.release()\n#     print(f\"âœ… Video saved to: {output_video_path}\")\n\n# # --- Enhanced BBox Utilities ---\n# def get_center_of_bbox(bbox):\n#     x1, y1, x2, y2 = bbox\n#     return int((x1 + x2) / 2), int((y1 + y2) / 2)\n\n# def get_bbox_width(bbox):\n#     return int(bbox[2] - bbox[0])\n\n# def get_bbox_height(bbox):\n#     return int(bbox[3] - bbox[1])\n\n# def get_bbox_area(bbox):\n#     return get_bbox_width(bbox) * get_bbox_height(bbox)\n\n# def get_foot_position(bbox):\n#     x1, y1, x2, y2 = bbox\n#     return int((x1 + x2) / 2), int(y2)\n \n# def measure_distance(p1, p2):\n#     return ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**0.5\n\n# def measure_xy_distance(p1, p2):\n#     return p1[0] - p2[0], p1[1] - p2[1]\n\n# def calculate_angle(p1, p2):\n#     \"\"\"Calculate angle between two points.\"\"\"\n#     dx = p2[0] - p1[0]\n#     dy = p2[1] - p1[1]\n#     return math.atan2(dy, dx) * 180 / math.pi\n\n# def enhance_image_for_ocr(image):\n#     \"\"\"Enhance image quality for better OCR results.\"\"\"\n#     if len(image.shape) == 3:\n#         gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n#     else:\n#         gray = image.copy()\n    \n#     # Apply multiple enhancement techniques\n#     # 1. Contrast enhancement\n#     clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n#     enhanced = clahe.apply(gray)\n    \n#     # 2. Noise reduction\n#     denoised = cv2.bilateralFilter(enhanced, 9, 75, 75)\n    \n#     # 3. Sharpening\n#     kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n#     sharpened = cv2.filter2D(denoised, -1, kernel)\n    \n#     # 4. Morphological operations to clean up\n#     kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n#     cleaned = cv2.morphologyEx(sharpened, cv2.MORPH_CLOSE, kernel)\n    \n#     return cleaned\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T09:27:59.639566Z","iopub.execute_input":"2025-08-26T09:27:59.639849Z","iopub.status.idle":"2025-08-26T09:28:05.275760Z","shell.execute_reply.started":"2025-08-26T09:27:59.639826Z","shell.execute_reply":"2025-08-26T09:28:05.274971Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# class AdvancedJerseyNumberRecognizer:\n#     def __init__(self):\n#         # Initialize multiple OCR engines for better accuracy\n#         self.easyocr_reader = easyocr.Reader(['en'], gpu=True)\n        \n#         # Initialize PaddleOCR if available\n#         self.paddleocr_reader = None\n#         if PaddleOCR:\n#             try:\n#                 self.paddleocr_reader = PaddleOCR(use_angle_cls=True, lang='en', use_gpu=True)\n#                 print(\"âœ… PaddleOCR initialized successfully\")\n#             except:\n#                 print(\"âš ï¸ PaddleOCR initialization failed, using EasyOCR only\")\n        \n#         self.jersey_cache = {}\n#         self.confidence_threshold = 0.5\n#         self.number_history = {}  # Track number consistency\n#         print(\"âœ… Advanced Jersey OCR module initialized.\")\n\n#     def preprocess_jersey_crop(self, player_crop):\n#         \"\"\"Enhanced preprocessing for jersey number detection.\"\"\"\n#         if player_crop.size == 0:\n#             return None, None\n            \n#         # Focus on upper torso area where jersey numbers typically are\n#         height, width = player_crop.shape[:2]\n        \n#         # Multiple crop regions to try\n#         regions = [\n#             # Center chest area\n#             (int(width*0.2), int(height*0.1), int(width*0.8), int(height*0.6)),\n#             # Front chest area\n#             (int(width*0.3), int(height*0.15), int(width*0.7), int(height*0.5)),\n#             # Full upper body\n#             (0, 0, width, int(height*0.7))\n#         ]\n        \n#         processed_regions = []\n#         for x1, y1, x2, y2 in regions:\n#             if x2 > x1 and y2 > y1 and x2 <= width and y2 <= height:\n#                 region = player_crop[y1:y2, x1:x2]\n#                 if region.size > 0:\n#                     enhanced = enhance_image_for_ocr(region)\n#                     processed_regions.append(enhanced)\n        \n#         return processed_regions\n\n#     def recognize_with_easyocr(self, image_regions):\n#         \"\"\"Use EasyOCR to detect jersey numbers.\"\"\"\n#         best_result = None\n#         best_confidence = 0\n        \n#         for region in image_regions:\n#             try:\n#                 results = self.easyocr_reader.readtext(\n#                     region, \n#                     allowlist='0123456789', \n#                     detail=1,\n#                     width_ths=0.7,\n#                     height_ths=0.7\n#                 )\n                \n#                 for (bbox, text, confidence) in results:\n#                     # Validate jersey number format\n#                     if (confidence > self.confidence_threshold and \n#                         text.isdigit() and \n#                         1 <= len(text) <= 2 and\n#                         1 <= int(text) <= 99):\n                        \n#                         if confidence > best_confidence:\n#                             best_confidence = confidence\n#                             best_result = text\n                            \n#             except Exception as e:\n#                 continue\n                \n#         return best_result, best_confidence\n\n#     def recognize_with_paddleocr(self, image_regions):\n#         \"\"\"Use PaddleOCR to detect jersey numbers.\"\"\"\n#         if not self.paddleocr_reader:\n#             return None, 0\n            \n#         best_result = None\n#         best_confidence = 0\n        \n#         for region in image_regions:\n#             try:\n#                 results = self.paddleocr_reader.ocr(region, cls=True)\n                \n#                 if results and results[0]:\n#                     for line in results[0]:\n#                         if len(line) == 2:\n#                             bbox, (text, confidence) = line\n                            \n#                             # Clean and validate text\n#                             text = ''.join(filter(str.isdigit, text))\n                            \n#                             if (confidence > self.confidence_threshold and \n#                                 text.isdigit() and \n#                                 1 <= len(text) <= 2 and\n#                                 1 <= int(text) <= 99):\n                                \n#                                 if confidence > best_confidence:\n#                                     best_confidence = confidence\n#                                     best_result = text\n                                    \n#             except Exception as e:\n#                 continue\n                \n#         return best_result, best_confidence\n\n#     def validate_number_consistency(self, tracker_id, detected_number):\n#         \"\"\"Validate number consistency across frames.\"\"\"\n#         if tracker_id not in self.number_history:\n#             self.number_history[tracker_id] = {}\n            \n#         if detected_number in self.number_history[tracker_id]:\n#             self.number_history[tracker_id][detected_number] += 1\n#         else:\n#             self.number_history[tracker_id][detected_number] = 1\n            \n#         # Return most frequent number if we have enough samples\n#         total_detections = sum(self.number_history[tracker_id].values())\n#         if total_detections >= 3:\n#             most_common = max(self.number_history[tracker_id], \n#                             key=self.number_history[tracker_id].get)\n#             frequency = self.number_history[tracker_id][most_common] / total_detections\n            \n#             if frequency >= 0.6:  # 60% consistency threshold\n#                 return most_common\n                \n#         return detected_number\n\n#     def recognize_jersey_number(self, player_crop, tracker_id, frame_num=0):\n#         \"\"\"Enhanced jersey number recognition with multiple OCR engines.\"\"\"\n        \n#         # Check cache first\n#         if tracker_id in self.jersey_cache:\n#             cached_result = self.jersey_cache[tracker_id]\n#             # Re-validate periodically\n#             if frame_num % 60 != 0:  # Re-check every 60 frames\n#                 return cached_result\n        \n#         # Preprocess the crop\n#         image_regions = self.preprocess_jersey_crop(player_crop)\n#         if not image_regions:\n#             return f\"P{tracker_id}\"\n            \n#         # Try multiple OCR engines\n#         results = []\n        \n#         # EasyOCR\n#         easy_result, easy_conf = self.recognize_with_easyocr(image_regions)\n#         if easy_result:\n#             results.append((easy_result, easy_conf, \"EasyOCR\"))\n            \n#         # PaddleOCR\n#         paddle_result, paddle_conf = self.recognize_with_paddleocr(image_regions)\n#         if paddle_result:\n#             results.append((paddle_result, paddle_conf, \"PaddleOCR\"))\n        \n#         # Select best result\n#         if results:\n#             # Sort by confidence\n#             results.sort(key=lambda x: x[1], reverse=True)\n#             best_number = results[0][0]\n            \n#             # Validate consistency\n#             validated_number = self.validate_number_consistency(tracker_id, best_number)\n            \n#             # Cache the result\n#             self.jersey_cache[tracker_id] = validated_number\n#             return validated_number\n        \n#         # Return player ID if no number detected\n#         return f\"P{tracker_id}\"\n\n#     def get_detection_stats(self):\n#         \"\"\"Get statistics about jersey number detection.\"\"\"\n#         total_tracked = len(self.number_history)\n#         successful_detections = len([k for k, v in self.jersey_cache.items() \n#                                    if not v.startswith('P')])\n        \n#         return {\n#             'total_players': total_tracked,\n#             'successful_detections': successful_detections,\n#             'detection_rate': successful_detections / max(1, total_tracked) * 100\n#         }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T09:28:11.301406Z","iopub.execute_input":"2025-08-26T09:28:11.303155Z","iopub.status.idle":"2025-08-26T09:28:11.333401Z","shell.execute_reply.started":"2025-08-26T09:28:11.303114Z","shell.execute_reply":"2025-08-26T09:28:11.332391Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# class AdvancedCommentaryEngine:\n#     def __init__(self, clip_duration_seconds=3, fps=24):\n#         self.clip_length_frames = int(clip_duration_seconds * fps)\n#         self.frame_buffer = deque(maxlen=self.clip_length_frames)\n#         self.latest_commentary = \"Match analysis is starting...\"\n#         self.fps = fps\n#         self.frame_count = 0\n        \n#         # Enhanced context tracking\n#         self.match_context = {\n#             'possession_changes': [], \n#             'recent_events': [],\n#             'ball_position_history': deque(maxlen=30),\n#             'player_movements': {},\n#             'formation_changes': [],\n#             'speed_events': [],\n#             'tactical_events': []\n#         }\n        \n#         print(\"ðŸŽ™ï¸ Initializing Advanced Vision Commentary Engine...\")\n        \n#         # Initialize multiple models for different aspects\n#         self.models = {}\n#         self._initialize_models()\n        \n#     def _initialize_models(self):\n#         \"\"\"Initialize multiple vision-language models for comprehensive analysis.\"\"\"\n#         try:\n#             # Primary captioning model - BLIP2 for detailed scene understanding\n#             print(\"Loading BLIP2 for detailed scene analysis...\")\n#             self.blip2_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n#             self.blip2_model = Blip2ForConditionalGeneration.from_pretrained(\n#                 \"Salesforce/blip2-opt-2.7b\",\n#                 torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n#                 device_map=\"auto\" if torch.cuda.is_available() else None\n#             )\n#             self.models['blip2'] = True\n#             print(\"âœ… BLIP2 model loaded successfully\")\n            \n#         except Exception as e:\n#             print(f\"âš ï¸ Failed to load BLIP2: {e}\")\n#             self.models['blip2'] = False\n            \n#         try:\n#             # Secondary model - BLIP for quick analysis\n#             print(\"Loading BLIP for quick analysis...\")\n#             self.blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n#             self.blip_model = BlipForConditionalGeneration.from_pretrained(\n#                 \"Salesforce/blip-image-captioning-base\",\n#                 torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n#             )\n#             if torch.cuda.is_available():\n#                 self.blip_model = self.blip_model.cuda()\n#             self.models['blip'] = True\n#             print(\"âœ… BLIP model loaded successfully\")\n            \n#         except Exception as e:\n#             print(f\"âš ï¸ Failed to load BLIP: {e}\")\n#             self.models['blip'] = False\n            \n#         self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n#     def _analyze_tactical_situation(self, tracks_data, frame_num):\n#         \"\"\"Analyze tactical situation from tracking data.\"\"\"\n#         tactical_info = {\n#             'formation_analysis': None,\n#             'pressure_zones': [],\n#             'attacking_patterns': None,\n#             'defensive_shape': None\n#         }\n        \n#         try:\n#             player_positions = []\n#             team_positions = {1: [], 2: []}\n            \n#             for player_id, player_data in tracks_data['players'][frame_num].items():\n#                 if 'position_transformed' in player_data and player_data['position_transformed']:\n#                     pos = player_data['position_transformed']\n#                     team = player_data.get('team', 0)\n                    \n#                     if team in [1, 2]:\n#                         team_positions[team].append(pos)\n                        \n#             # Analyze team formations\n#             for team_id, positions in team_positions.items():\n#                 if len(positions) >= 7:  # Need enough players for formation analysis\n#                     # Simple formation detection based on y-coordinates\n#                     y_coords = [pos[1] for pos in positions]\n#                     y_coords.sort()\n                    \n#                     # Detect defensive, midfield, and attacking lines\n#                     defensive_line = np.mean(y_coords[:3]) if len(y_coords) >= 3 else None\n#                     midfield_line = np.mean(y_coords[3:6]) if len(y_coords) >= 6 else None\n#                     attacking_line = np.mean(y_coords[6:]) if len(y_coords) > 6 else None\n                    \n#                     tactical_info[f'team_{team_id}_formation'] = {\n#                         'defensive_line': defensive_line,\n#                         'midfield_line': midfield_line,\n#                         'attacking_line': attacking_line\n#                     }\n                    \n#         except Exception as e:\n#             print(f\"Tactical analysis error: {e}\")\n            \n#         return tactical_info\n\n#     def _detect_micro_events(self, tracks_data, frame_num):\n#         \"\"\"Detect micro-events from tracking data.\"\"\"\n#         events = []\n        \n#         try:\n#             # Ball speed analysis\n#             if frame_num > 0:\n#                 current_ball = tracks_data['ball'][frame_num].get(1, {})\n#                 prev_ball = tracks_data['ball'][frame_num-1].get(1, {})\n                \n#                 if (current_ball.get('position_transformed') and \n#                     prev_ball.get('position_transformed')):\n                    \n#                     ball_speed = measure_distance(\n#                         current_ball['position_transformed'],\n#                         prev_ball['position_transformed']\n#                     ) * self.fps  # Speed in units/second\n                    \n#                     if ball_speed > 15:  # High speed threshold\n#                         events.append(f\"Fast ball movement detected (speed: {ball_speed:.1f})\")\n#                     elif ball_speed < 0.5:  # Very slow/stationary\n#                         events.append(\"Ball nearly stationary\")\n            \n#             # Player clustering analysis\n#             player_positions = []\n#             for player_id, player_data in tracks_data['players'][frame_num].items():\n#                 if 'position_transformed' in player_data and player_data['position_transformed']:\n#                     player_positions.append(player_data['position_transformed'])\n            \n#             if len(player_positions) > 6:\n#                 # Detect player clusters\n#                 from sklearn.cluster import DBSCAN\n#                 clustering = DBSCAN(eps=5, min_samples=3).fit(player_positions)\n#                 n_clusters = len(set(clustering.labels_)) - (1 if -1 in clustering.labels_ else 0)\n                \n#                 if n_clusters >= 3:\n#                     events.append(f\"Multiple player clusters detected ({n_clusters} groups)\")\n#                 elif n_clusters == 1:\n#                     events.append(\"Players tightly clustered together\")\n                    \n#         except Exception as e:\n#             pass  # Silently handle micro-event detection errors\n            \n#         return events\n\n#     def update_with_context(self, frame, tracks_data, frame_num, events_data=None):\n#         \"\"\"Enhanced context update with micro-event detection.\"\"\"\n#         if not any(self.models.values()):\n#             return\n            \n#         self.frame_count += 1\n        \n#         # Extract enhanced game context\n#         game_context = self._extract_enhanced_game_context(tracks_data, frame_num, events_data)\n        \n#         # Detect tactical situation\n#         tactical_info = self._analyze_tactical_situation(tracks_data, frame_num)\n#         game_context['tactical_analysis'] = tactical_info\n        \n#         # Detect micro-events\n#         micro_events = self._detect_micro_events(tracks_data, frame_num)\n#         game_context['micro_events'] = micro_events\n        \n#         # Update context history\n#         self.match_context['recent_events'].append(game_context)\n#         if len(self.match_context['recent_events']) > 15:\n#             self.match_context['recent_events'].pop(0)\n        \n#         # Add frame to buffer\n#         self.frame_buffer.append(frame)\n        \n#         # Generate commentary when buffer is full or at regular intervals\n#         if (len(self.frame_buffer) == self.clip_length_frames or \n#             self.frame_count % 60 == 0):  # Every 60 frames as backup\n            \n#             print(f\"Generating enhanced commentary... (Frame {frame_num})\")\n#             new_comment = self._generate_enhanced_commentary(game_context)\n#             if new_comment:\n#                 self.latest_commentary = new_comment\n            \n#             # Clear buffer periodically to prevent memory issues\n#             if len(self.frame_buffer) >= self.clip_length_frames:\n#                 self.frame_buffer.clear()\n\n#     def _extract_enhanced_game_context(self, tracks_data, frame_num, events_data):\n#         \"\"\"Extract comprehensive game context.\"\"\"\n#         context = {\n#             'frame_num': frame_num,\n#             'timestamp': f\"{int(frame_num / (self.fps * 60))}:{int((frame_num / self.fps) % 60):02d}\",\n#             'players_detected': len(tracks_data['players'][frame_num]),\n#             'ball_detected': 1 in tracks_data['ball'][frame_num],\n#             'possession': None,\n#             'ball_speed': 0,\n#             'recent_events': [],\n#             'player_speeds': {},\n#             'formation_info': {},\n#             'pressure_areas': []\n#         }\n        \n#         # Enhanced possession analysis\n#         max_speed = 0\n#         for player_id, player_info in tracks_data['players'][frame_num].items():\n#             if player_info.get('has_ball', False):\n#                 context['possession'] = {\n#                     'player_id': player_id,\n#                     'team': player_info.get('team', 'Unknown'),\n#                     'position': player_info.get('position_transformed'),\n#                     'jersey_number': player_info.get('jersey_number', f'P{player_id}')\n#                 }\n#                 break\n            \n#             # Track player speeds\n#             speed = player_info.get('speed', 0)\n#             if speed > max_speed:\n#                 max_speed = speed\n#             context['player_speeds'][player_id] = speed\n        \n#         context['max_player_speed'] = max_speed\n        \n#         # Ball position history for movement analysis\n#         if 1 in tracks_data['ball'][frame_num]:\n#             ball_pos = tracks_data['ball'][frame_num][1].get('position_transformed')\n#             if ball_pos:\n#                 self.match_context['ball_position_history'].append(ball_pos)\n        \n#         # Recent events from event detector\n#         if events_data is not None and not events_data.empty:\n#             recent_events = events_data[\n#                 (events_data['minute'] * 60 + events_data['second']) >= (frame_num / self.fps - 15)\n#             ].tail(5)\n#             context['recent_events'] = recent_events.to_dict('records')\n        \n#         return context\n\n#     def _create_enhanced_prompt(self, context, micro_events):\n#         \"\"\"Create comprehensive prompt for detailed analysis.\"\"\"\n#         possession_info = \"Ball possession unclear\"\n#         if context.get('possession'):\n#             poss = context['possession']\n#             possession_info = f\"Player {poss['jersey_number']} (Team {poss['team']}) has possession\"\n        \n#         # Compile micro-events\n#         micro_event_text = \"; \".join(micro_events[:3]) if micro_events else \"No micro-events detected\"\n        \n#         # Speed information\n#         speed_info = f\"Max player speed: {context.get('max_player_speed', 0):.1f} km/h\"\n        \n#         prompt = f\"\"\"You are an expert football analyst with deep tactical knowledge. Analyze this football match scene in detail.\n\n# CURRENT SITUATION:\n# - Match Time: {context['timestamp']}\n# - {possession_info}\n# - Players on field: {context['players_detected']}\n# - {speed_info}\n# - Micro-events: {micro_event_text}\n# - Recent match events: {self._format_recent_events(context.get('recent_events', []))}\n\n# ANALYSIS REQUIREMENTS:\n# 1. Describe the immediate tactical situation\n# 2. Identify any significant player movements, formations, or patterns\n# 3. Note any pressing, attacking moves, defensive actions, or transitions\n# 4. Mention speed of play, player positioning, and ball movement\n# 5. Focus on tactical elements: space utilization, player roles, team shape\n# 6. Include jersey numbers when mentioning specific players\n# 7. Be concise but comprehensive (max 35 words)\n\n# Provide your tactical analysis:\"\"\"\n        \n#         return prompt\n\n#     def _generate_enhanced_commentary(self, game_context):\n#         \"\"\"Generate detailed commentary using multiple models.\"\"\"\n#         if not self.frame_buffer:\n#             return self._generate_fallback_commentary(game_context)\n            \n#         try:\n#             # Use the middle frame for analysis\n#             middle_frame_idx = len(self.frame_buffer) // 2\n#             frame = self.frame_buffer[middle_frame_idx]\n            \n#             # Convert to PIL Image\n#             frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n#             pil_image = Image.fromarray(frame_rgb)\n            \n#             # Enhance image quality\n#             enhancer = ImageEnhance.Contrast(pil_image)\n#             pil_image = enhancer.enhance(1.2)\n#             enhancer = ImageEnhance.Sharpness(pil_image)\n#             pil_image = enhancer.enhance(1.1)\n            \n#             # Get micro-events\n#             micro_events = game_context.get('micro_events', [])\n            \n#             # Generate commentary using available models\n#             commentary_parts = []\n            \n#             # Try BLIP2 first for detailed analysis\n#             if self.models.get('blip2', False):\n#                 try:\n#                     prompt = self._create_enhanced_prompt(game_context, micro_events)\n                    \n#                     inputs = self.blip2_processor(pil_image, prompt, return_tensors=\"pt\")\n#                     if torch.cuda.is_available():\n#                         inputs = {k: v.cuda() for k, v in inputs.items()}\n                    \n#                     with torch.no_grad():\n#                         outputs = self.blip2_model.generate(\n#                             **inputs,\n#                             max_new_tokens=50,\n#                             do_sample=True,\n#                             temperature=0.7,\n#                             top_p=0.9,\n#                             num_beams=3\n#                         )\n                    \n#                     commentary = self.blip2_processor.decode(outputs[0], skip_special_tokens=True)\n#                     # Clean up the output\n#                     commentary = commentary.replace(prompt, \"\").strip()\n#                     if commentary:\n#                         commentary_parts.append(commentary)\n                        \n#                 except Exception as e:\n#                     print(f\"BLIP2 generation error: {e}\")\n            \n#             # Fallback to BLIP\n#             if not commentary_parts and self.models.get('blip', False):\n#                 try:\n#                     inputs = self.blip_processor(pil_image, return_tensors=\"pt\")\n#                     if torch.cuda.is_available():\n#                         inputs = {k: v.cuda() for k, v in inputs.items()}\n                    \n#                     with torch.no_grad():\n#                         outputs = self.blip_model.generate(\n#                             **inputs,\n#                             max_new_tokens=30,\n#                             do_sample=True,\n#                             temperature=0.8\n#                         )\n                    \n#                     caption = self.blip_processor.decode(outputs[0], skip_special_tokens=True)\n                    \n#                     # Enhance caption with context\n#                     if game_context.get('possession'):\n#                         poss = game_context['possession']\n#                         enhanced_caption = f\"{caption}. {poss['jersey_number']} (Team {poss['team']}) has possession.\"\n#                     else:\n#                         enhanced_caption = caption\n                        \n#                     commentary_parts.append(enhanced_caption)\n                    \n#                 except Exception as e:\n#                     print(f\"BLIP generation error: {e}\")\n            \n#             # Combine commentary parts\n#             if commentary_parts:\n#                 final_commentary = \" \".join(commentary_parts)\n#                 # Add micro-events if significant\n#                 if len(micro_events) > 0:\n#                     final_commentary += f\" {micro_events[0]}\"\n                    \n#                 return final_commentary[:150]  # Limit length\n                \n#         except Exception as e:\n#             print(f\"Enhanced commentary generation error: {e}\")\n            \n#         return self._generate_fallback_commentary(game_context)\n\n#     def _format_recent_events(self, events):\n#         \"\"\"Format recent events for prompt.\"\"\"\n#         if not events:\n#             return \"No recent significant events\"\n        \n#         formatted = []\n#         for event in events[-3:]:\n#             if isinstance(event, dict):\n#                 event_type = event.get('type_name', 'Action')\n#                 team = event.get('team_name', 'Team')\n#                 formatted.append(f\"{event_type} by {team}\")\n        \n#         return \"; \".join(formatted) if formatted else \"No recent significant events\"\n\n#     def _generate_fallback_commentary(self, context):\n#         \"\"\"Generate fallback commentary when models fail.\"\"\"\n#         if context.get('possession'):\n#             poss = context['possession']\n#             return f\"Play continues with {poss['jersey_number']} (Team {poss['team']}) controlling the ball.\"\n        \n#         max_speed = context.get('max_player_speed', 0)\n#         if max_speed > 20:\n#             return f\"High-intensity play with players reaching {max_speed:.1f} km/h.\"\n#         elif context.get('players_detected', 0) > 15:\n#             return \"Dense midfield battle with multiple players involved.\"\n#         else:\n#             return \"Match continues with tactical positioning and ball movement.\"\n\n# class RealTimeTicker:\n#     \"\"\"Enhanced real-time ticker with more detailed events.\"\"\"\n#     def __init__(self, fps=24):\n#         self.fps = fps\n#         self.last_player_id = -1\n#         self.last_team_id = -1\n#         self.ticker_text = \"âš½ Match begins!\"\n#         self.text_display_frames = 0\n#         self.event_history = deque(maxlen=10)\n        \n#     def _get_ball_carrier(self, player_track):\n#         for player_id, data in player_track.items():\n#             if data.get('has_ball', False):\n#                 jersey_num = data.get('jersey_number', f'P{player_id}')\n#                 return player_id, data.get('team'), jersey_num\n#         return -1, -1, None\n\n#     def _detect_advanced_events(self, tracks, frame_num):\n#         \"\"\"Detect advanced events like sprints, clusters, etc.\"\"\"\n#         events = []\n        \n#         # Detect high-speed movements\n#         high_speed_players = []\n#         for player_id, player_data in tracks['players'][frame_num].items():\n#             speed = player_data.get('speed', 0)\n#             if speed > 25:  # High speed threshold\n#                 jersey = player_data.get('jersey_number', f'P{player_id}')\n#                 team = player_data.get('team', '?')\n#                 high_speed_players.append(f\"{jersey}(T{team})\")\n        \n#         if high_speed_players:\n#             events.append(f\"ðŸƒ Sprint: {', '.join(high_speed_players[:2])}\")\n        \n#         return events\n\n#     def update(self, tracks, frame_num):\n#         \"\"\"Enhanced ticker update with more event types.\"\"\"\n#         if self.text_display_frames > 0:\n#             self.text_display_frames -= 1\n#             return self.ticker_text\n        \n#         player_track = tracks['players'][frame_num]\n#         current_player_id, current_team_id, current_jersey = self._get_ball_carrier(player_track)\n\n#         # Pass detection\n#         if (current_player_id != -1 and self.last_player_id != -1 and \n#             current_player_id != self.last_player_id and current_team_id == self.last_team_id):\n            \n#             last_jersey = None\n#             if self.last_player_id in player_track:\n#                 last_jersey = player_track[self.last_player_id].get('jersey_number', f'P{self.last_player_id}')\n            \n#             self.ticker_text = f\"âš½ Pass: {last_jersey or self.last_player_id} â†’ {current_jersey} (Team {current_team_id})\"\n#             self.text_display_frames = self.fps * 2\n        \n#         # Possession change\n#         elif current_player_id != -1 and self.last_team_id != -1 and current_team_id != self.last_team_id:\n#             self.ticker_text = f\"ðŸ”„ Team {current_team_id} gains possession! ({current_jersey})\"\n#             self.text_display_frames = self.fps * 3\n        \n#         # Advanced events\n#         else:\n#             advanced_events = self._detect_advanced_events(tracks, frame_num)\n#             if advanced_events:\n#                 self.ticker_text = advanced_events[0]\n#                 self.text_display_frames = self.fps * 2\n#             elif current_player_id != -1:\n#                 self.ticker_text = f\"âš½ {current_jersey} (Team {current_team_id}) on the ball\"\n#             else:\n#                 self.ticker_text = \"ðŸ” Ball is loose\"\n\n#         # Update tracking\n#         if current_player_id != -1:\n#             self.last_player_id = current_player_id\n#             self.last_team_id = current_team_id\n#         else:\n#             self.last_player_id = -1\n        \n#         return self.ticker_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T09:29:57.932834Z","iopub.execute_input":"2025-08-26T09:29:57.933410Z","iopub.status.idle":"2025-08-26T09:29:58.470605Z","shell.execute_reply.started":"2025-08-26T09:29:57.933388Z","shell.execute_reply":"2025-08-26T09:29:58.469970Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# class EnhancedTracker:\n#     def __init__(self, model_name='yolov8x.pt'):\n#         self.model = YOLO(model_name)\n#         self.tracker = sv.ByteTrack()\n#         self.jersey_recognizer = AdvancedJerseyNumberRecognizer()\n#         self.confidence_threshold = 0.3  # Lowered for better detection\n        \n#         # Enhanced tracking parameters\n#         self.tracker_params = {\n#             'track_thresh': 0.25,\n#             'track_buffer': 60,\n#             'match_thresh': 0.8,\n#             'frame_rate': 24\n#         }\n\n#     def get_object_tracks(self, frames, read_from_stub=False, stub_path=None):\n#         \"\"\"Enhanced object tracking with better detection.\"\"\"\n#         if read_from_stub and stub_path and os.path.exists(stub_path):\n#             with open(stub_path, 'rb') as f:\n#                 print(\"ðŸ“ Loading tracks from cache...\")\n#                 return pickle.load(f)\n\n#         tracks = {\"players\": [], \"referees\": [], \"ball\": []}\n        \n#         print(f\"ðŸŽ¯ Starting enhanced tracking on {len(frames)} frames...\")\n        \n#         for frame_num, frame in enumerate(frames):\n#             if frame_num % 50 == 0:\n#                 print(f\"ðŸ“Š Processing frame {frame_num}/{len(frames)} \"\n#                       f\"({frame_num/len(frames)*100:.1f}%)\")\n                \n#             # Enhanced YOLO prediction\n#             results = self.model.predict(\n#                 frame, \n#                 conf=self.confidence_threshold,\n#                 iou=0.7,\n#                 classes=[0, 32],  # person and sports ball\n#                 verbose=False\n#             )[0]\n            \n#             detections = sv.Detections.from_ultralytics(results)\n            \n#             # Enhanced player detection and tracking\n#             player_detections = detections[detections.class_id == 0]\n            \n#             # Filter out very small detections (likely false positives)\n#             if len(player_detections) > 0:\n#                 areas = []\n#                 for i, bbox in enumerate(player_detections.xyxy):\n#                     area = get_bbox_area(bbox)\n#                     areas.append(area)\n                \n#                 # Keep only reasonably sized detections\n#                 min_area = np.median(areas) * 0.3 if areas else 0\n#                 valid_indices = [i for i, area in enumerate(areas) if area >= min_area]\n                \n#                 if valid_indices:\n#                     player_detections = player_detections[valid_indices]\n            \n#             tracked_players = self.tracker.update_with_detections(player_detections)\n            \n#             tracks[\"players\"].append({})\n#             tracks[\"referees\"].append({})\n            \n#             # Process tracked players with enhanced jersey recognition\n#             for detection_data in tracked_players:\n#                 bbox = detection_data[0]\n#                 track_id = detection_data[4]\n                \n#                 # Extract player crop with padding\n#                 x1, y1, x2, y2 = map(int, bbox)\n                \n#                 # Add padding for better OCR\n#                 padding = 5\n#                 x1 = max(0, x1 - padding)\n#                 y1 = max(0, y1 - padding)\n#                 x2 = min(frame.shape[1], x2 + padding)\n#                 y2 = min(frame.shape[0], y2 + padding)\n                \n#                 player_crop = frame[y1:y2, x1:x2]\n                \n#                 # Enhanced jersey number recognition\n#                 jersey_num = self.jersey_recognizer.recognize_jersey_number(\n#                     player_crop, track_id, frame_num\n#                 )\n                \n#                 tracks[\"players\"][frame_num][track_id] = {\n#                     \"bbox\": bbox.tolist(),\n#                     \"jersey_number\": jersey_num,\n#                     \"detection_confidence\": detection_data[1] if len(detection_data) > 1 else 0.0\n#                 }\n\n#             # Enhanced ball detection\n#             ball_detections = detections[detections.class_id == 32]\n#             tracks[\"ball\"].append({})\n            \n#             if len(ball_detections) > 0:\n#                 # If multiple ball detections, choose the most confident one\n#                 best_idx = np.argmax(ball_detections.confidence)\n#                 best_ball_bbox = ball_detections.xyxy[best_idx]\n#                 best_ball_conf = ball_detections.confidence[best_idx]\n                \n#                 tracks[\"ball\"][frame_num][1] = {\n#                     \"bbox\": best_ball_bbox.tolist(),\n#                     \"confidence\": float(best_ball_conf)\n#                 }\n        \n#         # Print detection statistics\n#         jersey_stats = self.jersey_recognizer.get_detection_stats()\n#         print(f\"ðŸ† Jersey Detection Stats: {jersey_stats['successful_detections']}/{jersey_stats['total_players']} \"\n#               f\"({jersey_stats['detection_rate']:.1f}% success rate)\")\n        \n#         if stub_path:\n#             with open(stub_path, 'wb') as f:\n#                 pickle.dump(tracks, f)\n#             print(f\"ðŸ’¾ Tracks saved to cache: {stub_path}\")\n            \n#         return tracks\n\n#     def add_position_to_tracks(self, tracks):\n#         \"\"\"Add position information to tracks.\"\"\"\n#         for type, obj_tracks in tracks.items():\n#             for frame_num, track in enumerate(obj_tracks):\n#                 for id, info in track.items():\n#                     bbox = info['bbox']\n#                     if type == 'ball':\n#                         info['position'] = get_center_of_bbox(bbox)\n#                     else:\n#                         info['position'] = get_foot_position(bbox)\n    \n#     def interpolate_ball_positions(self, ball_positions):\n#         \"\"\"Enhanced ball position interpolation.\"\"\"\n#         ball_bboxes = []\n#         confidences = []\n        \n#         for frame_data in ball_positions:\n#             if 1 in frame_data:\n#                 ball_bboxes.append(frame_data[1]['bbox'])\n#                 confidences.append(frame_data[1].get('confidence', 1.0))\n#             else:\n#                 ball_bboxes.append([])\n#                 confidences.append(0.0)\n        \n#         df = pd.DataFrame(ball_bboxes, columns=['x1', 'y1', 'x2', 'y2'])\n        \n#         # More sophisticated interpolation\n#         df_interpolated = df.interpolate(method='cubic').bfill().ffill()\n        \n#         # Rebuild ball positions with interpolated data\n#         result = []\n#         for i, (_, row) in enumerate(df_interpolated.iterrows()):\n#             if not row.isna().any():\n#                 result.append({\n#                     1: {\n#                         \"bbox\": row.tolist(),\n#                         \"confidence\": confidences[i],\n#                         \"interpolated\": i >= len(ball_positions) or 1 not in ball_positions[i]\n#                     }\n#                 })\n#             else:\n#                 result.append({})\n                \n#         return result\n\n#     def _draw_enhanced_player_ellipse(self, frame, bbox, color, track_id, jersey_num, has_ball=False):\n#         \"\"\"Enhanced player visualization with more information.\"\"\"\n#         y2 = int(bbox[3])\n#         x_center, _ = get_center_of_bbox(bbox)\n#         width = get_bbox_width(bbox)\n        \n#         # Draw ellipse\n#         ellipse_color = (0, 255, 0) if has_ball else color\n#         cv2.ellipse(\n#             frame, \n#             center=(x_center, y2), \n#             axes=(int(width), int(0.35 * width)), \n#             angle=0.0, \n#             startAngle=-45, \n#             endAngle=235, \n#             color=ellipse_color, \n#             thickness=3, \n#             lineType=cv2.LINE_AA\n#         )\n        \n#         # Enhanced label\n#         if jersey_num and not jersey_num.startswith('P'):\n#             label = f\"#{jersey_num}\"\n#             label_color = (255, 255, 255)\n#         else:\n#             label = f\"ID{track_id}\"\n#             label_color = (200, 200, 200)\n        \n#         # Add ball possession indicator\n#         if has_ball:\n#             label = f\"âš½{label}\"\n        \n#         (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n#         rect_w, rect_h = w + 12, h + 12\n#         x1_rect, y1_rect = x_center - rect_w//2, (y2 - rect_h//2) + 15\n        \n#         # Enhanced label background\n#         cv2.rectangle(frame, (x1_rect, y1_rect), (x1_rect + rect_w, y1_rect + rect_h), \n#                      ellipse_color, cv2.FILLED)\n#         cv2.rectangle(frame, (x1_rect, y1_rect), (x1_rect + rect_w, y1_rect + rect_h), \n#                      (0, 0, 0), 2)\n        \n#         cv2.putText(frame, label, (x1_rect + 6, y1_rect + h + 6), \n#                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, label_color, 2)\n        \n#         return frame\n\n#     def _draw_enhanced_triangle(self, frame, bbox, color, label=\"BALL\"):\n#         \"\"\"Enhanced ball/object visualization.\"\"\"\n#         y, x = int(bbox[1]), int(get_center_of_bbox(bbox)[0])\n        \n#         # Larger, more visible triangle\n#         points = np.array([[x, y], [x - 15, y - 25], [x + 15, y - 25]])\n        \n#         # Draw filled triangle\n#         cv2.drawContours(frame, [points], 0, color, cv2.FILLED)\n#         cv2.drawContours(frame, [points], 0, (0, 0, 0), 2)\n        \n#         # Add label\n#         cv2.putText(frame, label, (x - 20, y - 30), cv2.FONT_HERSHEY_SIMPLEX, \n#                    0.5, (255, 255, 255), 2)\n        \n#         return frame\n\n#     def _draw_enhanced_team_ball_control(self, frame, frame_num, team_ball_control):\n#         \"\"\"Enhanced possession display with additional stats.\"\"\"\n#         overlay = frame.copy()\n        \n#         # Larger, more informative panel\n#         panel_height = 100\n#         cv2.rectangle(overlay, (10, 10), (400, panel_height), (0, 0, 0), -1)\n#         cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)\n        \n#         # Calculate possession percentages\n#         team_1_frames = np.sum(team_ball_control[:frame_num + 1] == 1)\n#         team_2_frames = np.sum(team_ball_control[:frame_num + 1] == 2)\n#         total = max(1, team_1_frames + team_2_frames)\n        \n#         p1 = (team_1_frames / total) * 100\n#         p2 = (team_2_frames / total) * 100\n        \n#         # Enhanced display\n#         cv2.putText(frame, \"âš½ POSSESSION STATS\", (20, 30), \n#                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n        \n#         # Team 1 stats\n#         cv2.putText(frame, f\"Team 1: {p1:.1f}%\", (20, 55), \n#                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 255, 100), 2)\n        \n#         # Team 2 stats  \n#         cv2.putText(frame, f\"Team 2: {p2:.1f}%\", (20, 80), \n#                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 255), 2)\n        \n#         # Add possession bar\n#         bar_width = 300\n#         bar_height = 8\n#         bar_x, bar_y = 20, 90\n        \n#         # Background bar\n#         cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), \n#                      (50, 50, 50), -1)\n        \n#         # Team 1 portion\n#         team1_width = int(bar_width * p1 / 100)\n#         cv2.rectangle(frame, (bar_x, bar_y), (bar_x + team1_width, bar_y + bar_height), \n#                      (100, 255, 100), -1)\n        \n#         # Team 2 portion\n#         team2_width = int(bar_width * p2 / 100)\n#         cv2.rectangle(frame, (bar_x + team1_width, bar_y), \n#                      (bar_x + team1_width + team2_width, bar_y + bar_height), \n#                      (100, 100, 255), -1)\n        \n#         return frame\n\n#     def _draw_enhanced_commentary_overlay(self, frame, text):\n#         \"\"\"Enhanced commentary display with better formatting.\"\"\"\n#         h, w, _ = frame.shape\n#         font = cv2.FONT_HERSHEY_SIMPLEX\n#         thickness = 2\n        \n#         # Calculate optimal font size\n#         font_scale = 0.8\n#         (text_w, text_h), _ = cv2.getTextSize(text, font, font_scale, thickness)\n        \n#         target_w = w * 0.95\n#         while text_w > target_w and font_scale > 0.3:\n#             font_scale -= 0.05\n#             (text_w, text_h), _ = cv2.getTextSize(text, font, font_scale, thickness)\n        \n#         # Enhanced banner\n#         banner_h = text_h + 30\n#         overlay = frame.copy()\n        \n#         # Gradient background effect\n#         for i in range(banner_h):\n#             alpha = 0.8 * (1 - i / banner_h)\n#             cv2.rectangle(overlay, (0, h - banner_h + i), (w, h - banner_h + i + 1), \n#                          (0, 0, 0), -1)\n        \n#         cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)\n        \n#         # Add border\n#         cv2.rectangle(frame, (0, h - banner_h), (w, h), (100, 100, 100), 2)\n        \n#         # Center text\n#         text_x = (w - text_w) // 2\n#         text_y = h - 15\n        \n#         # Add text shadow\n#         cv2.putText(frame, text, (text_x + 2, text_y + 2), font, font_scale, \n#                    (0, 0, 0), thickness + 1)\n        \n#         # Add main text\n#         cv2.putText(frame, text, (text_x, text_y), font, font_scale, \n#                    (255, 255, 255), thickness)\n        \n#         return frame\n\n# # Keep all other existing classes (EventDetector, TeamAssigner, etc.) as they were\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T09:30:09.486113Z","iopub.execute_input":"2025-08-26T09:30:09.486711Z","iopub.status.idle":"2025-08-26T09:30:09.516137Z","shell.execute_reply.started":"2025-08-26T09:30:09.486684Z","shell.execute_reply":"2025-08-26T09:30:09.515404Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# # Enhanced Event Detection\n# class EnhancedEventDetector:\n#     def __init__(self):\n#         self.shot_speed_threshold = 20  # Enhanced threshold\n#         self.pass_speed_threshold = 8\n#         self.frame_rate = 24\n#         self.min_pass_distance = 3  # Minimum distance for pass\n#         self.max_possession_gap = 10  # Frames\n        \n#     def detect_events(self, tracks):\n#         \"\"\"Enhanced event detection with more event types.\"\"\"\n#         player_assigner = PlayerBallAssigner()\n#         ball_possession_log = []\n#         ball_speeds = []\n        \n#         # Track ball possession and speeds\n#         for frame_num in range(len(tracks['players'])):\n#             player_track = tracks['players'][frame_num]\n#             ball_data = tracks['ball'][frame_num].get(1, {})\n#             ball_bbox = ball_data.get('bbox')\n            \n#             assigned_player_id = player_assigner.assign_ball_to_player(player_track, ball_bbox) if ball_bbox else -1\n#             ball_possession_log.append(assigned_player_id)\n            \n#             # Calculate ball speed\n#             if frame_num > 0 and ball_data.get('position_transformed'):\n#                 prev_ball = tracks['ball'][frame_num-1].get(1, {})\n#                 if prev_ball.get('position_transformed'):\n#                     speed = measure_distance(\n#                         ball_data['position_transformed'],\n#                         prev_ball['position_transformed']\n#                     ) * self.frame_rate\n#                     ball_speeds.append(speed)\n#                 else:\n#                     ball_speeds.append(0)\n#             else:\n#                 ball_speeds.append(0)\n\n#         # Detect various events\n#         events = []\n#         last_player_with_ball = -1\n#         pass_start_info = {}\n        \n#         for frame_num, current_player_id in enumerate(ball_possession_log):\n#             ball_data = tracks['ball'][frame_num].get(1, {})\n#             ball_pos = ball_data.get('position_transformed')\n#             ball_speed = ball_speeds[frame_num] if frame_num < len(ball_speeds) else 0\n            \n#             if not ball_pos:\n#                 continue\n                \n#             # Detect passes\n#             is_valid_pass = (current_player_id != last_player_with_ball and \n#                            last_player_with_ball != -1 and \n#                            current_player_id != -1)\n                           \n#             if is_valid_pass and pass_start_info:\n#                 start_frame = pass_start_info.get('frame', frame_num - 1)\n#                 if start_frame in range(len(tracks['players'])):\n#                     start_player_data = tracks['players'][start_frame].get(last_player_with_ball, {})\n#                     end_player_data = tracks['players'][frame_num].get(current_player_id, {})\n                    \n#                     start_team = start_player_data.get('team')\n#                     end_team = end_player_data.get('team')\n                    \n#                     # Same team pass\n#                     if start_team == end_team and start_team is not None:\n#                         pass_distance = measure_distance(pass_start_info['position'], ball_pos)\n                        \n#                         # Classify pass type\n#                         pass_type = \"Pass\"\n#                         if pass_distance > 20:\n#                             pass_type = \"Long Pass\"\n#                         elif ball_speed > 15:\n#                             pass_type = \"Fast Pass\"\n                        \n#                         start_jersey = start_player_data.get('jersey_number', f'P{last_player_with_ball}')\n#                         end_jersey = end_player_data.get('jersey_number', f'P{current_player_id}')\n                        \n#                         events.append({\n#                             \"type_name\": pass_type,\n#                             \"player_name\": start_jersey,\n#                             \"team_name\": f\"Team {start_team}\",\n#                             \"x\": pass_start_info['position'][0],\n#                             \"y\": pass_start_info['position'][1],\n#                             \"end_x\": ball_pos[0],\n#                             \"end_y\": ball_pos[1],\n#                             \"end_player\": end_jersey,\n#                             \"distance\": pass_distance,\n#                             \"speed\": ball_speed,\n#                             \"minute\": int(frame_num / (self.frame_rate * 60)),\n#                             \"second\": int((frame_num / self.frame_rate) % 60)\n#                         })\n            \n#             # Detect shots (high-speed ball movement toward goal)\n#             if ball_speed > self.shot_speed_threshold:\n#                 # Estimate if ball is moving toward goal area\n#                 if ball_pos[1] < 10 or ball_pos[1] > 42:  # Near goal areas\n#                     player_data = tracks['players'][frame_num].get(current_player_id, {}) if current_player_id != -1 else {}\n#                     player_jersey = player_data.get('jersey_number', f'P{current_player_id}' if current_player_id != -1 else 'Unknown')\n#                     team = player_data.get('team', 'Unknown')\n                    \n#                     events.append({\n#                         \"type_name\": \"Shot\",\n#                         \"player_name\": player_jersey,\n#                         \"team_name\": f\"Team {team}\",\n#                         \"x\": ball_pos[0],\n#                         \"y\": ball_pos[1],\n#                         \"speed\": ball_speed,\n#                         \"minute\": int(frame_num / (self.frame_rate * 60)),\n#                         \"second\": int((frame_num / self.frame_rate) % 60)\n#                     })\n            \n#             # Detect possession changes (interceptions)\n#             if (current_player_id != -1 and last_player_with_ball != -1 and\n#                 current_player_id != last_player_with_ball):\n                \n#                 curr_player = tracks['players'][frame_num].get(current_player_id, {})\n#                 prev_player_frame = max(0, frame_num - 5)\n#                 prev_player = tracks['players'][prev_player_frame].get(last_player_with_ball, {})\n                \n#                 curr_team = curr_player.get('team')\n#                 prev_team = prev_player.get('team')\n                \n#                 if curr_team != prev_team and curr_team is not None and prev_team is not None:\n#                     curr_jersey = curr_player.get('jersey_number', f'P{current_player_id}')\n                    \n#                     events.append({\n#                         \"type_name\": \"Interception\",\n#                         \"player_name\": curr_jersey,\n#                         \"team_name\": f\"Team {curr_team}\",\n#                         \"x\": ball_pos[0],\n#                         \"y\": ball_pos[1],\n#                         \"minute\": int(frame_num / (self.frame_rate * 60)),\n#                         \"second\": int((frame_num / self.frame_rate) % 60)\n#                     })\n            \n#             # Update tracking info\n#             if current_player_id != -1:\n#                 pass_start_info = {'frame': frame_num, 'position': ball_pos}\n#                 last_player_with_ball = current_player_id\n        \n#         return pd.DataFrame(events)\n\n# # Keep existing classes with same implementation\n# class TeamAssigner:\n#     def __init__(self):\n#         self.team_colors, self.player_team_dict, self.kmeans = {}, {}, None\n#     def get_player_color(self, frame, bbox):\n#         image = frame[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n#         if image.size == 0: return np.array([0,0,0])\n#         top_half = image[0:int(image.shape[0] / 2), :]\n#         if top_half.size == 0: return np.array([0,0,0])\n#         kmeans = KMeans(n_clusters=2, init=\"k-means++\", n_init=1, random_state=0).fit(top_half.reshape(-1, 3))\n#         labels = kmeans.labels_.reshape(top_half.shape[0], top_half.shape[1])\n#         corner_clusters = [labels[0, 0], labels[0, -1], labels[-1, 0], labels[-1, -1]]\n#         non_player_cluster = max(set(corner_clusters), key=corner_clusters.count)\n#         return kmeans.cluster_centers_[1 - non_player_cluster]\n#     def assign_team_color(self, frame, player_detections):\n#         if not player_detections: return\n#         colors = [self.get_player_color(frame, det[\"bbox\"]) for _, det in player_detections.items()]\n#         self.kmeans = KMeans(n_clusters=2, init=\"k-means++\", n_init=10, random_state=0).fit(colors)\n#         self.team_colors[1], self.team_colors[2] = self.kmeans.cluster_centers_\n#     def get_player_team(self, frame, bbox, player_id):\n#         if player_id in self.player_team_dict: return self.player_team_dict[player_id]\n#         if self.kmeans is None: return 0\n#         color = self.get_player_color(frame, bbox)\n#         team_id = self.kmeans.predict(color.reshape(1, -1))[0] + 1\n#         self.player_team_dict[player_id] = team_id\n#         return team_id\n\n# class PlayerBallAssigner:\n#     def __init__(self): \n#         self.max_dist = 70\n#         self.history = deque(maxlen=10)  # Track assignment history\n        \n#     def assign_ball_to_player(self, players, ball_bbox):\n#         if not ball_bbox: return -1\n#         ball_pos = get_center_of_bbox(ball_bbox)\n#         min_dist = float('inf')\n#         assigned_player = -1\n        \n#         candidates = []\n#         for id, player in players.items():\n#             foot_pos = get_foot_position(player['bbox'])\n#             dist = measure_distance(foot_pos, ball_pos)\n#             if dist < self.max_dist:\n#                 candidates.append((id, dist))\n        \n#         # Sort by distance and choose closest\n#         if candidates:\n#             candidates.sort(key=lambda x: x[1])\n#             assigned_player = candidates[0][0]\n            \n#         self.history.append(assigned_player)\n#         return assigned_player\n\n# class CameraMovementEstimator:\n#     def __init__(self, frame):\n#         self.lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n#         self.features = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n#     def get_camera_movement(self, frames, read_from_stub=False, stub_path=None):\n#         if read_from_stub and stub_path and os.path.exists(stub_path):\n#             with open(stub_path, 'rb') as f: return pickle.load(f)\n#         movements = [[0, 0]] * len(frames)\n#         old_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n#         old_features = cv2.goodFeaturesToTrack(old_gray, **self.features)\n#         for i in range(1, len(frames)):\n#             new_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n#             new_features, status, _ = cv2.calcOpticalFlowPyrLK(old_gray, new_gray, old_features, None, **self.lk_params)\n            \n#             good_new = new_features[status==1]\n#             good_old = old_features[status==1]\n\n#             move_x, move_y = 0, 0\n#             if len(good_new) > 0:\n#                 move_x, move_y = np.mean(good_old - good_new, axis=0).ravel()\n\n#             movements[i] = [move_x, move_y]\n#             old_gray = new_gray.copy()\n#             old_features = good_new.reshape(-1, 1, 2)\n#         if stub_path:\n#             with open(stub_path, 'wb') as f: pickle.dump(movements, f)\n#         return movements\n#     def add_adjust_positions_to_tracks(self, tracks, movements):\n#         for type, obj_tracks in tracks.items():\n#             for i, track in enumerate(obj_tracks):\n#                 for id, info in track.items():\n#                     info['position_adjusted'] = (info['position'][0] + movements[i][0], info['position'][1] + movements[i][1])\n\n# class ViewTransformer:\n#     def __init__(self):\n#         court_w, court_l = 34, 52.5\n#         self.pixel_verts = np.float32([[110, 1035], [265, 275], [910, 260], [1640, 915]])\n#         self.target_verts = np.float32([[0, court_w], [0, 0], [court_l, 0], [court_l, court_w]])\n#         self.transformer = cv2.getPerspectiveTransform(self.pixel_verts, self.target_verts)\n#     def transform_point(self, point):\n#         p = (int(point[0]), int(point[1]))\n#         is_inside = cv2.pointPolygonTest(self.pixel_verts, p, False) >= 0\n#         if not is_inside: return None\n#         reshaped = np.array(point).reshape(-1, 1, 2).astype(np.float32)\n#         transformed = cv2.perspectiveTransform(reshaped, self.transformer)\n#         return transformed.reshape(-1, 2)\n#     def add_transformed_position_to_tracks(self, tracks):\n#         for type, obj_tracks in tracks.items():\n#             for track in obj_tracks:\n#                 for id, info in track.items():\n#                     pos = info.get('position_adjusted', info.get('position'))\n#                     if pos:\n#                         transformed = self.transform_point(pos)\n#                         info['position_transformed'] = transformed.squeeze().tolist() if transformed is not None else None\n\n# class EnhancedSpeedAndDistanceEstimator:\n#     def __init__(self):\n#         self.frame_window = 5  # Smaller window for more responsive speed\n#         self.frame_rate = 24\n#         self.speed_history = {}  # Track speed history for smoothing\n        \n#     def add_speed_and_distance_to_tracks(self, tracks):\n#         \"\"\"Enhanced speed calculation with smoothing.\"\"\"\n#         total_dist = {}\n        \n#         for type, obj_tracks in tracks.items():\n#             if type not in [\"players\", \"referees\"]: continue\n            \n#             for i in range(len(obj_tracks)):\n#                 for id, info in obj_tracks[i].items():\n#                     if i > 0:\n#                         prev_info = tracks[type][i-1].get(id)\n#                         if prev_info and info.get('position_transformed') and prev_info.get('position_transformed'):\n#                             # Calculate distance and speed\n#                             dist = measure_distance(info['position_transformed'], prev_info['position_transformed'])\n#                             total_dist[id] = total_dist.get(id, 0) + dist\n#                             instantaneous_speed = dist * self.frame_rate * 3.6  # km/h\n                            \n#                             # Smooth speed using history\n#                             if id not in self.speed_history:\n#                                 self.speed_history[id] = deque(maxlen=self.frame_window)\n                            \n#                             self.speed_history[id].append(instantaneous_speed)\n#                             smoothed_speed = np.mean(list(self.speed_history[id]))\n                            \n#                             info['speed'] = smoothed_speed\n#                             info['distance'] = total_dist[id]\n#                             info['instantaneous_speed'] = instantaneous_speed\n#                     else:\n#                         info['speed'] = 0\n#                         info['distance'] = 0\n                        \n#     def draw_speed_and_distance(self, frames, tracks):\n#         \"\"\"Enhanced speed visualization.\"\"\"\n#         output_frames = []\n#         for i, frame in enumerate(frames):\n#             for type, obj_tracks in tracks.items():\n#                 if type not in [\"players\", \"referees\"]: continue\n#                 for id, info in obj_tracks[i].items():\n#                     if \"speed\" in info and info['speed'] > 5:  # Only show significant speeds\n#                         x, y = get_foot_position(info['bbox'])\n                        \n#                         # Color code speed\n#                         speed = info['speed']\n#                         if speed > 25:\n#                             color = (0, 0, 255)  # Red for high speed\n#                         elif speed > 15:\n#                             color = (0, 165, 255)  # Orange for medium speed\n#                         else:\n#                             color = (0, 255, 255)  # Yellow for low speed\n                            \n#                         # Enhanced speed display\n#                         speed_text = f\"{speed:.1f}\"\n#                         cv2.putText(frame, speed_text, (x - 15, y + 25), \n#                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n#                         cv2.putText(frame, \"km/h\", (x - 10, y + 40), \n#                                   cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)\n                                  \n#             output_frames.append(frame)\n#         return output_frames\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T09:30:25.582580Z","iopub.execute_input":"2025-08-26T09:30:25.582915Z","iopub.status.idle":"2025-08-26T09:30:25.618692Z","shell.execute_reply.started":"2025-08-26T09:30:25.582892Z","shell.execute_reply":"2025-08-26T09:30:25.618054Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# def enhanced_main():\n#     # --- ENHANCED SETUP ---\n#     INPUT_VIDEO_PATH = \"/kaggle/input/football-video2/CityUtdR.mp4\"\n#     STUB_PATH = \"/kaggle/working/enhanced_tracks_stub.pkl\"\n#     OUTPUT_VIDEO_PATH = \"/kaggle/working/enhanced_football_analysis-blip.mp4\"\n    \n#     print(\"ðŸš€ Starting Enhanced Football Analysis System\")\n#     print(\"=\" * 60)\n    \n#     # Load video\n#     frames = read_video(INPUT_VIDEO_PATH)\n#     if not frames:\n#         print(\"âŒ Video file not found or could not be read. Check the path.\")\n#         return None\n\n#     cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n#     fps = cap.get(cv2.CAP_PROP_FPS) or 24\n#     cap.release()\n#     print(f\"ðŸ“º Video loaded: {len(frames)} frames at {fps} FPS\")\n\n#     # --- INITIALIZE ENHANCED MODULES ---\n#     print(\"\\nðŸ”§ Initializing Enhanced Components...\")\n    \n#     tracker = EnhancedTracker('yolov8x.pt')\n#     commentary_engine = AdvancedCommentaryEngine(fps=fps)\n#     camera_estimator = CameraMovementEstimator(frames[0])\n#     view_transformer = ViewTransformer()\n#     speed_estimator = EnhancedSpeedAndDistanceEstimator()\n#     team_assigner = TeamAssigner()\n#     player_assigner = PlayerBallAssigner()\n#     ticker = RealTimeTicker(fps=fps)\n#     event_detector = EnhancedEventDetector()\n\n#     # --- STAGE 1: ENHANCED TRACKING ---\n#     print(\"\\nðŸŽ¯ Stage 1: Enhanced Object Detection and Tracking...\")\n#     tracks = tracker.get_object_tracks(frames, read_from_stub=False, stub_path=STUB_PATH)\n#     tracks[\"ball\"] = tracker.interpolate_ball_positions(tracks[\"ball\"])\n#     tracker.add_position_to_tracks(tracks)\n    \n#     # --- STAGE 2: MOTION & PERSPECTIVE ANALYSIS ---\n#     print(\"\\nðŸ“ Stage 2: Camera Motion and Perspective Transformation...\")\n#     camera_movement = camera_estimator.get_camera_movement(frames)\n#     camera_estimator.add_adjust_positions_to_tracks(tracks, camera_movement)\n#     view_transformer.add_transformed_position_to_tracks(tracks)\n#     speed_estimator.add_speed_and_distance_to_tracks(tracks)\n    \n#     # --- STAGE 3: ENHANCED TEAM ASSIGNMENT ---\n#     print(\"\\nðŸ‘¥ Stage 3: Team Assignment and Player Identification...\")\n#     team_assigner.assign_team_color(frames[0], tracks['players'][0])\n    \n#     for frame_num, frame in enumerate(frames):\n#         if frame_num % 100 == 0:\n#             print(f\"   Processing team assignment: {frame_num}/{len(frames)}\")\n            \n#         player_track = tracks['players'][frame_num]\n#         for player_id, track in player_track.items():\n#             team = team_assigner.get_player_team(frame, track['bbox'], player_id)\n#             tracks['players'][frame_num][player_id]['team'] = team\n#             tracks['players'][frame_num][player_id]['team_color'] = team_assigner.team_colors.get(team, (0,0,255))\n    \n#     # --- STAGE 4: ENHANCED EVENT DETECTION ---\n#     print(\"\\nâš½ Stage 4: Advanced Event Detection...\")\n#     events_df = event_detector.detect_events(tracks)\n#     print(f\"   Detected {len(events_df)} events:\")\n#     if not events_df.empty:\n#         event_summary = events_df['type_name'].value_counts()\n#         for event_type, count in event_summary.items():\n#             print(f\"     - {event_type}: {count}\")\n    \n#     # --- STAGE 5: ENHANCED COMMENTARY GENERATION ---\n#     print(\"\\nðŸŽ™ï¸ Stage 5: Advanced Ball Possession & AI Commentary...\")\n#     team_ball_control = []\n#     ticker_history = []\n#     advanced_commentary_history = []\n    \n#     commentary_interval = max(1, len(frames) // 20)  # Generate commentary 20 times\n    \n#     for frame_num, frame in enumerate(frames):\n#         if frame_num % 200 == 0:\n#             print(f\"   Commentary progress: {frame_num}/{len(frames)} frames\")\n        \n#         player_track = tracks['players'][frame_num]\n#         ball_bbox = tracks['ball'][frame_num].get(1, {}).get('bbox')\n        \n#         # Reset ball possession flags\n#         for player_id in tracks['players'][frame_num]:\n#             tracks['players'][frame_num][player_id]['has_ball'] = False\n        \n#         # Assign ball possession\n#         assigned_player = player_assigner.assign_ball_to_player(player_track, ball_bbox)\n#         if assigned_player != -1:\n#             tracks['players'][frame_num][assigned_player]['has_ball'] = True\n#             team_ball_control.append(tracks['players'][frame_num][assigned_player]['team'])\n#         else:\n#             team_ball_control.append(team_ball_control[-1] if team_ball_control else 0)\n        \n#         # Update ticker and commentary\n#         ticker_history.append(ticker.update(tracks, frame_num))\n#         commentary_engine.update_with_context(frame, tracks, frame_num, events_df)\n#         advanced_commentary_history.append(commentary_engine.latest_commentary)\n\n#     team_ball_control = np.array(team_ball_control)\n\n#     # --- STAGE 6: ENHANCED VISUALIZATION ---\n#     print(\"\\nðŸŽ¨ Stage 6: Enhanced Video Generation...\")\n#     display_commentary = ticker_history.copy()\n    \n#     # Apply advanced commentary at strategic points\n#     last_advanced_comment = advanced_commentary_history[0]\n#     for i, comment in enumerate(advanced_commentary_history):\n#         if comment != last_advanced_comment and len(comment) > 50:  # Substantial commentary\n#             # Apply commentary to a range of frames\n#             start_frame = max(0, i - commentary_engine.clip_length_frames)\n#             end_frame = min(len(display_commentary), i + commentary_engine.clip_length_frames//2)\n            \n#             for j in range(start_frame, end_frame):\n#                 if j < len(display_commentary):\n#                     display_commentary[j] = comment\n#             last_advanced_comment = comment\n\n#     # Generate enhanced output frames\n#     output_frames = []\n#     for frame_num, frame in enumerate(frames):\n#         if frame_num % 100 == 0:\n#             print(f\"   Rendering: {frame_num}/{len(frames)} frames\")\n            \n#         frame_copy = frame.copy()\n#         current_commentary = display_commentary[frame_num] if frame_num < len(display_commentary) else \"\"\n        \n#         player_dict = tracks[\"players\"][frame_num]\n#         ball_dict = tracks.get(\"ball\", [])[frame_num]\n        \n#         # Draw enhanced players\n#         for track_id, player in player_dict.items():\n#             color = player.get(\"team_color\", (0, 0, 255))\n#             has_ball = player.get('has_ball', False)\n#             jersey_num = player.get(\"jersey_number\", f\"ID{track_id}\")\n            \n#             frame_copy = tracker._draw_enhanced_player_ellipse(\n#                 frame_copy, player[\"bbox\"], color, track_id, jersey_num, has_ball\n#             )\n        \n#         # Draw enhanced ball\n#         if 1 in ball_dict:\n#             frame_copy = tracker._draw_enhanced_triangle(\n#                 frame_copy, ball_dict[1][\"bbox\"], (0, 255, 0), \"âš½\"\n#             )\n        \n#         # Draw enhanced UI elements\n#         frame_copy = tracker._draw_enhanced_team_ball_control(\n#             frame_copy, frame_num, team_ball_control\n#         )\n#         frame_copy = tracker._draw_enhanced_commentary_overlay(\n#             frame_copy, current_commentary\n#         )\n        \n#         output_frames.append(frame_copy)\n    \n#     # Apply enhanced speed visualization\n#     output_frames = speed_estimator.draw_speed_and_distance(output_frames, tracks)\n    \n#     # Save enhanced video\n#     save_video(output_frames, OUTPUT_VIDEO_PATH, fps)\n\n#     # --- ENHANCED FINAL STATISTICS ---\n#     print(\"\\n\" + \"ðŸ†\" + \"=\"*58 + \"ðŸ†\")\n#     print(\"            ENHANCED MATCH ANALYSIS COMPLETE\")\n#     print(\"ðŸ†\" + \"=\"*58 + \"ðŸ†\")\n    \n#     # Jersey detection stats\n#     jersey_stats = tracker.jersey_recognizer.get_detection_stats()\n#     print(f\"ðŸ‘• Jersey Recognition: {jersey_stats['successful_detections']}/{jersey_stats['total_players']} players ({jersey_stats['detection_rate']:.1f}%)\")\n    \n#     # Event detection stats\n#     if not events_df.empty:\n#         print(f\"âš½ Events Detected: {len(events_df)} total\")\n#         for event_type, count in events_df['type_name'].value_counts().items():\n#             print(f\"   - {event_type}: {count}\")\n    \n#     # Possession stats\n#     total_frames = len(team_ball_control)\n#     team1_possession = np.sum(team_ball_control == 1) / total_frames * 100\n#     team2_possession = np.sum(team_ball_control == 2) / total_frames * 100\n#     print(f\"ðŸ“Š Final Possession: Team 1: {team1_possession:.1f}%, Team 2: {team2_possession:.1f}%\")\n    \n#     # Commentary stats\n#     unique_comments = len(set(advanced_commentary_history))\n#     print(f\"ðŸŽ™ï¸ Commentary Generated: {unique_comments} unique insights\")\n    \n#     print(f\"âœ… Enhanced video saved: {OUTPUT_VIDEO_PATH}\")\n#     print(f\"ðŸ“ File size: {os.path.getsize(OUTPUT_VIDEO_PATH) / (1024*1024):.1f} MB\")\n    \n#     return {\n#         'tracks': tracks,\n#         'events': events_df,\n#         'team_ball_control': team_ball_control,\n#         'jersey_stats': jersey_stats,\n#         'output_path': OUTPUT_VIDEO_PATH\n#     }\n\n# # Run the enhanced system\n# if __name__ == \"__main__\":\n#     results = enhanced_main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T09:36:16.118819Z","iopub.execute_input":"2025-08-26T09:36:16.119113Z","iopub.status.idle":"2025-08-26T09:39:02.312249Z","shell.execute_reply.started":"2025-08-26T09:36:16.119094Z","shell.execute_reply":"2025-08-26T09:39:02.311344Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Starting Enhanced Football Analysis System\n============================================================\nâœ… Loaded 268/284 frames from video\nðŸ“º Video loaded: 268 frames at 25.0 FPS\n\nðŸ”§ Initializing Enhanced Components...\n","output_type":"stream"},{"name":"stderr","text":"DeprecationWarning: The parameter `use_angle_cls` has been deprecated and will be removed in the future. Please use `use_textline_orientation` instead.\n","output_type":"stream"},{"name":"stdout","text":"âš ï¸ PaddleOCR initialization failed, using EasyOCR only\nâœ… Advanced Jersey OCR module initialized.\nðŸŽ™ï¸ Initializing Advanced Vision Commentary Engine...\nLoading BLIP2 for detailed scene analysis...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ed11807cde949a9a97ea9a71ab0aaaa"}},"metadata":{}},{"name":"stdout","text":"âœ… BLIP2 model loaded successfully\nLoading BLIP for quick analysis...\nâœ… BLIP model loaded successfully\n\nðŸŽ¯ Stage 1: Enhanced Object Detection and Tracking...\nðŸŽ¯ Starting enhanced tracking on 268 frames...\nðŸ“Š Processing frame 0/268 (0.0%)\nðŸ“Š Processing frame 50/268 (18.7%)\nðŸ“Š Processing frame 100/268 (37.3%)\nðŸ“Š Processing frame 150/268 (56.0%)\nðŸ“Š Processing frame 200/268 (74.6%)\nðŸ“Š Processing frame 250/268 (93.3%)\nðŸ† Jersey Detection Stats: 3/3 (100.0% success rate)\nðŸ’¾ Tracks saved to cache: /kaggle/working/enhanced_tracks_stub.pkl\n\nðŸ“ Stage 2: Camera Motion and Perspective Transformation...\n\nðŸ‘¥ Stage 3: Team Assignment and Player Identification...\n   Processing team assignment: 0/268\n   Processing team assignment: 100/268\n   Processing team assignment: 200/268\n\nâš½ Stage 4: Advanced Event Detection...\n   Detected 54 events:\n     - Shot: 41\n     - Interception: 7\n     - Fast Pass: 5\n     - Pass: 1\n\nðŸŽ™ï¸ Stage 5: Advanced Ball Possession & AI Commentary...\n   Commentary progress: 0/268 frames\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Generating enhanced commentary... (Frame 59)\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Generating enhanced commentary... (Frame 74)\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Generating enhanced commentary... (Frame 119)\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Generating enhanced commentary... (Frame 149)\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Generating enhanced commentary... (Frame 179)\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"   Commentary progress: 200/268 frames\nGenerating enhanced commentary... (Frame 224)\n","output_type":"stream"},{"name":"stderr","text":"The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n","output_type":"stream"},{"name":"stdout","text":"Generating enhanced commentary... (Frame 239)\n\nðŸŽ¨ Stage 6: Enhanced Video Generation...\n   Rendering: 0/268 frames\n   Rendering: 100/268 frames\n   Rendering: 200/268 frames\nâœ… Video saved to: /kaggle/working/enhanced_football_analysis.mp4\n\nðŸ†==========================================================ðŸ†\n            ENHANCED MATCH ANALYSIS COMPLETE\nðŸ†==========================================================ðŸ†\nðŸ‘• Jersey Recognition: 3/3 players (100.0%)\nâš½ Events Detected: 54 total\n   - Shot: 41\n   - Interception: 7\n   - Fast Pass: 5\n   - Pass: 1\nðŸ“Š Final Possession: Team 1: 34.7%, Team 2: 65.3%\nðŸŽ™ï¸ Commentary Generated: 8 unique insights\nâœ… Enhanced video saved: /kaggle/working/enhanced_football_analysis.mp4\n","output_type":"stream"},{"name":"stderr","text":"OpenCV: FFMPEG: tag 0x34363248/'H264' is not supported with codec id 27 and format 'mp4 / MP4 (MPEG-4 Part 14)'\nOpenCV: FFMPEG: fallback to use tag 0x31637661/'avc1'\n[ERROR:0@3345.661] global cap_ffmpeg_impl.hpp:3203 open Could not find encoder for codec_id=27, error: Encoder not found\n[ERROR:0@3345.661] global cap_ffmpeg_impl.hpp:3281 open VIDEOIO/FFMPEG: Failed to initialize VideoWriter\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/499686652.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;31m# Run the enhanced system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menhanced_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/499686652.py\u001b[0m in \u001b[0;36menhanced_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âœ… Enhanced video saved: {OUTPUT_VIDEO_PATH}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ðŸ“ File size: {os.path.getsize(OUTPUT_VIDEO_PATH) / (1024*1024):.1f} MB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     return {\n","\u001b[0;32m/usr/lib/python3.11/genericpath.py\u001b[0m in \u001b[0;36mgetsize\u001b[0;34m(filename)\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/enhanced_football_analysis.mp4'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/enhanced_football_analysis.mp4'","output_type":"error"}],"execution_count":22},{"cell_type":"markdown","source":"# CogVLM2-Llama3-Caption","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics supervision numpy opencv-python scikit-learn pandas\n!pip install --upgrade ultralytics torch torchvision\n!pip install mplsoccer transformers accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:00:59.722870Z","iopub.execute_input":"2025-08-26T10:00:59.723204Z","iopub.status.idle":"2025-08-26T10:05:24.410172Z","shell.execute_reply.started":"2025-08-26T10:00:59.723182Z","shell.execute_reply":"2025-08-26T10:05:24.409391Z"}},"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.3.186-py3-none-any.whl.metadata (37 kB)\nCollecting supervision\n  Downloading supervision-0.26.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.4)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.16-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from supervision) (0.7.1)\nRequirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.11/dist-packages (from supervision) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nDownloading ultralytics-8.3.186-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading supervision-0.26.1-py3-none-any.whl (207 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.16-py3-none-any.whl (28 kB)\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics, supervision\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 supervision-0.26.1 ultralytics-8.3.186 ultralytics-thop-2.0.16\nRequirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.186)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nCollecting torch\n  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nCollecting torchvision\n  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.4)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.16)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-curand-cu12==10.3.9.90 (from torch)\n  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\nCollecting nvidia-nccl-cu12==2.27.3 (from torch)\n  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvtx-cu12==12.8.90 (from torch)\n  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.4.0 (from torch)\n  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch) (75.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.0->ultralytics) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nDownloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n  Attempting uninstall: nvidia-cusparselt-cu12\n    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.5.147\n    Uninstalling nvidia-curand-cu12-10.3.5.147:\n      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.21.0+cu124\n    Uninstalling torchvision-0.21.0+cu124:\n      Successfully uninstalled torchvision-0.21.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.8.0 torchvision-0.23.0 triton-3.4.0\nCollecting mplsoccer\n  Downloading mplsoccer-1.5.1-py3-none-any.whl.metadata (4.8 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nRequirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mplsoccer) (3.7.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mplsoccer) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from mplsoccer) (2.2.3)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from mplsoccer) (11.2.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from mplsoccer) (2.32.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from mplsoccer) (1.15.3)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from mplsoccer) (0.12.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.8.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mplsoccer) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mplsoccer) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mplsoccer) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mplsoccer) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mplsoccer) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mplsoccer) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->mplsoccer) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->mplsoccer) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->mplsoccer) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->mplsoccer) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->mplsoccer) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->mplsoccer) (2.4.1)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\nRequirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\nRequirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch>=2.0.0->accelerate) (75.2.0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->mplsoccer) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->mplsoccer) (2025.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->mplsoccer) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->mplsoccer) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->mplsoccer) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->mplsoccer) (2025.6.15)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mplsoccer) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->mplsoccer) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->mplsoccer) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->mplsoccer) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->mplsoccer) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->mplsoccer) (2024.2.0)\nDownloading mplsoccer-1.5.1-py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.4/86.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: mplsoccer\nSuccessfully installed mplsoccer-1.5.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade torch torchvision transformers accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:05:24.411828Z","iopub.execute_input":"2025-08-26T10:05:24.412135Z","iopub.status.idle":"2025-08-26T10:05:38.260015Z","shell.execute_reply.started":"2025-08-26T10:05:24.412112Z","shell.execute_reply":"2025-08-26T10:05:38.258804Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.23.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nCollecting transformers\n  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nCollecting accelerate\n  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.93)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.83)\nRequirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.90)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.3.90)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.8.93)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.93)\nRequirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1.3)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch) (75.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nCollecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, transformers, accelerate\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.33.1\n    Uninstalling huggingface-hub-0.33.1:\n      Successfully uninstalled huggingface-hub-0.33.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.52.4\n    Uninstalling transformers-4.52.4:\n      Successfully uninstalled transformers-4.52.4\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.8.1\n    Uninstalling accelerate-1.8.1:\n      Successfully uninstalled accelerate-1.8.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.10.1 huggingface-hub-0.34.4 transformers-4.55.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import sys\nimport os\nimport cv2\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import deque\nimport time\n\n# Machine Learning & Computer Vision Libraries\nfrom ultralytics import YOLO\nimport supervision as sv\nfrom sklearn.cluster import KMeans\nimport easyocr\n\n# CogVLM2 for AI Commentary (replacing Gemini)\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\nimport torch\nfrom PIL import Image\n\n# Plotting for Heatmaps\nfrom mplsoccer import Pitch\n\n# --- Video Utilities ---\ndef read_video(video_path):\n    \"\"\"Reads a video file and returns a list of its frames.\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frames.append(frame)\n    cap.release()\n    return frames\n\ndef save_video(output_video_frames, output_video_path):\n    \"\"\"Saves a list of frames as a video file.\"\"\"\n    if not output_video_frames:\n        print(\"No frames to save.\")\n        return\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_video_path, fourcc, 24, (output_video_frames[0].shape[1], output_video_frames[0].shape[0]))\n    for frame in output_video_frames:\n        out.write(frame)\n    out.release()\n\n# --- BBox Utilities ---\ndef get_center_of_bbox(bbox):\n    x1, y1, x2, y2 = bbox\n    return int((x1 + x2) / 2), int((y1 + y2) / 2)\n\ndef get_bbox_width(bbox):\n    return int(bbox[2] - bbox[0])\n\ndef get_foot_position(bbox):\n    x1, y1, x2, y2 = bbox\n    return int((x1 + x2) / 2), int(y2)\n \ndef measure_distance(p1, p2):\n    return ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**0.5\n\ndef measure_xy_distance(p1, p2):\n    return p1[0] - p2[0], p1[1] - p2[1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:05:38.261657Z","iopub.execute_input":"2025-08-26T10:05:38.261969Z","iopub.status.idle":"2025-08-26T10:05:59.313294Z","shell.execute_reply.started":"2025-08-26T10:05:38.261932Z","shell.execute_reply":"2025-08-26T10:05:59.312480Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756202747.553406      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756202747.614416      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install pytorchvideo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:23:40.821040Z","iopub.execute_input":"2025-08-26T10:23:40.821776Z","iopub.status.idle":"2025-08-26T10:23:51.273930Z","shell.execute_reply.started":"2025-08-26T10:23:40.821754Z","shell.execute_reply":"2025-08-26T10:23:51.272987Z"}},"outputs":[{"name":"stdout","text":"Collecting pytorchvideo\n  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting fvcore (from pytorchvideo)\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting av (from pytorchvideo)\n  Downloading av-15.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\nCollecting parameterized (from pytorchvideo)\n  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\nCollecting iopath (from pytorchvideo)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pytorchvideo) (3.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (1.26.4)\nCollecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (4.67.1)\nRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (3.1.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (11.2.1)\nRequirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (0.9.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath->pytorchvideo) (4.14.0)\nCollecting portalocker (from iopath->pytorchvideo)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore->pytorchvideo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore->pytorchvideo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore->pytorchvideo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore->pytorchvideo) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore->pytorchvideo) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->fvcore->pytorchvideo) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->fvcore->pytorchvideo) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->fvcore->pytorchvideo) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->fvcore->pytorchvideo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->fvcore->pytorchvideo) (2024.2.0)\nDownloading av-15.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (39.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nBuilding wheels for collected packages: pytorchvideo, fvcore, iopath\n  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188686 sha256=0f33aa7a748b2e1bb803ebbcc45b405f33a1e55812cad985594d771a246c8b6b\n  Stored in directory: /root/.cache/pip/wheels/a4/6d/ae/d016375a73be141a0e11bb42289e2d0b046c35687fc8010ecc\n  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=e2108d5db8de59b3eccdda9b1ad766a9b40fb821765ea7ca6c0fad52c73ad79c\n  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=62e42c35f9c70f8b1a31afe457812ca375ed61a07dd09c8729676fd8084bc672\n  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\nSuccessfully built pytorchvideo fvcore iopath\nInstalling collected packages: yacs, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\nSuccessfully installed av-15.0.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-3.2.0 pytorchvideo-0.1.5 yacs-0.1.8\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import math\nfrom collections import deque\nfrom typing import List, Dict, Any, Optional\n\nimport torch\nimport cv2\nfrom PIL import Image\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nclass ImprovedCommentaryEngine:\n    def __init__(self, clip_duration_seconds=5, fps=24, keyframes=3, max_words=25):\n        self.clip_length_frames = int(clip_duration_seconds * fps)\n        self.frame_buffer = deque(maxlen=self.clip_length_frames)\n        self.latest_commentary = \"Match analysis is starting...\"\n        self.fps = fps\n        self.keyframes = max(1, min(keyframes, 5))  # keep small for latency\n        self.max_words = max(6, max_words)\n\n        self.match_context = {\n            'possession_changes': [], 'recent_events': [],\n            'ball_position_history': [], 'player_movements': []\n        }\n\n        print(\"ðŸŽ™ï¸ Initializing CogVLM2 Commentary Engine...\")\n        self.model = None\n        self.tokenizer = None\n        try:\n            self.model_path = \"THUDM/cogvlm2-llama3-caption\"\n            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                self.model_path,\n                trust_remote_code=True\n            )\n            self.model = AutoModelForCausalLM.from_pretrained(\n                self.model_path,\n                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n                trust_remote_code=True,\n                device_map=\"auto\" if self.device == \"cuda\" else None\n            )\n            if self.device == \"cpu\":\n                self.model.to(self.device)\n\n            self._compiled = False\n            print(f\"âœ… CogVLM2-Llama3-Caption model loaded on {self.device}.\")\n        except Exception as e:\n            print(f\"âš ï¸ Could not initialize CogVLM2 model: {e}\")\n\n    # ---------- public API ----------\n    def update_with_context(self, frame, tracks_data, frame_num, events_data=None):\n        if self.model is None:\n            return  # graceful: use previous commentary\n\n        game_context = self._extract_game_context(tracks_data, frame_num, events_data)\n        self.match_context['recent_events'].append(game_context)\n        if len(self.match_context['recent_events']) > 10:\n            self.match_context['recent_events'].pop(0)\n\n        self.frame_buffer.append(frame)\n\n        if len(self.frame_buffer) == self.clip_length_frames:\n            print(\"Generating tactical summary...\")\n            try:\n                new_comment = self._generate_contextual_commentary(game_context)\n            except Exception as e:\n                print(f\"Commentary generation error: {e}\")\n                new_comment = self._generate_fallback_commentary(game_context)\n\n            if new_comment:\n                self.latest_commentary = new_comment\n            self.frame_buffer.clear()\n\n    # ---------- context extraction ----------\n    def _extract_game_context(self, tracks_data, frame_num, events_data):\n        players_track = tracks_data.get('players', [])\n        ball_track = tracks_data.get('ball', [])\n\n        # guard against missing indices\n        players_at_f = players_track[frame_num] if frame_num < len(players_track) else {}\n        ball_at_f = ball_track[frame_num] if frame_num < len(ball_track) else {}\n\n        # infer match clock\n        total_sec = frame_num / self.fps\n        minutes = int(total_sec // 60)\n        seconds = int(total_sec % 60)\n\n        # possession\n        possession = None\n        if isinstance(players_at_f, dict):\n            for pid, pinfo in players_at_f.items():\n                if pinfo.get('has_ball', False):\n                    possession = f\"Player {pid} (Team {pinfo.get('team', 'Unknown')})\"\n                    break\n\n        # ball presence robust check\n        ball_detected = False\n        if isinstance(ball_at_f, dict):\n            # consider presence if key 'visible' or track dict non-empty\n            ball_detected = bool(ball_at_f) or bool(ball_at_f.get('visible', False))\n        elif isinstance(ball_at_f, (list, tuple, set)):\n            ball_detected = len(ball_at_f) > 0 or (1 in ball_at_f)\n\n        # recent structured events in last 12s\n        recent_records = []\n        if events_data is not None and hasattr(events_data, \"empty\") and not events_data.empty:\n            lower_bound = max(0, total_sec - 12.0)\n            ev_secs = events_data['minute'] * 60 + events_data['second']\n            recent = events_data[ev_secs >= lower_bound].tail(4)\n            recent_records = recent.to_dict('records')\n\n        return {\n            'frame_num': frame_num,\n            'timestamp': f\"{minutes}:{seconds:02d}\",\n            'players_detected': len(players_at_f) if isinstance(players_at_f, dict) else 0,\n            'ball_detected': ball_detected,\n            'possession': possession,\n            'ball_speed': ball_at_f.get('speed', 0) if isinstance(ball_at_f, dict) else 0,\n            'recent_events': recent_records\n        }\n\n    # ---------- VLM prompting & decoding ----------\n    def _generate_contextual_commentary(self, game_context):\n        if self.model is None or self.tokenizer is None or len(self.frame_buffer) == 0:\n            return self._generate_fallback_commentary(game_context)\n\n        # choose K keyframes from the buffer\n        images = self._sample_keyframes(self.frame_buffer, self.keyframes)\n        pil_images = [self._to_pil(img) for img in images]\n\n        prompt = self._create_detailed_prompt(game_context)\n\n        # one-time torch.compile for speed (if PyTorch>=2 and cuda)\n        if self.device == \"cuda\" and not self._compiled:\n            try:\n                self.model = torch.compile(self.model)  # no-op on older versions\n                self._compiled = True\n            except Exception:\n                pass\n\n        with torch.inference_mode():\n            if self.device == \"cuda\":\n                autocast_dtype = torch.float16\n            else:\n                autocast_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n\n            # Some CogVLM2 builds expect build_conversation_input_ids() with multiple images\n            convo = self.model.build_conversation_input_ids(\n                self.tokenizer,\n                query=prompt,\n                images=pil_images,  # <â€” multiple frames\n                template_version='chat'\n            )\n\n            inputs = {\n                'input_ids': convo['input_ids'].unsqueeze(0).to(self.device),\n                'token_type_ids': convo['token_type_ids'].unsqueeze(0).to(self.device),\n                'attention_mask': convo['attention_mask'].unsqueeze(0).to(self.device),\n                'images': [[x.to(self.device).to(self.model.dtype) for x in convo['images']]]\n            }\n\n            with torch.autocast(device_type=self.device if self.device != \"cpu\" else \"cpu\",\n                                dtype=autocast_dtype, enabled=(self.device!=\"cpu\")):\n                outputs = self.model.generate(\n                    **inputs,\n                    max_new_tokens=64,\n                    do_sample=False,          # deterministic, clearer\n                    temperature=0.0,\n                    top_p=1.0,\n                    repetition_penalty=1.1,\n                    pad_token_id=self.tokenizer.eos_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id\n                )\n\n        raw = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:],\n                                    skip_special_tokens=True)\n        return self._postprocess_caption(raw)\n\n    def _create_detailed_prompt(self, context):\n        # compact events\n        events_str = self._format_recent_events(context.get('recent_events', []))\n\n        # tightly structured, single sentence, no hype words\n        return (\n            \"You are a professional football (soccer) tactical analyst.\\n\"\n            \"TASK: Describe the most significant on-ball action shown across these frames.\\n\"\n            \"CONSTRAINTS:\\n\"\n            f\"- Time: {context['timestamp']}\\n\"\n            f\"- Possession: {context.get('possession', 'Unclear')}\\n\"\n            f\"- Recent: {events_str}\\n\"\n            \"- Style: factual, objective, 1 sentence, â‰¤25 words, no exclamations.\\n\"\n            \"FORMAT: <subject> <action> <outcome/intent>. Examples:\\n\"\n            \"- The red winger receives a diagonal pass, drives inside past one defender, and squares toward the penalty spot.\\n\"\n            \"- The blue fullback overlaps and delivers a low cross that is intercepted near the near post.\\n\"\n            \"Now write your single-sentence summary:\"\n        )\n\n    def _format_recent_events(self, events):\n        if not events:\n            return \"None\"\n        formatted = []\n        for e in events[-3:]:\n            if not isinstance(e, dict):\n                continue\n            et = e.get('type_name', 'Event')\n            tm = e.get('team_name', 'Team')\n            formatted.append(f\"{et} â€“ {tm}\")\n        return \"; \".join(formatted) if formatted else \"None\"\n\n    # ---------- helpers ----------\n    def _sample_keyframes(self, buffer: deque, k: int) -> List[Any]:\n        n = len(buffer)\n        if k >= n:\n            return list(buffer)\n        # evenly spaced indices\n        st\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:23:57.652501Z","iopub.execute_input":"2025-08-26T10:23:57.653121Z","iopub.status.idle":"2025-08-26T10:23:57.675312Z","shell.execute_reply.started":"2025-08-26T10:23:57.653096Z","shell.execute_reply":"2025-08-26T10:23:57.674660Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class JerseyNumberRecognizer:\n    def __init__(self):\n        self.reader = easyocr.Reader(['en'], gpu=True)\n        self.jersey_cache = {}\n        print(\"âœ… Jersey OCR module initialized.\")\n\n    def recognize_jersey_number(self, player_crop, tracker_id):\n        if tracker_id in self.jersey_cache: return self.jersey_cache[tracker_id]\n        if player_crop.size == 0: return None\n        \n        crop_gray = cv2.cvtColor(player_crop, cv2.COLOR_BGR2GRAY)\n        results = self.reader.readtext(crop_gray, allowlist='0123456789', detail=1)\n\n        best_result = None\n        for (bbox, text, prob) in results:\n            if prob > 0.6 and text.isdigit() and len(text) <= 2:\n                if best_result is None or prob > best_result[2]:\n                    best_result = (bbox, text, prob)\n        \n        if best_result:\n            self.jersey_cache[tracker_id] = best_result[1]\n            return best_result[1]\n        \n        return None\n\nclass Tracker:\n    def __init__(self, model_name='yolov8x.pt'):\n        self.model = YOLO(model_name)\n        self.tracker = sv.ByteTrack()\n        self.jersey_recognizer = JerseyNumberRecognizer()\n\n    def get_object_tracks(self, frames, read_from_stub=False, stub_path=None):\n        if read_from_stub and stub_path and os.path.exists(stub_path):\n            with open(stub_path, 'rb') as f: return pickle.load(f)\n\n        tracks = {\"players\": [], \"referees\": [], \"ball\": []}\n        \n        for frame_num, frame in enumerate(frames):\n            if frame_num % 20 == 0: print(f\"Processing frame {frame_num}/{len(frames)}\")\n            results = self.model.predict(frame, conf=0.1)[0]\n            detections = sv.Detections.from_ultralytics(results)\n            \n            # Filter for players (class_id for 'person' is typically 0)\n            player_detections = detections[detections.class_id == 0]\n            tracked_players = self.tracker.update_with_detections(player_detections)\n            \n            tracks[\"players\"].append({})\n            tracks[\"referees\"].append({})\n            \n            for detection_data in tracked_players:\n                bbox = detection_data[0]\n                track_id = detection_data[4]\n                \n                player_crop = frame[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n                jersey_num = self.jersey_recognizer.recognize_jersey_number(player_crop, track_id)\n                tracks[\"players\"][frame_num][track_id] = {\"bbox\": bbox.tolist(), \"jersey_number\": jersey_num}\n\n            # Filter for ball (class_id for 'sports ball' is typically 32)\n            ball_detections = detections[detections.class_id == 32]\n            tracks[\"ball\"].append({})\n            if len(ball_detections) > 0:\n                tracks[\"ball\"][frame_num][1] = {\"bbox\": ball_detections.xyxy[0].tolist()}\n        \n        if stub_path:\n            with open(stub_path, 'wb') as f: pickle.dump(tracks, f)\n        return tracks\n\n    def add_position_to_tracks(self, tracks):\n        for type, obj_tracks in tracks.items():\n            for frame_num, track in enumerate(obj_tracks):\n                for id, info in track.items():\n                    bbox = info['bbox']\n                    info['position'] = get_foot_position(bbox) if type != 'ball' else get_center_of_bbox(bbox)\n    \n    def interpolate_ball_positions(self, ball_positions):\n        ball_bboxes = [x.get(1, {}).get('bbox', []) for x in ball_positions]\n        df = pd.DataFrame(ball_bboxes, columns=['x1', 'y1', 'x2', 'y2']).interpolate().bfill()\n        return [{1: {\"bbox\": x}} for x in df.to_numpy().tolist()]\n\n    def _draw_player_ellipse(self, frame, bbox, color, track_id, jersey_num):\n        y2 = int(bbox[3])\n        x_center, _ = get_center_of_bbox(bbox)\n        width = get_bbox_width(bbox)\n        cv2.ellipse(frame, center=(x_center, y2), axes=(int(width), int(0.35 * width)), angle=0.0, startAngle=-45, endAngle=235, color=color, thickness=2, lineType=cv2.LINE_4)\n        \n        label = f\"#{jersey_num}\" if jersey_num else str(track_id)\n        (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n        rect_w, rect_h = w + 10, h + 10\n        x1_rect, y1_rect = x_center - rect_w//2, (y2 - rect_h//2) + 15\n        \n        cv2.rectangle(frame, (x1_rect, y1_rect), (x1_rect + rect_w, y1_rect + rect_h), color, cv2.FILLED)\n        cv2.putText(frame, label, (x1_rect + 5, y1_rect + h + 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n        return frame\n\n    def _draw_triangle(self, frame, bbox, color):\n        y, x = int(bbox[1]), int(get_center_of_bbox(bbox)[0])\n        points = np.array([[x, y], [x - 10, y - 20], [x + 10, y - 20]])\n        cv2.drawContours(frame, [points], 0, color, cv2.FILLED)\n        cv2.drawContours(frame, [points], 0, (0, 0, 0), 2)\n        return frame\n\n    def _draw_team_ball_control(self, frame, frame_num, team_ball_control):\n        overlay = frame.copy()\n        cv2.rectangle(overlay, (10, 10), (350, 70), (255, 255, 255), -1)\n        cv2.addWeighted(overlay, 0.5, frame, 0.5, 0, frame)\n        \n        team_1_frames = np.sum(team_ball_control[:frame_num + 1] == 1)\n        team_2_frames = np.sum(team_ball_control[:frame_num + 1] == 2)\n        total = max(1, team_1_frames + team_2_frames)\n        p1 = (team_1_frames / total) * 100\n        p2 = (team_2_frames / total) * 100\n        \n        cv2.putText(frame, f\"Team 1 Possession: {p1:.1f}%\", (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 2)\n        cv2.putText(frame, f\"Team 2 Possession: {p2:.1f}%\", (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 2)\n        return frame\n\n    def _draw_commentary_overlay(self, frame, text):\n        h, w, _ = frame.shape\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        thickness = 2\n        \n        font_scale = 1.0\n        (text_w, text_h), _ = cv2.getTextSize(text, font, font_scale, thickness)\n        \n        target_w = w * 0.9\n        if text_w > target_w:\n            font_scale = target_w / text_w\n        \n        (text_w, text_h), _ = cv2.getTextSize(text, font, font_scale, thickness)\n\n        banner_h = text_h + 20\n        overlay = frame.copy()\n        cv2.rectangle(overlay, (0, h - banner_h), (w, h), (0, 0, 0), -1)\n        cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)\n        \n        text_x = (w - text_w) // 2\n        text_y = h - 10\n        cv2.putText(frame, text, (text_x, text_y), font, font_scale, (255, 255, 255), thickness)\n        \n        return frame\n\nclass EventDetector:\n    def __init__(self):\n        self.shot_speed_threshold_mps = 15\n        self.frame_rate = 24\n\n    def detect_events(self, tracks):\n        player_assigner = PlayerBallAssigner()\n        ball_possession_log = []\n        for frame_num in range(len(tracks['players'])):\n            player_track = tracks['players'][frame_num]\n            ball_bbox = tracks['ball'][frame_num].get(1, {}).get('bbox')\n            assigned_player_id = player_assigner.assign_ball_to_player(player_track, ball_bbox) if ball_bbox else -1\n            ball_possession_log.append(assigned_player_id)\n\n        events = []\n        last_player_with_ball, pass_start_info = -1, {}\n        for frame_num, current_player_id in enumerate(ball_possession_log):\n            ball_pos_transformed = tracks['ball'][frame_num].get(1, {}).get('position_transformed')\n            if not ball_pos_transformed: continue\n\n            is_valid_pass = (current_player_id != last_player_with_ball and last_player_with_ball != -1 and current_player_id != -1)\n            if is_valid_pass:\n                start_player_team = tracks['players'][pass_start_info['frame']][last_player_with_ball].get('team')\n                end_player_team = tracks['players'][frame_num].get(current_player_id, {}).get('team')\n                if start_player_team == end_player_team and start_player_team is not None:\n                    events.append({\n                        \"type_name\": \"Pass\", \"player_name\": f\"Player_{last_player_with_ball}\",\n                        \"team_name\": f\"Team {start_player_team}\", \"x\": pass_start_info['position'][0],\n                        \"y\": pass_start_info['position'][1], \"end_x\": ball_pos_transformed[0],\n                        \"end_y\": ball_pos_transformed[1], \"minute\": int(frame_num / (self.frame_rate * 60)),\n                        \"second\": int((frame_num / self.frame_rate) % 60)\n                    })\n            \n            if current_player_id != -1:\n                pass_start_info = {'frame': frame_num, 'position': ball_pos_transformed}\n                last_player_with_ball = current_player_id\n        \n        return pd.DataFrame(events)\n\n# Other classes (TeamAssigner, PlayerBallAssigner, etc.)\nclass TeamAssigner:\n    def __init__(self):\n        self.team_colors, self.player_team_dict, self.kmeans = {}, {}, None\n    def get_player_color(self, frame, bbox):\n        image = frame[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n        if image.size == 0: return np.array([0,0,0])\n        top_half = image[0:int(image.shape[0] / 2), :]\n        if top_half.size == 0: return np.array([0,0,0])\n        kmeans = KMeans(n_clusters=2, init=\"k-means++\", n_init=1, random_state=0).fit(top_half.reshape(-1, 3))\n        labels = kmeans.labels_.reshape(top_half.shape[0], top_half.shape[1])\n        corner_clusters = [labels[0, 0], labels[0, -1], labels[-1, 0], labels[-1, -1]]\n        non_player_cluster = max(set(corner_clusters), key=corner_clusters.count)\n        return kmeans.cluster_centers_[1 - non_player_cluster]\n    def assign_team_color(self, frame, player_detections):\n        if not player_detections: return\n        colors = [self.get_player_color(frame, det[\"bbox\"]) for _, det in player_detections.items()]\n        self.kmeans = KMeans(n_clusters=2, init=\"k-means++\", n_init=10, random_state=0).fit(colors)\n        self.team_colors[1], self.team_colors[2] = self.kmeans.cluster_centers_\n    def get_player_team(self, frame, bbox, player_id):\n        if player_id in self.player_team_dict: return self.player_team_dict[player_id]\n        if self.kmeans is None: return 0\n        color = self.get_player_color(frame, bbox)\n        team_id = self.kmeans.predict(color.reshape(1, -1))[0] + 1\n        self.player_team_dict[player_id] = team_id\n        return team_id\n\nclass PlayerBallAssigner:\n    def __init__(self): self.max_dist = 70\n    def assign_ball_to_player(self, players, ball_bbox):\n        if not ball_bbox: return -1\n        ball_pos, min_dist, assigned_player = get_center_of_bbox(ball_bbox), float('inf'), -1\n        for id, player in players.items():\n            dist = measure_distance(get_foot_position(player['bbox']), ball_pos)\n            if dist < self.max_dist and dist < min_dist: min_dist, assigned_player = dist, id\n        return assigned_player\n\nclass CameraMovementEstimator:\n    def __init__(self, frame):\n        self.lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n        self.features = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n    def get_camera_movement(self, frames, read_from_stub=False, stub_path=None):\n        if read_from_stub and stub_path and os.path.exists(stub_path):\n            with open(stub_path, 'rb') as f: return pickle.load(f)\n        movements = [[0, 0]] * len(frames)\n        old_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n        old_features = cv2.goodFeaturesToTrack(old_gray, **self.features)\n        for i in range(1, len(frames)):\n            new_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n            new_features, status, _ = cv2.calcOpticalFlowPyrLK(old_gray, new_gray, old_features, None, **self.lk_params)\n            \n            good_new = new_features[status==1]\n            good_old = old_features[status==1]\n\n            move_x, move_y = 0, 0\n            if len(good_new) > 0:\n                move_x, move_y = np.mean(good_old - good_new, axis=0).ravel()\n\n            movements[i] = [move_x, move_y]\n            old_gray = new_gray.copy()\n            old_features = good_new.reshape(-1, 1, 2)\n        if stub_path:\n            with open(stub_path, 'wb') as f: pickle.dump(movements, f)\n        return movements\n    def add_adjust_positions_to_tracks(self, tracks, movements):\n        for type, obj_tracks in tracks.items():\n            for i, track in enumerate(obj_tracks):\n                for id, info in track.items():\n                    info['position_adjusted'] = (info['position'][0] + movements[i][0], info['position'][1] + movements[i][1])\n\nclass ViewTransformer:\n    def __init__(self):\n        court_w, court_l = 34, 52.5\n        self.pixel_verts = np.float32([[110, 1035], [265, 275], [910, 260], [1640, 915]])\n        self.target_verts = np.float32([[0, court_w], [0, 0], [court_l, 0], [court_l, court_w]])\n        self.transformer = cv2.getPerspectiveTransform(self.pixel_verts, self.target_verts)\n    def transform_point(self, point):\n        p = (int(point[0]), int(point[1]))\n        is_inside = cv2.pointPolygonTest(self.pixel_verts, p, False) >= 0\n        if not is_inside: return None\n        reshaped = np.array(point).reshape(-1, 1, 2).astype(np.float32)\n        transformed = cv2.perspectiveTransform(reshaped, self.transformer)\n        return transformed.reshape(-1, 2)\n    def add_transformed_position_to_tracks(self, tracks):\n        for type, obj_tracks in tracks.items():\n            for track in obj_tracks:\n                for id, info in track.items():\n                    pos = info.get('position_adjusted', info.get('position'))\n                    if pos:\n                        transformed = self.transform_point(pos)\n                        info['position_transformed'] = transformed.squeeze().tolist() if transformed is not None else None\n\nclass SpeedAndDistanceEstimator:\n    def __init__(self):\n        self.frame_window, self.frame_rate = 24, 24\n    def add_speed_and_distance_to_tracks(self, tracks):\n        total_dist = {}\n        for type, obj_tracks in tracks.items():\n            if type not in [\"players\", \"referees\"]: continue\n            for i in range(len(obj_tracks)):\n                for id, info in obj_tracks[i].items():\n                    if i > 0:\n                        prev_info = tracks[type][i-1].get(id)\n                        if prev_info and info.get('position_transformed') and prev_info.get('position_transformed'):\n                            dist = measure_distance(info['position_transformed'], prev_info['position_transformed'])\n                            total_dist[id] = total_dist.get(id, 0) + dist\n                            speed_mps = dist * self.frame_rate\n                            info['speed'] = speed_mps * 3.6 # km/h\n                            info['distance'] = total_dist[id]\n    def draw_speed_and_distance(self, frames, tracks):\n        output_frames = []\n        for i, frame in enumerate(frames):\n            for type, obj_tracks in tracks.items():\n                if type not in [\"players\", \"referees\"]: continue\n                for id, info in obj_tracks[i].items():\n                    if \"speed\" in info:\n                        x, y = get_foot_position(info['bbox'])\n                        cv2.putText(frame, f\"{info['speed']:.1f} km/h\", (x - 20, y + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)\n            output_frames.append(frame)\n        return output_frames\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:24:04.924209Z","iopub.execute_input":"2025-08-26T10:24:04.924488Z","iopub.status.idle":"2025-08-26T10:24:04.967520Z","shell.execute_reply.started":"2025-08-26T10:24:04.924467Z","shell.execute_reply":"2025-08-26T10:24:04.966739Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def main():\n    # --- SETUP ---\n    INPUT_VIDEO_PATH = \"/kaggle/input/football-video2/CityUtdR.mp4\"\n    STUB_PATH = \"/kaggle/working/tracks_stub.pkl\"\n    OUTPUT_VIDEO_PATH = \"/kaggle/working/final_analysis_video-Llama3-v1.mp4\"\n    \n    frames = read_video(INPUT_VIDEO_PATH)\n    if not frames:\n        print(\"Video file not found or could not be read. Check the path.\")\n        return None\n\n    cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n    fps = cap.get(cv2.CAP_PROP_FPS) or 24\n    cap.release()\n\n    # --- INITIALIZE ALL MODULES ---\n    tracker = Tracker('yolov8x.pt')\n    commentary_engine = ImprovedCommentaryEngine(fps=fps)\n    camera_estimator = CameraMovementEstimator(frames[0])\n    view_transformer = ViewTransformer()\n    speed_estimator = SpeedAndDistanceEstimator()\n    team_assigner = TeamAssigner()\n    player_assigner = PlayerBallAssigner()\n    ticker = RealTimeTicker(fps=fps)\n\n    # --- STAGE 1: TRACKING ---\n    print(\"Stage 1: Performing object detection and tracking...\")\n    tracks = tracker.get_object_tracks(frames, read_from_stub=False, stub_path=STUB_PATH)\n    tracks[\"ball\"] = tracker.interpolate_ball_positions(tracks[\"ball\"])\n    tracker.add_position_to_tracks(tracks)\n    \n    # --- STAGE 2: MOTION & PERSPECTIVE ---\n    print(\"Stage 2: Estimating camera motion and transforming perspective...\")\n    camera_movement = camera_estimator.get_camera_movement(frames)\n    camera_estimator.add_adjust_positions_to_tracks(tracks, camera_movement)\n    view_transformer.add_transformed_position_to_tracks(tracks)\n    speed_estimator.add_speed_and_distance_to_tracks(tracks)\n    \n    # --- STAGE 3: TEAM ASSIGNMENT ---\n    print(\"Stage 3: Assigning teams...\")\n    team_assigner.assign_team_color(frames[0], tracks['players'][0])\n    \n    for frame_num, frame in enumerate(frames):\n        player_track = tracks['players'][frame_num]\n        for player_id, track in player_track.items():\n            team = team_assigner.get_player_team(frame, track['bbox'], player_id)\n            tracks['players'][frame_num][player_id]['team'] = team\n            tracks['players'][frame_num][player_id]['team_color'] = team_assigner.team_colors.get(team, (0,0,255))\n    \n    # --- STAGE 4: GENERATE EVENTS DATA ---\n    print(\"Stage 4: Detecting events for commentary context...\")\n    event_detector = EventDetector()\n    events_df = event_detector.detect_events(tracks)\n    print(f\"Detected {len(events_df)} events for commentary context\")\n    \n    # --- STAGE 5: BALL POSSESSION & COMMENTARY ---\n    print(\"Stage 5: Tracking ball possession and generating all commentary...\")\n    team_ball_control = []\n    ticker_history = []\n    cogvlm_history = []\n    \n    for frame_num, frame in enumerate(frames):\n        player_track = tracks['players'][frame_num]\n        ball_bbox = tracks['ball'][frame_num].get(1, {}).get('bbox')\n        \n        for player_id in tracks['players'][frame_num]:\n            tracks['players'][frame_num][player_id]['has_ball'] = False\n        \n        assigned_player = player_assigner.assign_ball_to_player(player_track, ball_bbox)\n        if assigned_player != -1:\n            tracks['players'][frame_num][assigned_player]['has_ball'] = True\n            team_ball_control.append(tracks['players'][frame_num][assigned_player]['team'])\n        else:\n            team_ball_control.append(team_ball_control[-1] if team_ball_control else 0)\n        \n        ticker_history.append(ticker.update(tracks, frame_num))\n        commentary_engine.update_with_context(frame, tracks, frame_num, events_df)\n        cogvlm_history.append(commentary_engine.latest_commentary)\n        \n        if frame_num % 100 == 0:\n            print(f\"Commentary progress: {frame_num}/{len(frames)} frames\")\n\n    team_ball_control = np.array(team_ball_control)\n\n    # --- STAGE 6: VISUALIZATION & SAVING ---\n    print(\"Stage 6: Combining commentary and saving final video...\")\n    display_commentary = ticker_history.copy()\n    last_cogvlm_comment = cogvlm_history[0]\n    for i, comment in enumerate(cogvlm_history):\n        if comment != last_cogvlm_comment:\n            start_frame = max(0, i - commentary_engine.clip_length_frames)\n            for j in range(start_frame, i):\n                if j < len(display_commentary):\n                    display_commentary[j] = comment\n            last_cogvlm_comment = comment\n\n    output_frames = []\n    for frame_num, frame in enumerate(frames):\n        frame_copy = frame.copy()\n        current_commentary = display_commentary[frame_num] if frame_num < len(display_commentary) else \" \"\n        \n        player_dict = tracks[\"players\"][frame_num]\n        ball_dict = tracks.get(\"ball\", [])[frame_num]\n        \n        for track_id, player in player_dict.items():\n            color = player.get(\"team_color\", (0, 0, 255))\n            frame_copy = tracker._draw_player_ellipse(frame_copy, player[\"bbox\"], color, track_id, player.get(\"jersey_number\"))\n            if player.get('has_ball', False):\n                frame_copy = tracker._draw_triangle(frame_copy, player[\"bbox\"], (0, 0, 255))\n        \n        if 1 in ball_dict:\n            frame_copy = tracker._draw_triangle(frame_copy, ball_dict[1][\"bbox\"], (0, 255, 0))\n        \n        frame_copy = tracker._draw_team_ball_control(frame_copy, frame_num, team_ball_control)\n        frame_copy = tracker._draw_commentary_overlay(frame_copy, current_commentary)\n        output_frames.append(frame_copy)\n    \n    output_frames = speed_estimator.draw_speed_and_distance(output_frames, tracks)\n    save_video(output_frames, OUTPUT_VIDEO_PATH)\n\n    # --- FINAL STATISTICS ---\n    print(\"\\n\" + \"=\"*50)\n    print(\"MATCH ANALYSIS COMPLETE\")\n    print(\"=\"*50)\n    print(f\"âœ… Video saved to: {OUTPUT_VIDEO_PATH}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:24:11.289275Z","iopub.execute_input":"2025-08-26T10:24:11.289574Z","iopub.status.idle":"2025-08-26T10:24:59.091471Z","shell.execute_reply.started":"2025-08-26T10:24:11.289553Z","shell.execute_reply":"2025-08-26T10:24:59.090617Z"}},"outputs":[{"name":"stdout","text":"âœ… Jersey OCR module initialized.\nðŸŽ™ï¸ Initializing CogVLM2 Commentary Engine...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"util.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e64d4e3bda6a413f97303aa46dccd294"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/cogvlm2-llama3-caption:\n- util.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"visual.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c850728384ef4ec295739aea9a195467"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/cogvlm2-llama3-caption:\n- visual.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/cogvlm2-llama3-caption:\n- util.py\n- visual.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"name":"stdout","text":"âš ï¸ Could not initialize CogVLM2 model: No module named 'torchvision.transforms.functional_tensor'\nStage 1: Performing object detection and tracking...\nProcessing frame 0/268\n\n0: 384x640 21 persons, 62.3ms\nSpeed: 1.9ms preprocess, 62.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 23 persons, 1 airplane, 36.7ms\nSpeed: 1.7ms preprocess, 36.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 23 persons, 1 airplane, 30.0ms\nSpeed: 1.6ms preprocess, 30.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 23 persons, 1 airplane, 29.7ms\nSpeed: 1.7ms preprocess, 29.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 24 persons, 1 airplane, 30.0ms\nSpeed: 1.5ms preprocess, 30.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 20 persons, 1 airplane, 30.7ms\nSpeed: 1.6ms preprocess, 30.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 22 persons, 1 airplane, 2 tennis rackets, 31.0ms\nSpeed: 1.5ms preprocess, 31.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 19 persons, 1 airplane, 29.6ms\nSpeed: 1.6ms preprocess, 29.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 23 persons, 1 airplane, 29.3ms\nSpeed: 1.6ms preprocess, 29.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 23 persons, 1 airplane, 1 bench, 30.6ms\nSpeed: 1.5ms preprocess, 30.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 24 persons, 1 airplane, 29.8ms\nSpeed: 1.8ms preprocess, 29.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 21 persons, 1 airplane, 29.5ms\nSpeed: 1.6ms preprocess, 29.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 25 persons, 1 airplane, 29.6ms\nSpeed: 1.6ms preprocess, 29.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 21 persons, 1 airplane, 1 sports ball, 28.6ms\nSpeed: 1.6ms preprocess, 28.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 23 persons, 1 airplane, 1 sports ball, 29.3ms\nSpeed: 1.5ms preprocess, 29.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 20 persons, 1 airplane, 1 sports ball, 29.6ms\nSpeed: 1.6ms preprocess, 29.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 23 persons, 1 airplane, 1 sports ball, 29.8ms\nSpeed: 1.5ms preprocess, 29.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 20 persons, 1 airplane, 29.6ms\nSpeed: 1.5ms preprocess, 29.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 19 persons, 2 airplanes, 31.8ms\nSpeed: 1.6ms preprocess, 31.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 19 persons, 1 airplane, 31.0ms\nSpeed: 1.6ms preprocess, 31.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\nProcessing frame 20/268\n\n0: 384x640 24 persons, 1 airplane, 29.6ms\nSpeed: 1.5ms preprocess, 29.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 25 persons, 2 airplanes, 31.7ms\nSpeed: 1.5ms preprocess, 31.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 2 airplanes, 30.9ms\nSpeed: 1.5ms preprocess, 30.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 19 persons, 1 airplane, 29.7ms\nSpeed: 1.5ms preprocess, 29.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 19 persons, 2 airplanes, 1 sports ball, 30.0ms\nSpeed: 1.5ms preprocess, 30.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 20 persons, 2 airplanes, 31.2ms\nSpeed: 1.6ms preprocess, 31.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 22 persons, 1 airplane, 1 sports ball, 28.8ms\nSpeed: 1.5ms preprocess, 28.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 23 persons, 1 airplane, 1 sports ball, 30.8ms\nSpeed: 1.5ms preprocess, 30.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 21 persons, 1 airplane, 28.9ms\nSpeed: 1.7ms preprocess, 28.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 1 airplane, 1 sports ball, 29.9ms\nSpeed: 1.5ms preprocess, 29.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 airplane, 1 sports ball, 31.2ms\nSpeed: 1.6ms preprocess, 31.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 airplane, 1 sports ball, 2 tennis rackets, 29.7ms\nSpeed: 1.5ms preprocess, 29.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 1 airplane, 1 sports ball, 29.7ms\nSpeed: 1.6ms preprocess, 29.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 21 persons, 1 sports ball, 31.9ms\nSpeed: 1.5ms preprocess, 31.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 1 airplane, 28.8ms\nSpeed: 1.6ms preprocess, 28.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 1 airplane, 31.2ms\nSpeed: 1.6ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 22 persons, 1 airplane, 1 sports ball, 31.8ms\nSpeed: 1.6ms preprocess, 31.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 airplane, 1 sports ball, 29.5ms\nSpeed: 1.6ms preprocess, 29.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 airplane, 1 sports ball, 29.6ms\nSpeed: 1.5ms preprocess, 29.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 23 persons, 1 airplane, 1 sports ball, 1 tennis racket, 31.9ms\nSpeed: 1.5ms preprocess, 31.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\nProcessing frame 40/268\n\n0: 384x640 20 persons, 1 sports ball, 30.9ms\nSpeed: 1.6ms preprocess, 30.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 20 persons, 1 sports ball, 30.5ms\nSpeed: 1.6ms preprocess, 30.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 19 persons, 1 sports ball, 1 tennis racket, 28.8ms\nSpeed: 1.5ms preprocess, 28.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 30.8ms\nSpeed: 1.7ms preprocess, 30.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 19 persons, 1 chair, 31.9ms\nSpeed: 1.6ms preprocess, 31.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 24 persons, 1 chair, 31.8ms\nSpeed: 1.5ms preprocess, 31.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 2 tennis rackets, 31.6ms\nSpeed: 1.6ms preprocess, 31.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 1 chair, 29.9ms\nSpeed: 1.5ms preprocess, 29.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 29.9ms\nSpeed: 1.5ms preprocess, 29.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 20 persons, 31.9ms\nSpeed: 1.5ms preprocess, 31.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 21 persons, 31.7ms\nSpeed: 1.5ms preprocess, 31.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 24 persons, 31.8ms\nSpeed: 1.5ms preprocess, 31.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 25 persons, 30.5ms\nSpeed: 1.5ms preprocess, 30.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 25 persons, 29.9ms\nSpeed: 1.5ms preprocess, 29.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 25 persons, 29.5ms\nSpeed: 1.6ms preprocess, 29.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 20 persons, 31.9ms\nSpeed: 1.6ms preprocess, 31.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 1 sports ball, 1 tennis racket, 31.3ms\nSpeed: 1.5ms preprocess, 31.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 19 persons, 31.8ms\nSpeed: 1.6ms preprocess, 31.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 22 persons, 1 sports ball, 32.3ms\nSpeed: 1.5ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 24 persons, 1 sports ball, 31.4ms\nSpeed: 1.6ms preprocess, 31.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\nProcessing frame 60/268\n\n0: 384x640 18 persons, 2 sports balls, 29.1ms\nSpeed: 1.5ms preprocess, 29.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 1 sports ball, 32.0ms\nSpeed: 1.7ms preprocess, 32.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 2 sports balls, 29.3ms\nSpeed: 1.5ms preprocess, 29.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 2 sports balls, 30.3ms\nSpeed: 1.5ms preprocess, 30.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 2 sports balls, 29.6ms\nSpeed: 1.8ms preprocess, 29.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 1 sports ball, 30.2ms\nSpeed: 1.6ms preprocess, 30.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 sports ball, 31.6ms\nSpeed: 1.6ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 31.1ms\nSpeed: 1.8ms preprocess, 31.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 32.2ms\nSpeed: 1.7ms preprocess, 32.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 1 sports ball, 30.7ms\nSpeed: 1.7ms preprocess, 30.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 baseball glove, 1 tennis racket, 31.3ms\nSpeed: 1.6ms preprocess, 31.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 29.0ms\nSpeed: 1.5ms preprocess, 29.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 32.1ms\nSpeed: 1.6ms preprocess, 32.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 32.6ms\nSpeed: 1.6ms preprocess, 32.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 sports ball, 32.7ms\nSpeed: 1.5ms preprocess, 32.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 sports balls, 30.1ms\nSpeed: 1.5ms preprocess, 30.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 sports ball, 32.1ms\nSpeed: 1.8ms preprocess, 32.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 sports ball, 30.8ms\nSpeed: 1.7ms preprocess, 30.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 sports ball, 31.2ms\nSpeed: 1.6ms preprocess, 31.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 sports ball, 30.8ms\nSpeed: 1.5ms preprocess, 30.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\nProcessing frame 80/268\n\n0: 384x640 18 persons, 1 sports ball, 29.8ms\nSpeed: 1.6ms preprocess, 29.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 30.0ms\nSpeed: 1.5ms preprocess, 30.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 tennis racket, 30.2ms\nSpeed: 1.6ms preprocess, 30.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 sports ball, 1 tennis racket, 32.1ms\nSpeed: 1.5ms preprocess, 32.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 32.3ms\nSpeed: 1.7ms preprocess, 32.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 sports ball, 30.8ms\nSpeed: 1.5ms preprocess, 30.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 sports balls, 29.4ms\nSpeed: 1.6ms preprocess, 29.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 sports ball, 32.5ms\nSpeed: 1.5ms preprocess, 32.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 2 sports balls, 29.7ms\nSpeed: 1.7ms preprocess, 29.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 sports ball, 30.0ms\nSpeed: 1.5ms preprocess, 30.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 sports ball, 30.6ms\nSpeed: 1.7ms preprocess, 30.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 32.6ms\nSpeed: 1.6ms preprocess, 32.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 sports ball, 29.9ms\nSpeed: 1.5ms preprocess, 29.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 sports ball, 29.3ms\nSpeed: 1.5ms preprocess, 29.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 32.5ms\nSpeed: 1.4ms preprocess, 32.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 sports ball, 31.7ms\nSpeed: 1.6ms preprocess, 31.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 sports ball, 29.7ms\nSpeed: 1.6ms preprocess, 29.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 31.6ms\nSpeed: 1.5ms preprocess, 31.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 sports ball, 31.6ms\nSpeed: 1.7ms preprocess, 31.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 sports balls, 1 tennis racket, 30.1ms\nSpeed: 1.6ms preprocess, 30.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\nProcessing frame 100/268\n\n0: 384x640 12 persons, 1 sports ball, 30.1ms\nSpeed: 1.5ms preprocess, 30.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 29.4ms\nSpeed: 1.5ms preprocess, 29.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 sports ball, 31.0ms\nSpeed: 1.6ms preprocess, 31.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 sports ball, 31.6ms\nSpeed: 1.5ms preprocess, 31.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 2 sports balls, 29.0ms\nSpeed: 1.6ms preprocess, 29.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 2 sports balls, 30.1ms\nSpeed: 1.5ms preprocess, 30.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 sports ball, 31.2ms\nSpeed: 1.5ms preprocess, 31.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 32.2ms\nSpeed: 1.5ms preprocess, 32.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 33.4ms\nSpeed: 1.5ms preprocess, 33.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 29.9ms\nSpeed: 1.5ms preprocess, 29.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 sports ball, 29.8ms\nSpeed: 1.5ms preprocess, 29.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 sports balls, 30.8ms\nSpeed: 1.6ms preprocess, 30.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 sports balls, 32.0ms\nSpeed: 1.5ms preprocess, 32.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 2 sports balls, 32.6ms\nSpeed: 1.5ms preprocess, 32.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 sports ball, 29.6ms\nSpeed: 1.5ms preprocess, 29.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 sports ball, 32.7ms\nSpeed: 1.6ms preprocess, 32.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 sports ball, 31.5ms\nSpeed: 1.5ms preprocess, 31.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 sports ball, 32.0ms\nSpeed: 1.7ms preprocess, 32.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 sports ball, 33.5ms\nSpeed: 1.5ms preprocess, 33.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 sports ball, 33.6ms\nSpeed: 1.4ms preprocess, 33.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\nProcessing frame 120/268\n\n0: 384x640 11 persons, 30.3ms\nSpeed: 1.5ms preprocess, 30.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 30.0ms\nSpeed: 1.5ms preprocess, 30.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 32.3ms\nSpeed: 1.5ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 32.4ms\nSpeed: 1.5ms preprocess, 32.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 1 sports ball, 31.5ms\nSpeed: 1.5ms preprocess, 31.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 tennis racket, 30.3ms\nSpeed: 1.5ms preprocess, 30.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 2 sports balls, 1 tennis racket, 32.7ms\nSpeed: 1.6ms preprocess, 32.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 1 sports ball, 1 baseball bat, 33.9ms\nSpeed: 1.6ms preprocess, 33.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 1 tennis racket, 32.0ms\nSpeed: 1.5ms preprocess, 32.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 10 persons, 32.0ms\nSpeed: 1.5ms preprocess, 32.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 33.8ms\nSpeed: 1.6ms preprocess, 33.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 11 persons, 32.7ms\nSpeed: 1.5ms preprocess, 32.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 32.5ms\nSpeed: 1.5ms preprocess, 32.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 32.2ms\nSpeed: 1.5ms preprocess, 32.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 33.8ms\nSpeed: 1.6ms preprocess, 33.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 31.3ms\nSpeed: 1.5ms preprocess, 31.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 33.5ms\nSpeed: 1.6ms preprocess, 33.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 12 persons, 33.0ms\nSpeed: 1.5ms preprocess, 33.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 33.1ms\nSpeed: 1.5ms preprocess, 33.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 32.6ms\nSpeed: 1.5ms preprocess, 32.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\nProcessing frame 140/268\n\n0: 384x640 18 persons, 33.9ms\nSpeed: 1.6ms preprocess, 33.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 19 persons, 1 sports ball, 30.8ms\nSpeed: 1.5ms preprocess, 30.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 32.5ms\nSpeed: 1.5ms preprocess, 32.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 32.9ms\nSpeed: 1.5ms preprocess, 32.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 33.2ms\nSpeed: 1.5ms preprocess, 33.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 29.9ms\nSpeed: 1.5ms preprocess, 29.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 30.2ms\nSpeed: 1.5ms preprocess, 30.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 30.4ms\nSpeed: 1.5ms preprocess, 30.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 33.6ms\nSpeed: 1.6ms preprocess, 33.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 30.5ms\nSpeed: 1.6ms preprocess, 30.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 32.9ms\nSpeed: 1.5ms preprocess, 32.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 31.7ms\nSpeed: 1.5ms preprocess, 31.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 29.5ms\nSpeed: 1.6ms preprocess, 29.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 32.5ms\nSpeed: 1.6ms preprocess, 32.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 32.7ms\nSpeed: 1.6ms preprocess, 32.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 29.9ms\nSpeed: 1.6ms preprocess, 29.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 30.8ms\nSpeed: 1.5ms preprocess, 30.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 29.4ms\nSpeed: 1.6ms preprocess, 29.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 29.5ms\nSpeed: 1.6ms preprocess, 29.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 32.7ms\nSpeed: 1.5ms preprocess, 32.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\nProcessing frame 160/268\n\n0: 384x640 15 persons, 32.0ms\nSpeed: 1.8ms preprocess, 32.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 31.9ms\nSpeed: 1.7ms preprocess, 31.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 31.4ms\nSpeed: 1.5ms preprocess, 31.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 13 persons, 32.0ms\nSpeed: 1.5ms preprocess, 32.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 32.5ms\nSpeed: 1.5ms preprocess, 32.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 31.5ms\nSpeed: 1.6ms preprocess, 31.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 30.9ms\nSpeed: 1.5ms preprocess, 30.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 sports ball, 29.5ms\nSpeed: 1.5ms preprocess, 29.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 32.4ms\nSpeed: 1.5ms preprocess, 32.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 30.2ms\nSpeed: 1.6ms preprocess, 30.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 30.0ms\nSpeed: 1.6ms preprocess, 30.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 1 sports ball, 31.2ms\nSpeed: 1.5ms preprocess, 31.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 31.2ms\nSpeed: 1.5ms preprocess, 31.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 sports ball, 30.6ms\nSpeed: 1.6ms preprocess, 30.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 1 sports ball, 30.8ms\nSpeed: 1.5ms preprocess, 30.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 30.7ms\nSpeed: 1.6ms preprocess, 30.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 1 tennis racket, 30.6ms\nSpeed: 1.6ms preprocess, 30.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 tennis rackets, 29.3ms\nSpeed: 1.5ms preprocess, 29.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 31.9ms\nSpeed: 1.5ms preprocess, 31.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 32.0ms\nSpeed: 1.5ms preprocess, 32.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\nProcessing frame 180/268\n\n0: 384x640 13 persons, 29.7ms\nSpeed: 1.5ms preprocess, 29.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 29.8ms\nSpeed: 1.6ms preprocess, 29.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 30.9ms\nSpeed: 1.5ms preprocess, 30.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 30.2ms\nSpeed: 1.6ms preprocess, 30.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 32.1ms\nSpeed: 1.6ms preprocess, 32.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 29.4ms\nSpeed: 1.5ms preprocess, 29.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 31.0ms\nSpeed: 1.5ms preprocess, 31.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 sports ball, 29.3ms\nSpeed: 1.7ms preprocess, 29.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 32.0ms\nSpeed: 1.5ms preprocess, 32.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 29.0ms\nSpeed: 1.4ms preprocess, 29.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 sports ball, 31.8ms\nSpeed: 1.6ms preprocess, 31.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 tennis racket, 29.9ms\nSpeed: 1.6ms preprocess, 29.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 30.2ms\nSpeed: 1.5ms preprocess, 30.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 31.7ms\nSpeed: 1.5ms preprocess, 31.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 31.9ms\nSpeed: 1.5ms preprocess, 31.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 31.6ms\nSpeed: 1.8ms preprocess, 31.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 28.9ms\nSpeed: 1.5ms preprocess, 28.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 30.9ms\nSpeed: 1.5ms preprocess, 30.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 1 tennis racket, 32.2ms\nSpeed: 1.5ms preprocess, 32.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 29.2ms\nSpeed: 1.5ms preprocess, 29.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\nProcessing frame 200/268\n\n0: 384x640 14 persons, 31.1ms\nSpeed: 1.5ms preprocess, 31.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 32.1ms\nSpeed: 1.6ms preprocess, 32.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 30.2ms\nSpeed: 1.5ms preprocess, 30.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 sports balls, 31.9ms\nSpeed: 1.5ms preprocess, 31.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 1 sports ball, 31.7ms\nSpeed: 1.5ms preprocess, 31.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 31.6ms\nSpeed: 1.6ms preprocess, 31.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 29.7ms\nSpeed: 1.5ms preprocess, 29.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 1 sports ball, 31.4ms\nSpeed: 1.4ms preprocess, 31.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 2 sports balls, 30.0ms\nSpeed: 1.6ms preprocess, 30.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 31.2ms\nSpeed: 1.6ms preprocess, 31.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 31.7ms\nSpeed: 1.6ms preprocess, 31.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 29.4ms\nSpeed: 1.5ms preprocess, 29.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 31.6ms\nSpeed: 1.5ms preprocess, 31.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 29.0ms\nSpeed: 1.7ms preprocess, 29.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 31.5ms\nSpeed: 1.4ms preprocess, 31.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 30.3ms\nSpeed: 1.6ms preprocess, 30.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 29.6ms\nSpeed: 1.5ms preprocess, 29.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 tennis racket, 31.2ms\nSpeed: 1.6ms preprocess, 31.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 tennis racket, 31.6ms\nSpeed: 1.7ms preprocess, 31.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 14 persons, 29.8ms\nSpeed: 1.5ms preprocess, 29.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\nProcessing frame 220/268\n\n0: 384x640 15 persons, 31.0ms\nSpeed: 1.5ms preprocess, 31.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 32.1ms\nSpeed: 1.6ms preprocess, 32.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 28.9ms\nSpeed: 1.5ms preprocess, 28.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 30.9ms\nSpeed: 1.5ms preprocess, 30.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 tennis racket, 31.9ms\nSpeed: 1.5ms preprocess, 31.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 31.4ms\nSpeed: 1.6ms preprocess, 31.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 19 persons, 30.9ms\nSpeed: 1.6ms preprocess, 30.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 19 persons, 29.8ms\nSpeed: 1.6ms preprocess, 29.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 30.7ms\nSpeed: 1.8ms preprocess, 30.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 31.6ms\nSpeed: 1.6ms preprocess, 31.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 31.2ms\nSpeed: 1.5ms preprocess, 31.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 29.9ms\nSpeed: 1.6ms preprocess, 29.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 29.2ms\nSpeed: 1.5ms preprocess, 29.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 chair, 31.9ms\nSpeed: 1.5ms preprocess, 31.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 31.8ms\nSpeed: 1.6ms preprocess, 31.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 31.5ms\nSpeed: 1.6ms preprocess, 31.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 31.5ms\nSpeed: 1.6ms preprocess, 31.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 1 sports ball, 1 tennis racket, 31.0ms\nSpeed: 1.6ms preprocess, 31.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 29.1ms\nSpeed: 1.5ms preprocess, 29.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 31.5ms\nSpeed: 1.5ms preprocess, 31.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\nProcessing frame 240/268\n\n0: 384x640 14 persons, 1 sports ball, 1 tennis racket, 31.5ms\nSpeed: 1.5ms preprocess, 31.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 31.8ms\nSpeed: 1.5ms preprocess, 31.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 1 sports ball, 31.8ms\nSpeed: 1.6ms preprocess, 31.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 1 sports ball, 31.6ms\nSpeed: 1.6ms preprocess, 31.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 30.4ms\nSpeed: 1.5ms preprocess, 30.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 1 sports ball, 30.1ms\nSpeed: 1.5ms preprocess, 30.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 1 sports ball, 30.4ms\nSpeed: 1.5ms preprocess, 30.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 1 sports ball, 30.8ms\nSpeed: 1.7ms preprocess, 30.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 19 persons, 1 sports ball, 30.7ms\nSpeed: 1.7ms preprocess, 30.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 31.3ms\nSpeed: 1.5ms preprocess, 31.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 29.1ms\nSpeed: 1.5ms preprocess, 29.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 15 persons, 31.9ms\nSpeed: 1.5ms preprocess, 31.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 20 persons, 31.6ms\nSpeed: 1.5ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 30.9ms\nSpeed: 1.5ms preprocess, 30.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 tennis racket, 30.4ms\nSpeed: 1.6ms preprocess, 30.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 31.0ms\nSpeed: 1.6ms preprocess, 31.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 20 persons, 31.6ms\nSpeed: 1.5ms preprocess, 31.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 30.5ms\nSpeed: 1.5ms preprocess, 30.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 1 chair, 30.1ms\nSpeed: 1.6ms preprocess, 30.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 1 sports ball, 31.9ms\nSpeed: 1.5ms preprocess, 31.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\nProcessing frame 260/268\n\n0: 384x640 18 persons, 31.3ms\nSpeed: 1.6ms preprocess, 31.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 1 bench, 31.5ms\nSpeed: 1.5ms preprocess, 31.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 30.4ms\nSpeed: 1.5ms preprocess, 30.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 17 persons, 28.7ms\nSpeed: 1.7ms preprocess, 28.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 16 persons, 1 sports ball, 31.8ms\nSpeed: 1.6ms preprocess, 31.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 21 persons, 1 sports ball, 31.3ms\nSpeed: 1.6ms preprocess, 31.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 19 persons, 30.4ms\nSpeed: 1.5ms preprocess, 30.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n0: 384x640 18 persons, 1 sports ball, 29.6ms\nSpeed: 1.5ms preprocess, 29.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\nStage 2: Estimating camera motion and transforming perspective...\nStage 3: Assigning teams...\nStage 4: Detecting events for commentary context...\nDetected 6 events for commentary context\nStage 5: Tracking ball possession and generating all commentary...\nCommentary progress: 0/268 frames\nCommentary progress: 100/268 frames\nCommentary progress: 200/268 frames\nStage 6: Combining commentary and saving final video...\n\n==================================================\nMATCH ANALYSIS COMPLETE\n==================================================\nâœ… Video saved to: /kaggle/working/final_analysis_video.mp4\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# CogVLM2-Llama3-Caption version 2","metadata":{}},{"cell_type":"code","source":"# Keep the runtime's torch/torchvision to avoid breakage; install everything else.\n!pip -q install --upgrade ultralytics supervision easyocr numpy opencv-python scikit-learn pandas mplsoccer transformers accelerate huggingface_hub\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:41:15.866317Z","iopub.execute_input":"2025-08-26T10:41:15.866677Z","iopub.status.idle":"2025-08-26T10:41:38.705816Z","shell.execute_reply.started":"2025-08-26T10:41:15.866652Z","shell.execute_reply":"2025-08-26T10:41:38.704980Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.2 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\nydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.2 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.1 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Shim for older torchvision import expected by CogVLM2 (functional_tensor)\nimport types, sys\ntry:\n    import torchvision.transforms.functional as F\n    ft = types.ModuleType(\"torchvision.transforms.functional_tensor\")\n    for name in [\"_is_tensor_image\",\"to_pil_image\",\"to_tensor\",\"normalize\"]:\n        if hasattr(F, name):\n            setattr(ft, name, getattr(F, name))\n    sys.modules[\"torchvision.transforms.functional_tensor\"] = ft\nexcept Exception:\n    pass\n\n# Optional: pin the HF revision to avoid repo code changing mid-run.\nHF_MODEL_ID = \"THUDM/cogvlm2-llama3-caption\"\nHF_REV = None  # put a specific commit hash or tag string here to pin; keep None to use latest\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:41:38.707246Z","iopub.execute_input":"2025-08-26T10:41:38.708040Z","iopub.status.idle":"2025-08-26T10:41:38.713905Z","shell.execute_reply.started":"2025-08-26T10:41:38.708009Z","shell.execute_reply":"2025-08-26T10:41:38.713050Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import sys\nimport os\nimport cv2\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import deque\nimport time\n\n# Machine Learning & Computer Vision Libraries\nfrom ultralytics import YOLO\nimport supervision as sv\nfrom sklearn.cluster import KMeans\nimport easyocr\n\n# CogVLM2 for AI Commentary (replacing Gemini)\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nfrom PIL import Image\n\n# Plotting for Heatmaps\nfrom mplsoccer import Pitch\n\n# --- Video Utilities ---\ndef read_video(video_path):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frames.append(frame)\n    cap.release()\n    return frames\n\ndef save_video(output_video_frames, output_video_path):\n    if not output_video_frames:\n        print(\"No frames to save.\")\n        return\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_video_path, fourcc, 24, (output_video_frames[0].shape[1], output_video_frames[0].shape[0]))\n    for frame in output_video_frames:\n        out.write(frame)\n    out.release()\n\n# --- BBox Utilities ---\ndef get_center_of_bbox(bbox):\n    x1, y1, x2, y2 = bbox\n    return int((x1 + x2) / 2), int((y1 + y2) / 2)\n\ndef get_bbox_width(bbox):\n    return int(bbox[2] - bbox[0])\n\ndef get_foot_position(bbox):\n    x1, y1, x2, y2 = bbox\n    return int((x1 + x2) / 2), int(y2)\n\ndef measure_distance(p1, p2):\n    return ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**0.5\n\ndef measure_xy_distance(p1, p2):\n    return p1[0] - p2[0], p1[1] - p2[1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:41:43.379866Z","iopub.execute_input":"2025-08-26T10:41:43.380173Z","iopub.status.idle":"2025-08-26T10:41:43.390358Z","shell.execute_reply.started":"2025-08-26T10:41:43.380152Z","shell.execute_reply":"2025-08-26T10:41:43.389397Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class ImprovedCommentaryEngine:\n    def __init__(self, clip_duration_seconds=5, fps=24, keyframes=3, max_words=25):\n        self.clip_length_frames = int(clip_duration_seconds * fps)\n        self.frame_buffer = deque(maxlen=self.clip_length_frames)\n        self.latest_commentary = \"Match analysis is starting...\"\n        self.fps = fps\n        self.keyframes = max(1, min(keyframes, 5))\n        self.max_words = max(6, max_words)\n\n        self.match_context = {\n            'possession_changes': [], 'recent_events': [],\n            'ball_position_history': [], 'player_movements': []\n        }\n\n        print(\"ðŸŽ™ï¸ Initializing CogVLM2 Commentary Engine...\")\n        self.model, self.tokenizer = None, None\n        try:\n            model_kwargs = dict(trust_remote_code=True)\n            if HF_REV is not None:\n                model_kwargs[\"revision\"] = HF_REV\n\n            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n            self.tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_ID, **model_kwargs)\n            self.model = AutoModelForCausalLM.from_pretrained(\n                HF_MODEL_ID,\n                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n                device_map=\"auto\" if self.device == \"cuda\" else None,\n                **model_kwargs\n            )\n            if self.device == \"cpu\":\n                self.model.to(self.device)\n\n            self._compiled = False\n            print(f\"âœ… CogVLM2-Llama3-Caption loaded on {self.device}.\")\n        except Exception as e:\n            print(f\"âš ï¸ Could not initialize CogVLM2 model: {e}\")\n\n    def update_with_context(self, frame, tracks_data, frame_num, events_data=None):\n        if not self.model:\n            return\n        game_context = self._extract_game_context(tracks_data, frame_num, events_data)\n        self.match_context['recent_events'].append(game_context)\n        if len(self.match_context['recent_events']) > 10:\n            self.match_context['recent_events'].pop(0)\n\n        self.frame_buffer.append(frame)\n\n        if len(self.frame_buffer) == self.clip_length_frames:\n            print(\"Generating tactical summary...\")\n            try:\n                new_comment = self._generate_contextual_commentary(game_context)\n            except Exception as e:\n                print(f\"Commentary generation error: {e}\")\n                new_comment = self._generate_fallback_commentary(game_context)\n            if new_comment:\n                self.latest_commentary = new_comment\n            self.frame_buffer.clear()\n\n    def _extract_game_context(self, tracks_data, frame_num, events_data):\n        players_seq = tracks_data.get('players', [])\n        ball_seq = tracks_data.get('ball', [])\n\n        players_at_f = players_seq[frame_num] if frame_num < len(players_seq) else {}\n        ball_at_f = ball_seq[frame_num] if frame_num < len(ball_seq) else {}\n\n        total_sec = frame_num / self.fps\n        minutes, seconds = int(total_sec // 60), int(total_sec % 60)\n\n        possession = None\n        if isinstance(players_at_f, dict):\n            for pid, info in players_at_f.items():\n                if info.get('has_ball', False):\n                    possession = f\"Player {pid} (Team {info.get('team', 'Unknown')})\"\n                    break\n\n        # robust ball presence check\n        ball_detected = False\n        if isinstance(ball_at_f, dict):\n            ball_detected = bool(ball_at_f) or bool(ball_at_f.get('visible', False))\n        elif isinstance(ball_at_f, (list, tuple, set)):\n            ball_detected = len(ball_at_f) > 0\n\n        recent_records = []\n        if events_data is not None and hasattr(events_data, \"empty\") and not events_data.empty:\n            ev_secs = events_data['minute'] * 60 + events_data['second']\n            lower = max(0, total_sec - 12.0)\n            recent = events_data[ev_secs >= lower].tail(4)\n            recent_records = recent.to_dict('records')\n\n        return {\n            'frame_num': frame_num,\n            'timestamp': f\"{minutes}:{seconds:02d}\",\n            'players_detected': len(players_at_f) if isinstance(players_at_f, dict) else 0,\n            'ball_detected': ball_detected,\n            'possession': possession,\n            'ball_speed': ball_at_f.get('speed', 0) if isinstance(ball_at_f, dict) else 0,\n            'recent_events': recent_records\n        }\n\n    def _generate_contextual_commentary(self, game_context):\n        if self.model is None or self.tokenizer is None or len(self.frame_buffer) == 0:\n            return self._generate_fallback_commentary(game_context)\n\n        images = self._sample_keyframes(self.frame_buffer, self.keyframes)\n        pil_images = [self._to_pil(img) for img in images]\n\n        prompt = self._create_detailed_prompt(game_context)\n\n        if self.device == \"cuda\" and not self._compiled:\n            try:\n                self.model = torch.compile(self.model)\n                self._compiled = True\n            except Exception:\n                pass\n\n        with torch.inference_mode():\n            convo = self.model.build_conversation_input_ids(\n                self.tokenizer,\n                query=prompt,\n                images=pil_images,\n                template_version='chat'\n            )\n            inputs = {\n                'input_ids': convo['input_ids'].unsqueeze(0).to(self.device),\n                'token_type_ids': convo['token_type_ids'].unsqueeze(0).to(self.device),\n                'attention_mask': convo['attention_mask'].unsqueeze(0).to(self.device),\n                'images': [[x.to(self.device).to(self.model.dtype) for x in convo['images']]]\n            }\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=64,\n                do_sample=False,\n                temperature=0.0,\n                top_p=1.0,\n                repetition_penalty=1.1,\n                pad_token_id=self.tokenizer.eos_token_id,\n                eos_token_id=self.tokenizer.eos_token_id\n            )\n\n        raw = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n        return self._postprocess_caption(raw)\n\n    def _create_detailed_prompt(self, context):\n        events = self._format_recent_events(context.get('recent_events', []))\n        return (\n            \"You are a professional football (soccer) tactical analyst.\\n\"\n            \"TASK: Describe the most significant on-ball action visible across these frames.\\n\"\n            f\"- Time: {context['timestamp']}\\n\"\n            f\"- Possession: {context.get('possession', 'Unclear')}\\n\"\n            f\"- Recent: {events}\\n\"\n            \"STYLE: factual, objective, single sentence, â‰¤25 words, no exclamations.\\n\"\n            \"FORMAT: <subject> <action> <outcome/intent>. Examples:\\n\"\n            \"- The red winger receives a diagonal pass, drives inside past one defender, and squares toward the penalty spot.\\n\"\n            \"- The blue fullback overlaps and delivers a low cross that is intercepted near the near post.\\n\"\n            \"Now write the single sentence:\"\n        )\n\n    def _format_recent_events(self, events):\n        if not events: return \"None\"\n        out = []\n        for e in events[-3:]:\n            if isinstance(e, dict):\n                et = e.get('type_name', 'Event')\n                tm = e.get('team_name', 'Team')\n                out.append(f\"{et} â€“ {tm}\")\n        return \"; \".join(out) if out else \"None\"\n\n    def _sample_keyframes(self, buffer, k):\n        n = len(buffer)\n        if k >= n: return list(buffer)\n        step = n / float(k + 1)\n        idxs = [int((i + 1) * step) - 1 for i in range(k)]\n        idxs = [min(max(0, idx), n - 1) for idx in idxs]\n        return [buffer[i] for i in idxs]\n\n    def _to_pil(self, frame):\n        if frame is None:\n            return Image.new(\"RGB\", (224, 224), color=(0, 0, 0))\n        if len(frame.shape) == 3 and frame.shape[2] == 3:\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        else:\n            frame_rgb = frame\n        return Image.fromarray(frame_rgb)\n\n    def _postprocess_caption(self, text: str) -> str:\n        s = \" \".join(text.strip().split())\n        # keep only first sentence\n        if \".\" in s:\n            s = s.split(\".\")[0].strip() + \".\"\n        # enforce word cap\n        words = s.split()\n        if len(words) > self.max_words:\n            s = \" \".join(words[:self.max_words]).rstrip(\",\") + \".\"\n        # remove mild hype words\n        ban = {\"incredible\", \"amazing\", \"unbelievable\", \"spectacular\", \"fantastic\"}\n        s = \" \".join([w for w in s.split() if w.lower() not in ban])\n        return s\n\n    def _generate_fallback_commentary(self, context):\n        if context.get('possession'):\n            return f\"{context['possession']} carries possession and progresses play.\"\n        return \"Possession unclear; play develops centrally with short passing.\"\n\nclass RealTimeTicker:\n    \"\"\"Debounced live ticker to reduce flicker.\"\"\"\n    def __init__(self, fps=24, hold_seconds=2.0):\n        self.fps = fps\n        self.last_player_id = -1\n        self.last_team_id = -1\n        self.ticker_text = \"Match begins!\"\n        self.text_display_frames = 0\n        self.hold_frames = max(1, int(hold_seconds * fps))\n\n    def _get_ball_carrier(self, player_track):\n        if not isinstance(player_track, dict): return -1, -1\n        for player_id, data in player_track.items():\n            if data.get('has_ball', False):\n                return player_id, data.get('team', -1)\n        return -1, -1\n\n    def update(self, tracks, frame_num):\n        if self.text_display_frames > 0:\n            self.text_display_frames -= 1\n            return self.ticker_text\n\n        player_track = tracks['players'][frame_num]\n        current_player_id, current_team_id = self._get_ball_carrier(player_track)\n\n        if (current_player_id != -1 and self.last_player_id != -1 and\n            current_player_id != self.last_player_id and current_team_id == self.last_team_id):\n            self.ticker_text = f\"Pass from Player {self.last_player_id} to Player {current_player_id}.\"\n            self.text_display_frames = self.hold_frames\n\n        elif current_player_id != -1 and self.last_team_id != -1 and current_team_id != self.last_team_id:\n            self.ticker_text = f\"Team {current_team_id} gains possession.\"\n            self.text_display_frames = self.hold_frames\n\n        else:\n            if current_player_id != -1:\n                self.ticker_text = f\"Player {current_player_id} (Team {current_team_id}) on the ball.\"\n                self.text_display_frames = int(self.hold_frames * 0.5)\n            else:\n                self.ticker_text = \"Ball is loose.\"\n                self.text_display_frames = int(self.hold_frames * 0.5)\n\n        self.last_player_id = current_player_id if current_player_id != -1 else -1\n        if current_player_id != -1:\n            self.last_team_id = current_team_id\n        return self.ticker_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:42:12.872332Z","iopub.execute_input":"2025-08-26T10:42:12.872629Z","iopub.status.idle":"2025-08-26T10:42:12.901796Z","shell.execute_reply.started":"2025-08-26T10:42:12.872607Z","shell.execute_reply":"2025-08-26T10:42:12.901069Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class JerseyNumberRecognizer:\n    def __init__(self):\n        self.reader = easyocr.Reader(['en'], gpu=True)\n        self.jersey_cache = {}\n        print(\"âœ… Jersey OCR module initialized.\")\n\n    def recognize_jersey_number(self, player_crop, tracker_id):\n        if tracker_id in self.jersey_cache: return self.jersey_cache[tracker_id]\n        if player_crop.size == 0: return None\n\n        crop_gray = cv2.cvtColor(player_crop, cv2.COLOR_BGR2GRAY)\n        results = self.reader.readtext(crop_gray, allowlist='0123456789', detail=1)\n\n        best_result = None\n        for (bbox, text, prob) in results:\n            if prob > 0.6 and text.isdigit() and len(text) <= 2:\n                if best_result is None or prob > best_result[2]:\n                    best_result = (bbox, text, prob)\n\n        if best_result:\n            self.jersey_cache[tracker_id] = best_result[1]\n            return best_result[1]\n        return None\n\nclass Tracker:\n    def __init__(self, model_name='yolov8x.pt'):\n        self.model = YOLO(model_name)\n        self.tracker = sv.ByteTrack()\n        self.jersey_recognizer = JerseyNumberRecognizer()\n\n    def get_object_tracks(self, frames, read_from_stub=False, stub_path=None):\n        if read_from_stub and stub_path and os.path.exists(stub_path):\n            with open(stub_path, 'rb') as f: return pickle.load(f)\n\n        tracks = {\"players\": [], \"referees\": [], \"ball\": []}\n\n        for frame_num, frame in enumerate(frames):\n            if frame_num % 20 == 0:\n                print(f\"Processing frame {frame_num}/{len(frames)}\")\n\n            # Restrict to person (0) and sports ball (32) to avoid \"airplane/racket\"\n            results = self.model.predict(\n                frame, conf=0.35, iou=0.5, classes=[0, 32], verbose=False\n            )[0]\n\n            detections = sv.Detections.from_ultralytics(results)\n\n            # Players\n            player_detections = detections[detections.class_id == 0]\n            tracked_players = self.tracker.update_with_detections(player_detections)\n\n            tracks[\"players\"].append({})\n            tracks[\"referees\"].append({})\n\n            for det in tracked_players:\n                bbox = det[0]\n                track_id = det[4]\n                player_crop = frame[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n                jersey_num = self.jersey_recognizer.recognize_jersey_number(player_crop, track_id)\n                tracks[\"players\"][frame_num][track_id] = {\"bbox\": bbox.tolist(), \"jersey_number\": jersey_num}\n\n            # Ball (keep the most confident one if multiple)\n            ball_detections = detections[detections.class_id == 32]\n            tracks[\"ball\"].append({})\n            if len(ball_detections) > 0:\n                # pick highest confidence\n                idx = int(np.argmax(ball_detections.confidence))\n                tracks[\"ball\"][frame_num][1] = {\"bbox\": ball_detections.xyxy[idx].tolist()}\n\n        if stub_path:\n            with open(stub_path, 'wb') as f: pickle.dump(tracks, f)\n        return tracks\n\n    def add_position_to_tracks(self, tracks):\n        for typ, obj_tracks in tracks.items():\n            for frame_num, track in enumerate(obj_tracks):\n                for id, info in track.items():\n                    bbox = info['bbox']\n                    info['position'] = get_foot_position(bbox) if typ != 'ball' else get_center_of_bbox(bbox)\n\n    def interpolate_ball_positions(self, ball_positions):\n        ball_bboxes = [x.get(1, {}).get('bbox', []) for x in ball_positions]\n        df = pd.DataFrame(ball_bboxes, columns=['x1','y1','x2','y2']).interpolate().bfill()\n        return [{1: {\"bbox\": x}} for x in df.to_numpy().tolist()]\n\n    def _draw_player_ellipse(self, frame, bbox, color, track_id, jersey_num):\n        y2 = int(bbox[3])\n        x_center, _ = get_center_of_bbox(bbox)\n        width = get_bbox_width(bbox)\n        cv2.ellipse(frame, center=(x_center, y2), axes=(int(width), int(0.35 * width)), angle=0.0,\n                    startAngle=-45, endAngle=235, color=color, thickness=2, lineType=cv2.LINE_4)\n        label = f\"#{jersey_num}\" if jersey_num else str(track_id)\n        (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n        rect_w, rect_h = w + 10, h + 10\n        x1_rect, y1_rect = x_center - rect_w//2, (y2 - rect_h//2) + 15\n        cv2.rectangle(frame, (x1_rect, y1_rect), (x1_rect + rect_w, y1_rect + rect_h), color, cv2.FILLED)\n        cv2.putText(frame, label, (x1_rect + 5, y1_rect + h + 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n        return frame\n\n    def _draw_triangle(self, frame, bbox, color):\n        y, x = int(bbox[1]), int(get_center_of_bbox(bbox)[0])\n        points = np.array([[x, y], [x - 10, y - 20], [x + 10, y - 20]])\n        cv2.drawContours(frame, [points], 0, color, cv2.FILLED)\n        cv2.drawContours(frame, [points], 0, (0, 0, 0), 2)\n        return frame\n\n    def _draw_team_ball_control(self, frame, frame_num, team_ball_control):\n        overlay = frame.copy()\n        cv2.rectangle(overlay, (10, 10), (350, 70), (255, 255, 255), -1)\n        cv2.addWeighted(overlay, 0.5, frame, 0.5, 0, frame)\n        team_1_frames = np.sum(team_ball_control[:frame_num + 1] == 1)\n        team_2_frames = np.sum(team_ball_control[:frame_num + 1] == 2)\n        total = max(1, team_1_frames + team_2_frames)\n        p1 = (team_1_frames / total) * 100\n        p2 = (team_2_frames / total) * 100\n        cv2.putText(frame, f\"Team 1 Possession: {p1:.1f}%\", (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 2)\n        cv2.putText(frame, f\"Team 2 Possession: {p2:.1f}%\", (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 2)\n        return frame\n\n    def _draw_commentary_overlay(self, frame, text):\n        h, w, _ = frame.shape\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        thickness = 2\n        font_scale = 1.0\n        (text_w, text_h), _ = cv2.getTextSize(text, font, font_scale, thickness)\n        target_w = w * 0.9\n        if text_w > target_w:\n            font_scale = max(0.5, target_w / text_w)\n        (text_w, text_h), _ = cv2.getTextSize(text, font, font_scale, thickness)\n        banner_h = text_h + 20\n        overlay = frame.copy()\n        cv2.rectangle(overlay, (0, h - banner_h), (w, h), (0, 0, 0), -1)\n        cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)\n        text_x = (w - text_w) // 2\n        text_y = h - 10\n        cv2.putText(frame, text, (text_x, text_y), font, font_scale, (255, 255, 255), thickness)\n        return frame\n\nclass EventDetector:\n    def __init__(self):\n        self.shot_speed_threshold_mps = 15\n        self.frame_rate = 24\n\n    def detect_events(self, tracks):\n        player_assigner = PlayerBallAssigner()\n        ball_possession_log = []\n        for frame_num in range(len(tracks['players'])):\n            player_track = tracks['players'][frame_num]\n            ball_bbox = tracks['ball'][frame_num].get(1, {}).get('bbox')\n            assigned_player_id = player_assigner.assign_ball_to_player(player_track, ball_bbox) if ball_bbox else -1\n            ball_possession_log.append(assigned_player_id)\n\n        events = []\n        last_player_with_ball, pass_start_info = -1, {}\n        for frame_num, current_player_id in enumerate(ball_possession_log):\n            ball_pos_transformed = tracks['ball'][frame_num].get(1, {}).get('position_transformed')\n            if not ball_pos_transformed: continue\n\n            is_valid_pass = (current_player_id != last_player_with_ball and last_player_with_ball != -1 and current_player_id != -1)\n            if is_valid_pass:\n                start_player_team = tracks['players'][pass_start_info['frame']][last_player_with_ball].get('team')\n                end_player_team = tracks['players'][frame_num].get(current_player_id, {}).get('team')\n                if start_player_team == end_player_team and start_player_team is not None:\n                    events.append({\n                        \"type_name\": \"Pass\", \"player_name\": f\"Player_{last_player_with_ball}\",\n                        \"team_name\": f\"Team {start_player_team}\", \"x\": pass_start_info['position'][0],\n                        \"y\": pass_start_info['position'][1], \"end_x\": ball_pos_transformed[0],\n                        \"end_y\": ball_pos_transformed[1], \"minute\": int(frame_num / (self.frame_rate * 60)),\n                        \"second\": int((frame_num / self.frame_rate) % 60)\n                    })\n\n            if current_player_id != -1:\n                pass_start_info = {'frame': frame_num, 'position': ball_pos_transformed}\n                last_player_with_ball = current_player_id\n\n        return pd.DataFrame(events)\n\n# TeamAssigner, PlayerBallAssigner, CameraMovementEstimator, ViewTransformer, SpeedAndDistanceEstimator (unchanged from your code)\nclass TeamAssigner:\n    def __init__(self):\n        self.team_colors, self.player_team_dict, self.kmeans = {}, {}, None\n    def get_player_color(self, frame, bbox):\n        image = frame[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2])]\n        if image.size == 0: return np.array([0,0,0])\n        top_half = image[0:int(image.shape[0] / 2), :]\n        if top_half.size == 0: return np.array([0,0,0])\n        kmeans = KMeans(n_clusters=2, init=\"k-means++\", n_init=1, random_state=0).fit(top_half.reshape(-1, 3))\n        labels = kmeans.labels_.reshape(top_half.shape[0], top_half.shape[1])\n        corner_clusters = [labels[0, 0], labels[0, -1], labels[-1, 0], labels[-1, -1]]\n        non_player_cluster = max(set(corner_clusters), key=corner_clusters.count)\n        return kmeans.cluster_centers_[1 - non_player_cluster]\n    def assign_team_color(self, frame, player_detections):\n        if not player_detections: return\n        colors = [self.get_player_color(frame, det[\"bbox\"]) for _, det in player_detections.items()]\n        self.kmeans = KMeans(n_clusters=2, init=\"k-means++\", n_init=10, random_state=0).fit(colors)\n        self.team_colors[1], self.team_colors[2] = self.kmeans.cluster_centers_\n    def get_player_team(self, frame, bbox, player_id):\n        if player_id in self.player_team_dict: return self.player_team_dict[player_id]\n        if self.kmeans is None: return 0\n        color = self.get_player_color(frame, bbox)\n        team_id = self.kmeans.predict(color.reshape(1, -1))[0] + 1\n        self.player_team_dict[player_id] = team_id\n        return team_id\n\nclass PlayerBallAssigner:\n    def __init__(self): self.max_dist = 70\n    def assign_ball_to_player(self, players, ball_bbox):\n        if not ball_bbox: return -1\n        ball_pos, min_dist, assigned_player = get_center_of_bbox(ball_bbox), float('inf'), -1\n        for id, player in players.items():\n            dist = measure_distance(get_foot_position(player['bbox']), ball_pos)\n            if dist < self.max_dist and dist < min_dist: min_dist, assigned_player = dist, id\n        return assigned_player\n\nclass CameraMovementEstimator:\n    def __init__(self, frame):\n        self.lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n        self.features = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n    def get_camera_movement(self, frames, read_from_stub=False, stub_path=None):\n        if read_from_stub and stub_path and os.path.exists(stub_path):\n            with open(stub_path, 'rb') as f: return pickle.load(f)\n        movements = [[0, 0]] * len(frames)\n        old_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n        old_features = cv2.goodFeaturesToTrack(old_gray, **self.features)\n        for i in range(1, len(frames)):\n            new_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n            new_features, status, _ = cv2.calcOpticalFlowPyrLK(old_gray, new_gray, old_features, None, **self.lk_params)\n            good_new = new_features[status==1]\n            good_old = old_features[status==1]\n            move_x, move_y = 0, 0\n            if len(good_new) > 0:\n                move_x, move_y = np.mean(good_old - good_new, axis=0).ravel()\n            movements[i] = [move_x, move_y]\n            old_gray = new_gray.copy()\n            old_features = good_new.reshape(-1, 1, 2)\n        if stub_path:\n            with open(stub_path, 'wb') as f: pickle.dump(movements, f)\n        return movements\n    def add_adjust_positions_to_tracks(self, tracks, movements):\n        for typ, obj_tracks in tracks.items():\n            for i, track in enumerate(obj_tracks):\n                for id, info in track.items():\n                    info['position_adjusted'] = (info['position'][0] + movements[i][0], info['position'][1] + movements[i][1])\n\nclass ViewTransformer:\n    def __init__(self):\n        court_w, court_l = 34, 52.5\n        self.pixel_verts = np.float32([[110, 1035], [265, 275], [910, 260], [1640, 915]])\n        self.target_verts = np.float32([[0, court_w], [0, 0], [court_l, 0], [court_l, court_w]])\n        self.transformer = cv2.getPerspectiveTransform(self.pixel_verts, self.target_verts)\n    def transform_point(self, point):\n        p = (int(point[0]), int(point[1]))\n        is_inside = cv2.pointPolygonTest(self.pixel_verts, p, False) >= 0\n        if not is_inside: return None\n        reshaped = np.array(point).reshape(-1, 1, 2).astype(np.float32)\n        transformed = cv2.perspectiveTransform(reshaped, self.transformer)\n        return transformed.reshape(-1, 2)\n    def add_transformed_position_to_tracks(self, tracks):\n        for typ, obj_tracks in tracks.items():\n            for track in obj_tracks:\n                for id, info in track.items():\n                    pos = info.get('position_adjusted', info.get('position'))\n                    if pos:\n                        transformed = self.transform_point(pos)\n                        info['position_transformed'] = transformed.squeeze().tolist() if transformed is not None else None\n\nclass SpeedAndDistanceEstimator:\n    def __init__(self):\n        self.frame_window, self.frame_rate = 24, 24\n    def add_speed_and_distance_to_tracks(self, tracks):\n        total_dist = {}\n        for typ, obj_tracks in tracks.items():\n            if typ not in [\"players\", \"referees\"]: continue\n            for i in range(len(obj_tracks)):\n                for id, info in obj_tracks[i].items():\n                    if i > 0:\n                        prev_info = tracks[typ][i-1].get(id)\n                        if prev_info and info.get('position_transformed') and prev_info.get('position_transformed'):\n                            dist = measure_distance(info['position_transformed'], prev_info['position_transformed'])\n                            total_dist[id] = total_dist.get(id, 0) + dist\n                            speed_mps = dist * self.frame_rate\n                            info['speed'] = speed_mps * 3.6\n                            info['distance'] = total_dist[id]\n    def draw_speed_and_distance(self, frames, tracks):\n        output_frames = []\n        for i, frame in enumerate(frames):\n            for typ, obj_tracks in tracks.items():\n                if typ not in [\"players\", \"referees\"]: continue\n                for id, info in obj_tracks[i].items():\n                    if \"speed\" in info:\n                        x, y = get_foot_position(info['bbox'])\n                        cv2.putText(frame, f\"{info['speed']:.1f} km/h\", (x - 20, y + 20),\n                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)\n            output_frames.append(frame)\n        return output_frames\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:42:31.091279Z","iopub.execute_input":"2025-08-26T10:42:31.091559Z","iopub.status.idle":"2025-08-26T10:42:31.145091Z","shell.execute_reply.started":"2025-08-26T10:42:31.091539Z","shell.execute_reply":"2025-08-26T10:42:31.144379Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def main():\n    # --- SETUP ---\n    INPUT_VIDEO_PATH = \"/kaggle/input/football-video2/CityUtdR.mp4\"\n    STUB_PATH = \"/kaggle/working/tracks_stub.pkl\"\n    OUTPUT_VIDEO_PATH = \"/kaggle/working/final_analysis_video-Llama-v2.mp4\"\n\n    frames = read_video(INPUT_VIDEO_PATH)\n    if not frames:\n        print(\"Video file not found or could not be read. Check the path.\")\n        return None\n\n    cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n    fps = cap.get(cv2.CAP_PROP_FPS) or 24\n    cap.release()\n\n    # --- INITIALIZE ALL MODULES ---\n    tracker = Tracker('yolov8x.pt')\n    commentary_engine = ImprovedCommentaryEngine(fps=fps, keyframes=3, max_words=25)\n    camera_estimator = CameraMovementEstimator(frames[0])\n    view_transformer = ViewTransformer()\n    speed_estimator = SpeedAndDistanceEstimator()\n    team_assigner = TeamAssigner()\n    player_assigner = PlayerBallAssigner()\n    ticker = RealTimeTicker(fps=fps)\n\n    # --- STAGE 1: TRACKING ---\n    print(\"Stage 1: Performing object detection and tracking...\")\n    tracks = tracker.get_object_tracks(frames, read_from_stub=False, stub_path=STUB_PATH)\n    tracks[\"ball\"] = tracker.interpolate_ball_positions(tracks[\"ball\"])\n    tracker.add_position_to_tracks(tracks)\n\n    # --- STAGE 2: MOTION & PERSPECTIVE ---\n    print(\"Stage 2: Estimating camera motion and transforming perspective...\")\n    camera_movement = camera_estimator.get_camera_movement(frames)\n    camera_estimator.add_adjust_positions_to_tracks(tracks, camera_movement)\n    view_transformer.add_transformed_position_to_tracks(tracks)\n    speed_estimator.add_speed_and_distance_to_tracks(tracks)\n\n    # --- STAGE 3: TEAM ASSIGNMENT ---\n    print(\"Stage 3: Assigning teams...\")\n    team_assigner.assign_team_color(frames[0], tracks['players'][0])\n    for frame_num, frame in enumerate(frames):\n        player_track = tracks['players'][frame_num]\n        for player_id, track in player_track.items():\n            team = team_assigner.get_player_team(frame, track['bbox'], player_id)\n            tracks['players'][frame_num][player_id]['team'] = team\n            tracks['players'][frame_num][player_id]['team_color'] = team_assigner.team_colors.get(team, (0,0,255))\n\n    # --- STAGE 4: GENERATE EVENTS DATA ---\n    print(\"Stage 4: Detecting events for commentary context...\")\n    event_detector = EventDetector()\n    events_df = event_detector.detect_events(tracks)\n    print(f\"Detected {len(events_df)} events for commentary context\")\n\n    # --- STAGE 5: BALL POSSESSION & COMMENTARY ---\n    print(\"Stage 5: Tracking ball possession and generating all commentary...\")\n    team_ball_control = []\n    ticker_history = []\n    cogvlm_history = []\n\n    for frame_num, frame in enumerate(frames):\n        player_track = tracks['players'][frame_num]\n        ball_bbox = tracks['ball'][frame_num].get(1, {}).get('bbox')\n\n        for player_id in player_track:\n            tracks['players'][frame_num][player_id]['has_ball'] = False\n\n        assigned_player = player_assigner.assign_ball_to_player(player_track, ball_bbox)\n        if assigned_player != -1:\n            tracks['players'][frame_num][assigned_player]['has_ball'] = True\n            team_ball_control.append(tracks['players'][frame_num][assigned_player]['team'])\n        else:\n            team_ball_control.append(team_ball_control[-1] if team_ball_control else 0)\n\n        ticker_history.append(ticker.update(tracks, frame_num))\n        commentary_engine.update_with_context(frame, tracks, frame_num, events_df)\n        cogvlm_history.append(commentary_engine.latest_commentary)\n\n        if frame_num % 100 == 0:\n            print(f\"Commentary progress: {frame_num}/{len(frames)} frames\")\n\n    team_ball_control = np.array(team_ball_control)\n\n    # --- STAGE 6: VISUALIZATION & SAVING ---\n    print(\"Stage 6: Combining commentary and saving final video...\")\n    display_commentary = ticker_history.copy()\n    last_cog = cogvlm_history[0]\n    for i, comment in enumerate(cogvlm_history):\n        if comment != last_cog:\n            start_frame = max(0, i - commentary_engine.clip_length_frames)\n            for j in range(start_frame, i):\n                if j < len(display_commentary):\n                    display_commentary[j] = comment\n            last_cog = comment\n\n    output_frames = []\n    for frame_num, frame in enumerate(frames):\n        frame_copy = frame.copy()\n        current_commentary = display_commentary[frame_num] if frame_num < len(display_commentary) else \" \"\n\n        player_dict = tracks[\"players\"][frame_num]\n        ball_dict = tracks.get(\"ball\", [])[frame_num]\n\n        for track_id, player in player_dict.items():\n            color = player.get(\"team_color\", (0, 0, 255))\n            frame_copy = tracker._draw_player_ellipse(frame_copy, player[\"bbox\"], color, track_id, player.get(\"jersey_number\"))\n            if player.get('has_ball', False):\n                frame_copy = tracker._draw_triangle(frame_copy, player[\"bbox\"], (0, 0, 255))\n\n        if 1 in ball_dict:\n            frame_copy = tracker._draw_triangle(frame_copy, ball_dict[1][\"bbox\"], (0, 255, 0))\n\n        frame_copy = tracker._draw_team_ball_control(frame_copy, frame_num, team_ball_control)\n        frame_copy = tracker._draw_commentary_overlay(frame_copy, current_commentary)\n        output_frames.append(frame_copy)\n\n    output_frames = speed_estimator.draw_speed_and_distance(output_frames, tracks)\n    save_video(output_frames, OUTPUT_VIDEO_PATH)\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"MATCH ANALYSIS COMPLETE\")\n    print(\"=\"*50)\n    print(f\"âœ… Video saved to: {OUTPUT_VIDEO_PATH}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:43:00.490947Z","iopub.execute_input":"2025-08-26T10:43:00.491695Z","iopub.status.idle":"2025-08-26T10:46:49.313093Z","shell.execute_reply.started":"2025-08-26T10:43:00.491667Z","shell.execute_reply":"2025-08-26T10:46:49.312173Z"}},"outputs":[{"name":"stdout","text":"âœ… Jersey OCR module initialized.\nðŸŽ™ï¸ Initializing CogVLM2 Commentary Engine...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69c94ee10dd249fa9c1a6642e215c53f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39191b9487e04eb39d53a0c9468837af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00006.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3f7957153494139ab444c69a1ed926b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00006.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d11452ae58e4d419a2ae19e5cb9aebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00006.safetensors:   0%|          | 0.00/4.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1e643faf7294d29b8c33b68c08cb807"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00006.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34e400fafbb142179e489e4bd98e62c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00006.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eef96afb625249e4bb33bdeb024de60b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00006.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93b2d82aa4aa4f90bab8306df64283a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"816d986f5bd843dd9411d1404311fc95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91edfaa8d6f34f6ca7d9be166c199d7d"}},"metadata":{}},{"name":"stdout","text":"âœ… CogVLM2-Llama3-Caption loaded on cuda.\nStage 1: Performing object detection and tracking...\nProcessing frame 0/268\nProcessing frame 20/268\nProcessing frame 40/268\nProcessing frame 60/268\nProcessing frame 80/268\nProcessing frame 100/268\nProcessing frame 120/268\nProcessing frame 140/268\nProcessing frame 160/268\nProcessing frame 180/268\nProcessing frame 200/268\nProcessing frame 220/268\nProcessing frame 240/268\nProcessing frame 260/268\nStage 2: Estimating camera motion and transforming perspective...\nStage 3: Assigning teams...\nStage 4: Detecting events for commentary context...\nDetected 7 events for commentary context\nStage 5: Tracking ball possession and generating all commentary...\nCommentary progress: 0/268 frames\nCommentary progress: 100/268 frames\nGenerating tactical summary...\nCommentary generation error: not support multi images by now.\nCommentary progress: 200/268 frames\nGenerating tactical summary...\nCommentary generation error: not support multi images by now.\nStage 6: Combining commentary and saving final video...\n\n==================================================\nMATCH ANALYSIS COMPLETE\n==================================================\nâœ… Video saved to: /kaggle/working/final_analysis_video-2.mp4\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Qwen2-VL-2B-Instruct","metadata":{}},{"cell_type":"code","source":"# keep torch/torchvision versions stable on Kaggle; install/upgrade the rest\n!pip -q install --upgrade ultralytics supervision easyocr numpy opencv-python scikit-learn pandas mplsoccer transformers accelerate huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T10:58:30.146828Z","iopub.execute_input":"2025-08-26T10:58:30.147975Z","iopub.status.idle":"2025-08-26T10:58:36.665991Z","shell.execute_reply.started":"2025-08-26T10:58:30.147948Z","shell.execute_reply":"2025-08-26T10:58:36.665109Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import json, re, math\nfrom typing import List, Dict, Any\nimport torch\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# ---- preferred models ----\nPREFERRED_VLM_ID = \"Qwen/Qwen2-VL-2B-Instruct\"   # primary\n# PREFERRED_VLM_ID = \"Qwen/Qwen2-VL-7B-Instruct\"\nFALLBACK_VLM_ID  = \"THUDM/cogvlm2-llama3-caption\"  # fallback if Qwen2-VL fails\nHF_REV = None  # optional: pin a hash/tag to freeze weights\n\n# small utilities\ndef clamp_words(s: str, max_words: int = 25) -> str:\n    s = \" \".join(s.strip().split())\n    if \".\" in s:\n        s = s.split(\".\")[0] + \".\"\n    words = s.split()\n    if len(words) > max_words:\n        s = \" \".join(words[:max_words]).rstrip(\",\") + \".\"\n    return s\n\ndef safe_json_extract(text: str) -> Dict[str, Any]:\n    # extract first {...} block\n    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n    if not m:\n        return {}\n    try:\n        return json.loads(m.group(0))\n    except Exception:\n        # try to fix trailing commas / quotes\n        x = re.sub(r\",\\s*}\", \"}\", m.group(0))\n        x = re.sub(r\",\\s*]\", \"]\", x)\n        try:\n            return json.loads(x)\n        except Exception:\n            return {}\n\nclass TacticalCaptioner:\n    \"\"\"\n    Analyst-grade captioner:\n      - multi-keyframe conditioning\n      - few-shot prompt with soccer ontology\n      - outputs (a) structured JSON, (b) one-line overlay sentence\n      - robust to model availability (Qwen2-VL -> CogVLM2 fallback)\n    \"\"\"\n    def __init__(self, fps=24, clip_seconds=5, keyframes=5, max_words=25):\n        self.fps = fps\n        self.clip_len = max(1, int(clip_seconds * fps))\n        self.keyframes = max(2, min(keyframes, 6))\n        self.max_words = max_words\n        self._buffer = deque(maxlen=self.clip_len)\n        self.latest_overlay = \"Match analysis is starting...\"\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        # try Qwen2-VL first\n        self.backend = None\n        self.processor = None\n        self.model_name = None\n        print(\"ðŸŽ™ï¸ Initializing Tactical Captioner (Qwen2-VL preferred)...\")\n        try:\n            from transformers import AutoProcessor\n            from transformers import Qwen2VLForConditionalGeneration\n            kw = dict()\n            if HF_REV: kw[\"revision\"] = HF_REV\n            self.processor = AutoProcessor.from_pretrained(PREFERRED_VLM_ID, **kw)\n            self.backend = Qwen2VLForConditionalGeneration.from_pretrained(\n                PREFERRED_VLM_ID,\n                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n                device_map=\"auto\" if torch.cuda.is_available() else None,\n                **kw\n            )\n            if self.device == \"cpu\": self.backend.to(self.device)\n            self.model_name = \"qwen2-vl\"\n            print(\"âœ… Using Qwen2-VL-7B-Instruct.\")\n        except Exception as e_qwen:\n            print(f\"âš ï¸ Qwen2-VL unavailable: {e_qwen} â€” falling back to CogVLM2.\")\n            # fallback: CogVLM2\n            try:\n                from transformers import AutoModelForCausalLM, AutoTokenizer\n                kw = dict(trust_remote_code=True)\n                if HF_REV: kw[\"revision\"] = HF_REV\n                self.processor = AutoTokenizer.from_pretrained(FALLBACK_VLM_ID, **kw)\n                self.backend = AutoModelForCausalLM.from_pretrained(\n                    FALLBACK_VLM_ID,\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    device_map=\"auto\" if torch.cuda.is_available() else None,\n                    **kw\n                )\n                self.model_name = \"cogvlm2\"\n                print(\"âœ… Using CogVLM2-Llama3-Caption.\")\n            except Exception as e_cog:\n                print(f\"âŒ Could not initialize any VLM: {e_cog}\")\n                self.backend = None\n                self.processor = None\n\n        # few-shot soccer examples (concise)\n        self.fewshot_examples = [\n            {\n              \"context\": {\n                \"time\": \"12:34\", \"possession\": \"Player 8 (Team 1)\",\n                \"stats\": {\"passes_last_10s\":2,\"recoveries_last_10s\":1,\"duels_last_10s\":1},\n                \"zones\": {\"ball_third\":\"middle\",\"ball_channel\":\"right half-space\"}\n              },\n              \"events\": [\n                {\"type\":\"recovery\",\"team\":\"Team 1\",\"loc\":[36.2,18.1]},\n                {\"type\":\"carry\",\"player\":\"8\",\"team\":\"Team 1\",\"meters\":7.5},\n                {\"type\":\"pass\",\"subtype\":\"through-ball\",\"from\":\"8\",\"to\":\"11\",\"team\":\"Team 1\",\"start\":[36.2,18.1],\"end\":[44.6,12.2]}\n              ],\n              \"caption\":\"Player 8 recovers centrally, drives forward, and threads a through ball to Player 11 running beyond the back line.\"\n            },\n            {\n              \"context\": {\n                \"time\":\"27:05\",\"possession\":\"Player 3 (Team 2)\",\n                \"stats\":{\"passes_last_10s\":3,\"recoveries_last_10s\":0,\"duels_last_10s\":2},\n                \"zones\":{\"ball_third\":\"final\",\"ball_channel\":\"left wing\"}\n              },\n              \"events\":[\n                {\"type\":\"duel\",\"outcome\":\"won\",\"team\":\"Team 2\",\"loc\":[50.8,5.3]},\n                {\"type\":\"cross\",\"from\":\"3\",\"target_zone\":\"six-yard\",\"height\":\"low\"}\n              ],\n              \"caption\":\"Left back wins a wide duel and delivers a low cross toward the six-yard area.\"\n            }\n        ]\n\n    # ---- public API ----\n    def push_frame(self, frame):\n        self._buffer.append(frame)\n\n    def maybe_caption(self, frame_idx:int, tracks:Dict, events_df:pd.DataFrame):\n        if self.backend is None or len(self._buffer) < self._buffer.maxlen:\n            return  # wait until clip filled\n        # build structured context using tracking + events_df\n        context = self._make_context(frame_idx, tracks, events_df)\n        images = self._sample_keyframes(list(self._buffer), self.keyframes)\n        try:\n            if self.model_name == \"qwen2-vl\":\n                result = self._caption_qwen(images, context)\n            else:\n                result = self._caption_cog(images, context)\n        except Exception as e:\n            print(f\"caption error: {e}\")\n            result = {\"overlay\": self._fallback_overlay(context), \"json\": {}}\n        self.latest_overlay = result.get(\"overlay\", self.latest_overlay)\n        self._buffer.clear()\n\n    # ---- context assembly ----\n    def _make_context(self, frame_idx:int, tracks:Dict, events_df:pd.DataFrame) -> Dict[str,Any]:\n        sec = frame_idx / max(1,self.fps)\n        mm = int(sec//60); ss = int(sec%60)\n        players = tracks[\"players\"][frame_idx]\n        # possession\n        poss = None\n        for pid, info in players.items():\n            if info.get(\"has_ball\"): poss = f\"Player {pid} (Team {info.get('team','?')})\"; break\n        # last 10s window events\n        recent = []\n        stats = {\"passes_last_10s\":0,\"recoveries_last_10s\":0,\"duels_last_10s\":0,\"carries_last_10s\":0}\n        if events_df is not None and not events_df.empty:\n            t_cut = sec - 10\n            ev_secs = events_df[\"minute\"]*60 + events_df[\"second\"]\n            recent_df = events_df[ev_secs >= t_cut].tail(6)\n            for _, r in recent_df.iterrows():\n                recent.append(r.to_dict())\n                et = r.get(\"type_name\",\"\").lower()\n                if \"pass\" in et: stats[\"passes_last_10s\"] += 1\n                if \"recovery\" in et: stats[\"recoveries_last_10s\"] += 1\n                if \"duel\" in et: stats[\"duels_last_10s\"] += 1\n                if \"carry\" in et: stats[\"carries_last_10s\"] += 1\n\n        # crude zone labels from transformed coords if ball exists\n        ball = tracks[\"ball\"][frame_idx].get(1, {})\n        zones = {\"ball_third\":\"unknown\",\"ball_channel\":\"central\"}\n        pt = ball.get(\"position_transformed\")\n        if isinstance(pt,(list,tuple)) and len(pt)==2:\n            x,y = pt # pitch: length x (0..52.5), width y (0..34)\n            thirds = [\"defensive\",\"middle\",\"final\"]\n            zones[\"ball_third\"] = thirds[min(2, max(0, int((x/52.5)*3)))]\n            # channels: left wing, left half-space, central, right half-space, right wing\n            if y<34*0.2: zones[\"ball_channel\"]=\"left wing\"\n            elif y<34*0.4: zones[\"ball_channel\"]=\"left half-space\"\n            elif y<34*0.6: zones[\"ball_channel\"]=\"central\"\n            elif y<34*0.8: zones[\"ball_channel\"]=\"right half-space\"\n            else: zones[\"ball_channel\"]=\"right wing\"\n\n        return {\"time\": f\"{mm}:{ss:02d}\",\"possession\": poss or \"Unclear\",\"stats\":stats,\"zones\":zones,\"recent\":recent}\n\n    # ---- model-specific captioning ----\n    def _caption_qwen(self, images:List[np.ndarray], ctx:Dict[str,Any]) -> Dict[str,Any]:\n        from transformers import AutoProcessor\n        # few-shot chat\n        msgs = [{\"role\":\"system\",\"content\":\"You are a professional football (soccer) tactical analyst. Output JSON then a single overlay sentence.\"}]\n        for ex in self.fewshot_examples:\n            msgs.append({\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":self._ctx_to_text(ex['context'])}]})\n            msgs.append({\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":json.dumps({\n                \"phase\":\"possession\",\"key_events\":ex[\"events\"],\"summary\":ex[\"caption\"]\n            }, ensure_ascii=False)}]})\n        # now the actual clip with multi-images\n        user_content = [{\"type\":\"text\",\"text\":self._ctx_to_text(ctx)}]\n        for img in images:\n            rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            user_content.insert(0, {\"type\":\"image\",\"image\":Image.fromarray(rgb)})\n        # ask for strict schema + overlay\n        schema = {\n            \"phase\": \"possession/transition/press/defense\",\n            \"key_events\":[\n                {\"type\":\"pass/carry/cross/through-ball/recovery/duel/interception/shot\",\n                 \"team\":\"Team 1/Team 2\",\"from\":\"<id?>\",\"to\":\"<id?>\",\"subtype\":\"\", \"meters\":\"<float?>\",\n                 \"outcome\":\"won/lost/completed/blocked\",\"loc\":[0,0],\"end\":[0,0]}\n            ],\n            \"summary\":\"<single sentence â‰¤25 words>\"\n        }\n        instruction = (\n            \"Return TWO parts:\\n\"\n            \"1) STRICT JSON matching this schema keys (omit unknown fields, keep lowercase types):\\n\"\n            + json.dumps(schema, ensure_ascii=False) +\n            \"\\n2) Then a newline and an overlay sentence for broadcast.\\n\"\n        )\n        msgs.append({\"role\":\"user\",\"content\": user_content + [{\"type\":\"text\",\"text\":instruction}]})\n        inputs = self.processor.apply_chat_template(msgs, add_generation_prompt=True, tokenize=False)\n        proc = self.processor(text=inputs, images=[x[\"image\"] for x in user_content if x.get(\"type\")==\"image\"], return_tensors=\"pt\")\n        for k in proc:\n            proc[k] = proc[k].to(self.backend.device)  # type: ignore\n\n        with torch.inference_mode():\n            out = self.backend.generate(**proc, max_new_tokens=220, do_sample=False, temperature=0.0, eos_token_id=self.processor.tokenizer.eos_token_id)\n        text = self.processor.batch_decode(out, skip_special_tokens=True)[0]\n        data = safe_json_extract(text)\n        overlay = clamp_words(data.get(\"summary\",\"\"), self.max_words) if isinstance(data, dict) else None\n        if not overlay:\n            # fallback: grab last line; then clamp\n            overlay = clamp_words(text.strip().splitlines()[-1], self.max_words)\n        return {\"json\": data, \"overlay\": overlay}\n\n    def _caption_cog(self, images:List[np.ndarray], ctx:Dict[str,Any]) -> Dict[str,Any]:\n        # Build a concise chat for CogVLM2 (single prompt with interleaved images)\n        prompt = (\n            \"You are a professional football (soccer) tactical analyst.\\n\"\n            f\"TIME: {ctx['time']}\\nPOSSESSION: {ctx['possession']}\\n\"\n            f\"ZONES: third={ctx['zones']['ball_third']}, channel={ctx['zones']['ball_channel']}\\n\"\n            f\"STATS(last 10s): {ctx['stats']}\\n\"\n            \"Identify passes, recoveries, duels, carries, crosses, through-balls, interceptions. \"\n            \"First output a compact JSON with keys phase, key_events[], summary; then one sentence for overlay (â‰¤25 words).\"\n        )\n        # CogVLM2 API expects tokenizer + images in build_conversation_input_ids\n        convo = self.backend.build_conversation_input_ids(self.processor, query=prompt,\n                                                         images=[Image.fromarray(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)) for im in images],\n                                                         template_version='chat')\n        inputs = {\n            'input_ids': convo['input_ids'].unsqueeze(0).to(self.backend.device),\n            'token_type_ids': convo['token_type_ids'].unsqueeze(0).to(self.backend.device),\n            'attention_mask': convo['attention_mask'].unsqueeze(0).to(self.backend.device),\n            'images': [[t.to(self.backend.device).to(self.backend.dtype) for t in convo['images']]]\n        }\n        with torch.inference_mode():\n            out = self.backend.generate(**inputs, max_new_tokens=220, do_sample=False, temperature=0.0,\n                                        pad_token_id=self.processor.eos_token_id, eos_token_id=self.processor.eos_token_id)\n        text = self.processor.decode(out[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n        data = safe_json_extract(text)\n        overlay = clamp_words(data.get(\"summary\",\"\"), self.max_words) if isinstance(data, dict) else clamp_words(text.splitlines()[-1], self.max_words)\n        return {\"json\": data, \"overlay\": overlay}\n\n    # helpers\n    def _ctx_to_text(self, ctx:Dict[str,Any]) -> str:\n        return (\n            f\"Time={ctx['time']}; Possession={ctx['possession']}; \"\n            f\"Zones(third={ctx['zones']['ball_third']}, channel={ctx['zones']['ball_channel']}); \"\n            f\"RecentStats={ctx['stats']}; RecentEvents(up to 3)={[(e.get('type_name'), e.get('team_name')) for e in ctx.get('recent', [])][-3:]}\"\n        )\n\n    def _sample_keyframes(self, frames:List[np.ndarray], k:int) -> List[np.ndarray]:\n        n = len(frames); \n        if k >= n: return frames\n        step = n / float(k+1)\n        idxs = [min(n-1, max(0, int((i+1)*step)-1)) for i in range(k)]\n        return [frames[i] for i in idxs]\n\n    def _fallback_overlay(self, ctx):\n        if ctx.get(\"possession\",\"Unclear\") != \"Unclear\":\n            return f\"{ctx['possession']} retains possession and progresses play.\"\n        return \"Play develops; possession unclear.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T11:23:00.653768Z","iopub.execute_input":"2025-08-26T11:23:00.654150Z","iopub.status.idle":"2025-08-26T11:23:00.693809Z","shell.execute_reply.started":"2025-08-26T11:23:00.654128Z","shell.execute_reply":"2025-08-26T11:23:00.693249Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class EventDetector:\n    \"\"\"\n    Lightweight heuristics to seed the captioner with grounded facts.\n    Relies on transformed pitch coords (meters) when available.\n    \"\"\"\n    def __init__(self, frame_rate=24):\n        self.frame_rate = frame_rate\n        self.max_assign_dist_px = 70  # already used upstream\n\n        # thresholds (meters / seconds)\n        self.carry_min_m = 4.0\n        self.pass_min_end_dist_m = 5.0\n        self.throughball_min_dx_m = 8.0   # forward progress\n        self.cross_y_band = (52.5*0.66, 52.5)  # into final third (approx box area in x)\n        self.wing_band = 34*0.2\n\n    def _dist(self, a, b):\n        return math.hypot(a[0]-b[0], a[1]-b[1])\n\n    def detect_events(self, tracks:Dict[str,Any]) -> pd.DataFrame:\n        n = len(tracks['players'])\n        # 1) ball possession per frame (you already tag has_ball)\n        poss = []\n        for t in range(n):\n            pid = -1\n            for p, info in tracks['players'][t].items():\n                if info.get(\"has_ball\"): pid = p; break\n            poss.append(pid)\n\n        events = []\n        last_pid = -1\n        last_pos_trans = None\n        last_pid_pos_trans = None\n        last_possess_frame = 0\n\n        for t in range(n):\n            ball_info = tracks['ball'][t].get(1, {})\n            bpos = ball_info.get(\"position_transformed\")\n            if not bpos: \n                continue\n            pid = poss[t]\n\n            # carries: same player keeps ball and moves enough distance\n            if pid != -1 and pid == last_pid:\n                if last_pid_pos_trans:\n                    moved = self._dist(bpos, last_pid_pos_trans)\n                    dt = (t - last_possess_frame) / self.frame_rate\n                    if moved >= self.carry_min_m and dt >= 0.5:\n                        events.append(dict(\n                            type_name=\"Carry\", player_name=f\"Player_{pid}\",\n                            team_name=f\"Team {tracks['players'][t][pid].get('team', '?')}\",\n                            x=last_pid_pos_trans[0], y=last_pid_pos_trans[1],\n                            end_x=bpos[0], end_y=bpos[1],\n                            minute=int(t/(self.frame_rate*60)), second=int((t/self.frame_rate)%60)\n                        ))\n                        last_possess_frame = t\n\n            # possession changes -> recovery/duel/interception/pass outcome\n            if pid != last_pid:\n                # recovery if previous frame had no owner (-1) then now someone does\n                if last_pid == -1 and pid != -1:\n                    events.append(dict(\n                        type_name=\"Recovery\", player_name=f\"Player_{pid}\",\n                        team_name=f\"Team {tracks['players'][t][pid].get('team','?')}\",\n                        x=bpos[0], y=bpos[1], end_x=bpos[0], end_y=bpos[1],\n                        minute=int(t/(self.frame_rate*60)), second=int((t/self.frame_rate)%60)\n                    ))\n                # duel if two opponents within ~1.5m of ball around switch\n                elif last_pid != -1 and pid != -1:\n                    last_team = tracks['players'][t].get(last_pid, {}).get('team')\n                    this_team = tracks['players'][t].get(pid, {}).get('team')\n                    if last_team is not None and this_team is not None and last_team != this_team:\n                        # crude proximity check\n                        last_pos = tracks['players'][t].get(last_pid, {}).get('position_transformed')\n                        new_pos = tracks['players'][t].get(pid, {}).get('position_transformed')\n                        if last_pos and new_pos and (self._dist(last_pos, bpos) < 1.5 or self._dist(new_pos, bpos) < 1.5):\n                            events.append(dict(\n                                type_name=\"Duel (won)\", player_name=f\"Player_{pid}\",\n                                team_name=f\"Team {this_team}\", x=bpos[0], y=bpos[1],\n                                end_x=bpos[0], end_y=bpos[1],\n                                minute=int(t/(self.frame_rate*60)), second=int((t/self.frame_rate)%60)\n                            ))\n\n            # passes: owner changes within same team after flight\n            if pid != -1 and last_pid != -1 and pid != last_pid:\n                team_a = tracks['players'][t].get(pid, {}).get('team')\n                team_b = tracks['players'][t].get(last_pid, {}).get('team')\n                if team_a is not None and team_b is not None and team_a == team_b and last_pos_trans:\n                    travel = self._dist(last_pos_trans, bpos)\n                    if travel >= self.pass_min_end_dist_m:\n                        subtype = \"through-ball\" if (bpos[0]-last_pos_trans[0]) > self.throughball_min_dx_m else \"pass\"\n                        # cross: from wing into final third\n                        wing = (last_pos_trans[1] < self.wing_band) or (last_pos_trans[1] > 34 - self.wing_band)\n                        into_final = bpos[0] >= self.cross_y_band[0]\n                        if wing and into_final:\n                            subtype = \"cross\"\n                        events.append(dict(\n                            type_name=\"Pass\" if subtype==\"pass\" else subtype.title(),\n                            player_name=f\"Player_{last_pid}\", team_name=f\"Team {team_a}\",\n                            x=last_pos_trans[0], y=last_pos_trans[1],\n                            end_x=bpos[0], end_y=bpos[1],\n                            minute=int(t/(self.frame_rate*60)), second=int((t/self.frame_rate)%60)\n                        ))\n\n            last_pid_pos_trans = bpos if pid != -1 else last_pid_pos_trans\n            last_pos_trans = bpos\n            if pid != -1:\n                last_pid = pid\n                last_possess_frame = t\n\n        return pd.DataFrame(events)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T11:23:08.183517Z","iopub.execute_input":"2025-08-26T11:23:08.183822Z","iopub.status.idle":"2025-08-26T11:23:08.199734Z","shell.execute_reply.started":"2025-08-26T11:23:08.183794Z","shell.execute_reply":"2025-08-26T11:23:08.199019Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Cell 6: Main Function (Updated for TacticalCaptioner + richer captions)\ndef main():\n    # --- SETUP ---\n    INPUT_VIDEO_PATH = \"/kaggle/input/football-video2/CityUtdR.mp4\"\n    STUB_PATH = \"/kaggle/working/tracks_stub.pkl\"\n    OUTPUT_VIDEO_PATH = \"/kaggle/working/final_analysis_video-qwen.mp4\"\n\n    frames = read_video(INPUT_VIDEO_PATH)\n    if not frames:\n        print(\"Video file not found or could not be read. Check the path.\")\n        return None\n\n    cap = cv2.VideoCapture(INPUT_VIDEO_PATH)\n    fps = cap.get(cv2.CAP_PROP_FPS) or 24\n    cap.release()\n\n    # --- INITIALIZE ALL MODULES ---\n    tracker = Tracker('yolov8x.pt')\n    captioner = TacticalCaptioner(fps=fps, clip_seconds=5, keyframes=5, max_words=25)\n    camera_estimator = CameraMovementEstimator(frames[0])\n    view_transformer = ViewTransformer()\n    speed_estimator = SpeedAndDistanceEstimator()\n    team_assigner = TeamAssigner()\n    player_assigner = PlayerBallAssigner()\n    ticker = RealTimeTicker(fps=fps)\n\n    # --- STAGE 1: TRACKING ---\n    print(\"Stage 1: Performing object detection and tracking...\")\n    tracks = tracker.get_object_tracks(frames, read_from_stub=False, stub_path=STUB_PATH)\n    tracks[\"ball\"] = tracker.interpolate_ball_positions(tracks[\"ball\"])\n    tracker.add_position_to_tracks(tracks)\n\n    # --- STAGE 2: MOTION & PERSPECTIVE ---\n    print(\"Stage 2: Estimating camera motion and transforming perspective...\")\n    camera_movement = camera_estimator.get_camera_movement(frames)\n    camera_estimator.add_adjust_positions_to_tracks(tracks, camera_movement)\n    view_transformer.add_transformed_position_to_tracks(tracks)\n    speed_estimator.add_speed_and_distance_to_tracks(tracks)\n\n    # --- STAGE 3: TEAM ASSIGNMENT ---\n    print(\"Stage 3: Assigning teams...\")\n    team_assigner.assign_team_color(frames[0], tracks['players'][0])\n    for frame_num, frame in enumerate(frames):\n        player_track = tracks['players'][frame_num]\n        for player_id, track in player_track.items():\n            team = team_assigner.get_player_team(frame, track['bbox'], player_id)\n            tracks['players'][frame_num][player_id]['team'] = team\n            tracks['players'][frame_num][player_id]['team_color'] = team_assigner.team_colors.get(team, (0,0,255))\n\n    # --- STAGE 4: GENERATE EVENTS DATA ---\n    print(\"Stage 4: Detecting events for commentary context...\")\n    event_detector = EventDetector(frame_rate=fps)\n    events_df = event_detector.detect_events(tracks)\n    print(f\"Detected {len(events_df)} events for commentary context\")\n\n    # --- STAGE 5: BALL POSSESSION, TICKER & TACTICAL CAPTIONS ---\n    print(\"Stage 5: Tracking ball possession and generating all commentary...\")\n    team_ball_control = []\n    ticker_history = []\n    overlay_history = []      # one-line sentences from TacticalCaptioner\n    # (optional) collect machine-readable JSON per clip if you want to save later\n    # caption_json_history = []\n\n    for frame_num, frame in enumerate(frames):\n        player_track = tracks['players'][frame_num]\n        ball_bbox = tracks['ball'][frame_num].get(1, {}).get('bbox')\n\n        # reset has_ball flags\n        for pid in player_track:\n            tracks['players'][frame_num][pid]['has_ball'] = False\n\n        # assign ball to nearest player\n        assigned_player = player_assigner.assign_ball_to_player(player_track, ball_bbox)\n        if assigned_player != -1:\n            tracks['players'][frame_num][assigned_player]['has_ball'] = True\n            team_ball_control.append(tracks['players'][frame_num][assigned_player]['team'])\n        else:\n            team_ball_control.append(team_ball_control[-1] if team_ball_control else 0)\n\n        # live ticker\n        ticker_history.append(ticker.update(tracks, frame_num))\n\n        # push frame to the tactical captioner and let it emit overlay when clip window fills\n        captioner.push_frame(frame)\n        captioner.maybe_caption(frame_num, tracks, events_df)\n        overlay_history.append(captioner.latest_overlay)\n\n        if frame_num % 100 == 0:\n            print(f\"Commentary progress: {frame_num}/{len(frames)} frames\")\n\n    team_ball_control = np.array(team_ball_control)\n\n    # --- STAGE 6: VISUALIZATION & SAVING ---\n    print(\"Stage 6: Combining commentary and saving final video...\")\n    # start from ticker, then backfill segments with tactical overlays when they update\n    display_commentary = ticker_history.copy()\n    last_overlay = overlay_history[0] if overlay_history else \" \"\n    for i, overlay in enumerate(overlay_history):\n        if overlay != last_overlay:\n            start_frame = max(0, i - captioner.clip_len)  # spread overlay over the last clip span\n            for j in range(start_frame, i):\n                if j < len(display_commentary):\n                    display_commentary[j] = overlay\n            last_overlay = overlay\n\n    output_frames = []\n    for frame_num, frame in enumerate(frames):\n        frame_copy = frame.copy()\n        current_commentary = display_commentary[frame_num] if frame_num < len(display_commentary) else \" \"\n\n        player_dict = tracks[\"players\"][frame_num]\n        ball_dict = tracks.get(\"ball\", [])[frame_num]\n\n        for track_id, player in player_dict.items():\n            color = player.get(\"team_color\", (0, 0, 255))\n            frame_copy = tracker._draw_player_ellipse(frame_copy, player[\"bbox\"], color, track_id, player.get(\"jersey_number\"))\n            if player.get('has_ball', False):\n                frame_copy = tracker._draw_triangle(frame_copy, player[\"bbox\"], (0, 0, 255))\n\n        if 1 in ball_dict:\n            frame_copy = tracker._draw_triangle(frame_copy, ball_dict[1][\"bbox\"], (0, 255, 0))\n\n        frame_copy = tracker._draw_team_ball_control(frame_copy, frame_num, team_ball_control)\n        frame_copy = tracker._draw_commentary_overlay(frame_copy, current_commentary)\n        output_frames.append(frame_copy)\n\n    output_frames = speed_estimator.draw_speed_and_distance(output_frames, tracks)\n    save_video(output_frames, OUTPUT_VIDEO_PATH)\n\n    # --- FINAL STATISTICS ---\n    print(\"\\n\" + \"=\"*50)\n    print(\"MATCH ANALYSIS COMPLETE\")\n    print(\"=\"*50)\n    print(f\"âœ… Video saved to: {OUTPUT_VIDEO_PATH}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T11:23:13.383020Z","iopub.execute_input":"2025-08-26T11:23:13.383317Z","iopub.status.idle":"2025-08-26T11:26:46.291566Z","shell.execute_reply.started":"2025-08-26T11:23:13.383297Z","shell.execute_reply":"2025-08-26T11:26:46.290659Z"}},"outputs":[{"name":"stdout","text":"âœ… Jersey OCR module initialized.\nðŸŽ™ï¸ Initializing Tactical Captioner (Qwen2-VL preferred)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5438a6a9b34e4671ae92acea705cdaab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58add75d485148a793f812961f4cd8de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e40f050ec8ea4588a60f66decacbe754"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19315c4a9b7c4102ad449ed737d0369c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a4bae22b18d4a1ab77ad03469813e50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e424e86c549545c9866379ebddef7927"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb5cda88217640c187af9f1adc382f77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f990b410ece9439fbb0ff0e1308bc95f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a877d74be0c147d29120138b38f33dcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddea6eeafa8a43149cb4dbe27cf6a6c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09270aa437654fa2a8a4fac257ec9ee8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52c725cb99e34925a30e895771a07de1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e5714cdeebd4c0eb8ea5e08743d625b"}},"metadata":{}},{"name":"stdout","text":"âœ… Using Qwen2-VL-7B-Instruct.\nStage 1: Performing object detection and tracking...\nProcessing frame 0/268\nProcessing frame 20/268\nProcessing frame 40/268\nProcessing frame 60/268\nProcessing frame 80/268\nProcessing frame 100/268\nProcessing frame 120/268\nProcessing frame 140/268\nProcessing frame 160/268\nProcessing frame 180/268\nProcessing frame 200/268\nProcessing frame 220/268\nProcessing frame 240/268\nProcessing frame 260/268\nStage 2: Estimating camera motion and transforming perspective...\nStage 3: Assigning teams...\nStage 4: Detecting events for commentary context...\nDetected 0 events for commentary context\nStage 5: Tracking ball possession and generating all commentary...\nCommentary progress: 0/268 frames\nCommentary progress: 100/268 frames\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Commentary progress: 200/268 frames\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Stage 6: Combining commentary and saving final video...\n\n==================================================\nMATCH ANALYSIS COMPLETE\n==================================================\nâœ… Video saved to: /kaggle/working/final_analysis_video-qwen.mp4\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}